{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Regularyzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from network import NN\n",
    "from activation_functions import *\n",
    "from metrics import *\n",
    "from prepare_data import read_classification_data, read_regression_data\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cv_network(seeds=[123, 1, 2, 23, 42], build_args=None, fit_args=None):\n",
    "    scores_test = []\n",
    "    scores_train = []\n",
    "    nns  = []\n",
    "    for s in seeds:\n",
    "        nn = NN(**build_args, seed=s)\n",
    "        last_fa = None\n",
    "        for fa in fit_args:\n",
    "            nn.fit(**fa)\n",
    "            last_fa = fa\n",
    "        nns.append(nn)\n",
    "        scores_test.append(last_fa['metric'](last_fa['y_test'], nn.predict(last_fa['x_test'])))\n",
    "        scores_train.append(last_fa['metric'](last_fa['y_train'], nn.predict(last_fa['x_train'])))\n",
    "\n",
    "    return scores_train, scores_test, nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=['no. cv', 'metric', 'mean metric train', 'mean metric test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_x_train, ms_y_train, ms_x_test, ms_y_test = read_regression_data('multimodal-sparse', index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bez regularyzacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/700\n",
      "Loss on training set: 2222.1382946641365, loss on test set: 2192.269161391641\n",
      "Epoch number 20/700\n",
      "Loss on training set: 3347.360247821107, loss on test set: 4439.912121278672\n",
      "Epoch number 30/700\n",
      "Loss on training set: 1467.4589243551031, loss on test set: 1564.1447173518623\n",
      "Epoch number 40/700\n",
      "Loss on training set: 1085.7517904636593, loss on test set: 1528.3619337427708\n",
      "Epoch number 50/700\n",
      "Loss on training set: 1644.954637368966, loss on test set: 1729.4080471606558\n",
      "Epoch number 60/700\n",
      "Loss on training set: 1288.832914576759, loss on test set: 1874.65933286635\n",
      "Epoch number 70/700\n",
      "Loss on training set: 1143.3702671785911, loss on test set: 1690.4639784213962\n",
      "Epoch number 80/700\n",
      "Loss on training set: 580.1899504025191, loss on test set: 885.422085420701\n",
      "Epoch number 90/700\n",
      "Loss on training set: 1515.5621906134934, loss on test set: 2139.1367138469145\n",
      "Epoch number 100/700\n",
      "Loss on training set: 1830.8758093233114, loss on test set: 1119.1301014124722\n",
      "Epoch number 110/700\n",
      "Loss on training set: 1538.2620561451902, loss on test set: 1849.2113193601313\n",
      "Epoch number 120/700\n",
      "Loss on training set: 1269.0547503042744, loss on test set: 1992.3269858291037\n",
      "Epoch number 130/700\n",
      "Loss on training set: 1258.3788490136853, loss on test set: 1943.8761406780957\n",
      "Epoch number 140/700\n",
      "Loss on training set: 1106.8247385282048, loss on test set: 1659.2337494623766\n",
      "Epoch number 150/700\n",
      "Loss on training set: 1148.687691687192, loss on test set: 1689.6749418282025\n",
      "Epoch number 160/700\n",
      "Loss on training set: 1122.8570047573785, loss on test set: 1793.1950413534062\n",
      "Epoch number 170/700\n",
      "Loss on training set: 1218.38490958615, loss on test set: 1443.4293064982915\n",
      "Epoch number 180/700\n",
      "Loss on training set: 971.1244735916604, loss on test set: 1451.541568857808\n",
      "Epoch number 190/700\n",
      "Loss on training set: 601.7767715511086, loss on test set: 936.4097091324377\n",
      "Epoch number 200/700\n",
      "Loss on training set: 1117.015254023264, loss on test set: 1760.999510597142\n",
      "Epoch number 210/700\n",
      "Loss on training set: 1063.60049448256, loss on test set: 1540.433324356757\n",
      "Epoch number 220/700\n",
      "Loss on training set: 1572.0193720355808, loss on test set: 2149.023191194913\n",
      "Epoch number 230/700\n",
      "Loss on training set: 1120.566667630554, loss on test set: 1492.0571810243002\n",
      "Epoch number 240/700\n",
      "Loss on training set: 899.5653124319273, loss on test set: 1205.5471536296238\n",
      "Epoch number 250/700\n",
      "Loss on training set: 1042.106224954716, loss on test set: 1155.8600228698676\n",
      "Epoch number 260/700\n",
      "Loss on training set: 1648.4769849686054, loss on test set: 2258.311747553478\n",
      "Epoch number 270/700\n",
      "Loss on training set: 817.7968450152488, loss on test set: 750.2745558994576\n",
      "Epoch number 280/700\n",
      "Loss on training set: 344.2313667009329, loss on test set: 819.8560475085254\n",
      "Epoch number 290/700\n",
      "Loss on training set: 367.7586316663569, loss on test set: 815.8763623499976\n",
      "Epoch number 300/700\n",
      "Loss on training set: 472.9078536809702, loss on test set: 783.4635733774054\n",
      "Epoch number 310/700\n",
      "Loss on training set: 634.0670173352277, loss on test set: 860.8954573456547\n",
      "Epoch number 320/700\n",
      "Loss on training set: 868.0778373028268, loss on test set: 900.3283521496621\n",
      "Epoch number 330/700\n",
      "Loss on training set: 334.70653228524804, loss on test set: 717.6861971641458\n",
      "Epoch number 340/700\n",
      "Loss on training set: 292.13529359329334, loss on test set: 534.8565622177232\n",
      "Epoch number 350/700\n",
      "Loss on training set: 874.2326702310902, loss on test set: 1130.1661726391383\n",
      "Epoch number 360/700\n",
      "Loss on training set: 478.1539532755818, loss on test set: 660.3823836360757\n",
      "Epoch number 370/700\n",
      "Loss on training set: 343.01802159285734, loss on test set: 585.0263474343012\n",
      "Epoch number 380/700\n",
      "Loss on training set: 268.83549599573996, loss on test set: 635.8103194550806\n",
      "Epoch number 390/700\n",
      "Loss on training set: 354.05721448816666, loss on test set: 713.7407061245544\n",
      "Epoch number 400/700\n",
      "Loss on training set: 350.5856933714444, loss on test set: 729.8738389026653\n",
      "Epoch number 410/700\n",
      "Loss on training set: 1104.4312901069757, loss on test set: 859.1969025031328\n",
      "Epoch number 420/700\n",
      "Loss on training set: 707.1813285364858, loss on test set: 1003.261373028185\n",
      "Epoch number 430/700\n",
      "Loss on training set: 557.3834598053324, loss on test set: 688.1783905551929\n",
      "Epoch number 440/700\n",
      "Loss on training set: 640.128935393663, loss on test set: 755.7969399321152\n",
      "Epoch number 450/700\n",
      "Loss on training set: 878.102244855224, loss on test set: 972.056193251028\n",
      "Epoch number 460/700\n",
      "Loss on training set: 356.45725885917045, loss on test set: 537.1954402968241\n",
      "Epoch number 470/700\n",
      "Loss on training set: 272.2672180304727, loss on test set: 402.5172254595172\n",
      "Epoch number 480/700\n",
      "Loss on training set: 175.98437824540122, loss on test set: 408.2331618334439\n",
      "Epoch number 490/700\n",
      "Loss on training set: 186.75093253093522, loss on test set: 428.43855769166754\n",
      "Epoch number 500/700\n",
      "Loss on training set: 177.32991962073166, loss on test set: 440.3033104565465\n",
      "Epoch number 510/700\n",
      "Loss on training set: 215.67013171055942, loss on test set: 342.22271034430645\n",
      "Epoch number 520/700\n",
      "Loss on training set: 212.31944495101416, loss on test set: 404.1685121480214\n",
      "Epoch number 530/700\n",
      "Loss on training set: 195.79888063855824, loss on test set: 448.2503659878993\n",
      "Epoch number 540/700\n",
      "Loss on training set: 151.61784100381834, loss on test set: 430.6045088167429\n",
      "Epoch number 550/700\n",
      "Loss on training set: 183.06460351356822, loss on test set: 326.5944298427457\n",
      "Epoch number 560/700\n",
      "Loss on training set: 147.05377430169864, loss on test set: 340.26903462992937\n",
      "Epoch number 570/700\n",
      "Loss on training set: 163.5561602965594, loss on test set: 423.5055095463542\n",
      "Epoch number 580/700\n",
      "Loss on training set: 595.6713418301997, loss on test set: 622.158221601527\n",
      "Epoch number 590/700\n",
      "Loss on training set: 274.4520050796312, loss on test set: 433.3513642233032\n",
      "Epoch number 600/700\n",
      "Loss on training set: 575.5465180757674, loss on test set: 561.8445749865588\n",
      "Epoch number 610/700\n",
      "Loss on training set: 177.90384717672268, loss on test set: 373.81853709911434\n",
      "Epoch number 620/700\n",
      "Loss on training set: 839.9322424574718, loss on test set: 717.6008564673498\n",
      "Epoch number 630/700\n",
      "Loss on training set: 136.46185152872638, loss on test set: 385.3565545322839\n",
      "Epoch number 640/700\n",
      "Loss on training set: 193.16301489455944, loss on test set: 355.8168062419929\n",
      "Epoch number 650/700\n",
      "Loss on training set: 170.7909760731174, loss on test set: 359.3956555774611\n",
      "Epoch number 660/700\n",
      "Loss on training set: 179.04627920232807, loss on test set: 333.89081008892623\n",
      "Epoch number 670/700\n",
      "Loss on training set: 142.620548014508, loss on test set: 326.85978371270545\n",
      "Epoch number 680/700\n",
      "Loss on training set: 118.96808029851638, loss on test set: 301.69018198796806\n",
      "Epoch number 690/700\n",
      "Loss on training set: 187.059922811273, loss on test set: 249.59767689649254\n",
      "Epoch number 700/700\n",
      "Loss on training set: 127.06774733575182, loss on test set: 435.9456119482397\n",
      "Epoch number 10/1130\n",
      "Loss on training set: 64.38516062562287, loss on test set: 249.86246115876511\n",
      "Epoch number 20/1130\n",
      "Loss on training set: 60.81323124791502, loss on test set: 223.02304211207192\n",
      "Epoch number 30/1130\n",
      "Loss on training set: 58.80035336199872, loss on test set: 257.71385228145647\n",
      "Epoch number 40/1130\n",
      "Loss on training set: 53.69318750899961, loss on test set: 217.37277353118793\n",
      "Epoch number 50/1130\n",
      "Loss on training set: 48.81800505967335, loss on test set: 218.48220141653968\n",
      "Epoch number 60/1130\n",
      "Loss on training set: 49.16532998768061, loss on test set: 234.1538820473777\n",
      "Epoch number 70/1130\n",
      "Loss on training set: 44.46344913896749, loss on test set: 242.30481315257578\n",
      "Epoch number 80/1130\n",
      "Loss on training set: 41.14182071762271, loss on test set: 207.14368278119866\n",
      "Epoch number 90/1130\n",
      "Loss on training set: 40.495951961363794, loss on test set: 236.85474375746278\n",
      "Epoch number 100/1130\n",
      "Loss on training set: 34.522121127785674, loss on test set: 185.97386097673652\n",
      "Epoch number 110/1130\n",
      "Loss on training set: 30.513293001810972, loss on test set: 198.67257987840085\n",
      "Epoch number 120/1130\n",
      "Loss on training set: 30.03685058237669, loss on test set: 184.818530506697\n",
      "Epoch number 130/1130\n",
      "Loss on training set: 29.253956183340108, loss on test set: 233.30619191061479\n",
      "Epoch number 140/1130\n",
      "Loss on training set: 26.91695979627866, loss on test set: 190.07363208442337\n",
      "Epoch number 150/1130\n",
      "Loss on training set: 24.44130153496387, loss on test set: 196.32065552658133\n",
      "Epoch number 160/1130\n",
      "Loss on training set: 23.655131373973184, loss on test set: 194.2275841380599\n",
      "Epoch number 170/1130\n",
      "Loss on training set: 24.809209218453695, loss on test set: 194.2937659977864\n",
      "Epoch number 180/1130\n",
      "Loss on training set: 21.747243414876458, loss on test set: 185.61909609256705\n",
      "Epoch number 190/1130\n",
      "Loss on training set: 19.069351431924208, loss on test set: 196.80010502801684\n",
      "Epoch number 200/1130\n",
      "Loss on training set: 18.81819527713131, loss on test set: 173.54889416557543\n",
      "Epoch number 210/1130\n",
      "Loss on training set: 17.181300268399468, loss on test set: 188.49065769024716\n",
      "Epoch number 220/1130\n",
      "Loss on training set: 17.338786059118384, loss on test set: 178.9893758925257\n",
      "Epoch number 230/1130\n",
      "Loss on training set: 16.12866293076039, loss on test set: 175.77136487745898\n",
      "Epoch number 240/1130\n",
      "Loss on training set: 20.143245102511827, loss on test set: 198.28061972374374\n",
      "Epoch number 250/1130\n",
      "Loss on training set: 15.967550805871776, loss on test set: 185.10874843466786\n",
      "Epoch number 260/1130\n",
      "Loss on training set: 18.561670997354234, loss on test set: 172.08582505685064\n",
      "Epoch number 270/1130\n",
      "Loss on training set: 16.009040507902935, loss on test set: 184.86200506529906\n",
      "Epoch number 280/1130\n",
      "Loss on training set: 15.014694137044449, loss on test set: 174.0805553020438\n",
      "Epoch number 290/1130\n",
      "Loss on training set: 15.902645579597404, loss on test set: 171.39639373109304\n",
      "Epoch number 300/1130\n",
      "Loss on training set: 14.626102350167429, loss on test set: 175.4998070106332\n",
      "Epoch number 310/1130\n",
      "Loss on training set: 18.48756697069166, loss on test set: 171.63016017193843\n",
      "Epoch number 320/1130\n",
      "Loss on training set: 15.685462980479778, loss on test set: 174.19076211427156\n",
      "Epoch number 330/1130\n",
      "Loss on training set: 14.242925663345398, loss on test set: 164.3798305026623\n",
      "Epoch number 340/1130\n",
      "Loss on training set: 15.721910966305966, loss on test set: 148.14066817962967\n",
      "Epoch number 350/1130\n",
      "Loss on training set: 16.937761492350894, loss on test set: 159.973358455225\n",
      "Epoch number 360/1130\n",
      "Loss on training set: 18.320792705131957, loss on test set: 159.73429607280138\n",
      "Epoch number 370/1130\n",
      "Loss on training set: 15.046497774091012, loss on test set: 176.37501299309884\n",
      "Epoch number 380/1130\n",
      "Loss on training set: 18.683507735402276, loss on test set: 153.9802530359302\n",
      "Epoch number 390/1130\n",
      "Loss on training set: 16.041615496161683, loss on test set: 161.73507361312036\n",
      "Epoch number 400/1130\n",
      "Loss on training set: 17.252779434621633, loss on test set: 165.21725027530755\n",
      "Epoch number 410/1130\n",
      "Loss on training set: 14.057150227611242, loss on test set: 158.51550355950516\n",
      "Epoch number 420/1130\n",
      "Loss on training set: 13.101285557480665, loss on test set: 150.25625062519728\n",
      "Epoch number 430/1130\n",
      "Loss on training set: 14.324379582234112, loss on test set: 142.56740397103601\n",
      "Epoch number 440/1130\n",
      "Loss on training set: 13.87283391934472, loss on test set: 162.3886884454043\n",
      "Epoch number 450/1130\n",
      "Loss on training set: 13.907229224205839, loss on test set: 164.96293493561717\n",
      "Epoch number 460/1130\n",
      "Loss on training set: 13.528410985735615, loss on test set: 162.85675148666454\n",
      "Epoch number 470/1130\n",
      "Loss on training set: 14.906727517714554, loss on test set: 157.38264846689808\n",
      "Epoch number 480/1130\n",
      "Loss on training set: 12.496491924206149, loss on test set: 150.24083029712844\n",
      "Epoch number 490/1130\n",
      "Loss on training set: 15.693451847629456, loss on test set: 129.98669709104107\n",
      "Epoch number 500/1130\n",
      "Loss on training set: 18.107553111529505, loss on test set: 142.86297408902186\n",
      "Epoch number 510/1130\n",
      "Loss on training set: 15.266353091539496, loss on test set: 160.8717609505334\n",
      "Epoch number 520/1130\n",
      "Loss on training set: 15.490926269217265, loss on test set: 156.4080566940539\n",
      "Epoch number 530/1130\n",
      "Loss on training set: 13.593862993567338, loss on test set: 162.07217747103294\n",
      "Epoch number 540/1130\n",
      "Loss on training set: 12.12077403148487, loss on test set: 148.98284933938288\n",
      "Epoch number 550/1130\n",
      "Loss on training set: 13.742654527615514, loss on test set: 156.34857811367357\n",
      "Epoch number 560/1130\n",
      "Loss on training set: 14.18428614128988, loss on test set: 139.06521694751984\n",
      "Epoch number 570/1130\n",
      "Loss on training set: 14.233943800388062, loss on test set: 125.51301361145103\n",
      "Epoch number 580/1130\n",
      "Loss on training set: 13.775081808443662, loss on test set: 127.80020022467495\n",
      "Epoch number 590/1130\n",
      "Loss on training set: 13.599852395873295, loss on test set: 154.79384416410053\n",
      "Epoch number 600/1130\n",
      "Loss on training set: 12.07367537494807, loss on test set: 141.5794368891923\n",
      "Epoch number 610/1130\n",
      "Loss on training set: 12.772430992174087, loss on test set: 147.34007089475773\n",
      "Epoch number 620/1130\n",
      "Loss on training set: 15.771760986887069, loss on test set: 130.79268749031291\n",
      "Epoch number 630/1130\n",
      "Loss on training set: 12.196820862777638, loss on test set: 145.9003902400261\n",
      "Epoch number 640/1130\n",
      "Loss on training set: 11.47432434022096, loss on test set: 138.67146139002676\n",
      "Epoch number 650/1130\n",
      "Loss on training set: 12.174913273377506, loss on test set: 146.76426304736523\n",
      "Epoch number 660/1130\n",
      "Loss on training set: 12.272015554723371, loss on test set: 145.98113192017064\n",
      "Epoch number 670/1130\n",
      "Loss on training set: 11.224122143818878, loss on test set: 131.18471447912225\n",
      "Epoch number 680/1130\n",
      "Loss on training set: 15.128189915775991, loss on test set: 118.4996160425742\n",
      "Epoch number 690/1130\n",
      "Loss on training set: 14.101246709669018, loss on test set: 116.71055727137158\n",
      "Epoch number 700/1130\n",
      "Loss on training set: 12.458842152965591, loss on test set: 124.71421479045432\n",
      "Epoch number 710/1130\n",
      "Loss on training set: 17.540407983429255, loss on test set: 164.3989963731664\n",
      "Epoch number 720/1130\n",
      "Loss on training set: 12.617693322330416, loss on test set: 142.86304129741094\n",
      "Epoch number 730/1130\n",
      "Loss on training set: 11.009801341137223, loss on test set: 123.52268526221073\n",
      "Epoch number 740/1130\n",
      "Loss on training set: 13.748439424862067, loss on test set: 139.8758738136538\n",
      "Epoch number 750/1130\n",
      "Loss on training set: 11.386951757150724, loss on test set: 136.8670603854802\n",
      "Epoch number 760/1130\n",
      "Loss on training set: 11.282467611079548, loss on test set: 135.27234103231385\n",
      "Epoch number 770/1130\n",
      "Loss on training set: 11.798715246542717, loss on test set: 128.60472693830067\n",
      "Epoch number 780/1130\n",
      "Loss on training set: 13.380673041605295, loss on test set: 109.21451777849506\n",
      "Epoch number 790/1130\n",
      "Loss on training set: 15.33706527896561, loss on test set: 148.3352763311946\n",
      "Epoch number 800/1130\n",
      "Loss on training set: 12.896813611945415, loss on test set: 140.48109008754543\n",
      "Epoch number 810/1130\n",
      "Loss on training set: 11.81756283810919, loss on test set: 136.945206483861\n",
      "Epoch number 820/1130\n",
      "Loss on training set: 13.80972272949117, loss on test set: 106.20472442768376\n",
      "Epoch number 830/1130\n",
      "Loss on training set: 10.515435578285963, loss on test set: 123.27624790521752\n",
      "Epoch number 840/1130\n",
      "Loss on training set: 9.904160203285505, loss on test set: 124.74751472085401\n",
      "Epoch number 850/1130\n",
      "Loss on training set: 10.715240168178088, loss on test set: 134.34580165088116\n",
      "Epoch number 860/1130\n",
      "Loss on training set: 9.647138647979626, loss on test set: 126.35204261200319\n",
      "Epoch number 870/1130\n",
      "Loss on training set: 9.837333935692275, loss on test set: 126.4568472280207\n",
      "Epoch number 880/1130\n",
      "Loss on training set: 12.103194560396975, loss on test set: 128.95350366276426\n",
      "Epoch number 890/1130\n",
      "Loss on training set: 15.193980756744462, loss on test set: 127.38333384043665\n",
      "Epoch number 900/1130\n",
      "Loss on training set: 14.742456766953518, loss on test set: 103.89536291835248\n",
      "Epoch number 910/1130\n",
      "Loss on training set: 11.18293123699787, loss on test set: 131.48085444604538\n",
      "Epoch number 920/1130\n",
      "Loss on training set: 15.592874739185325, loss on test set: 102.7332628609187\n",
      "Epoch number 930/1130\n",
      "Loss on training set: 15.319363845299682, loss on test set: 108.1995786230719\n",
      "Epoch number 940/1130\n",
      "Loss on training set: 14.479298159097478, loss on test set: 123.9650812970924\n",
      "Epoch number 950/1130\n",
      "Loss on training set: 14.745311659784841, loss on test set: 101.3569373544192\n",
      "Epoch number 960/1130\n",
      "Loss on training set: 11.287364929012575, loss on test set: 130.1547995746869\n",
      "Epoch number 970/1130\n",
      "Loss on training set: 10.827752140103579, loss on test set: 109.39434749850919\n",
      "Epoch number 980/1130\n",
      "Loss on training set: 10.08976939601295, loss on test set: 126.5573793044894\n",
      "Epoch number 990/1130\n",
      "Loss on training set: 9.228424500020095, loss on test set: 121.23735613947534\n",
      "Epoch number 1000/1130\n",
      "Loss on training set: 14.081422621396309, loss on test set: 102.22725675329906\n",
      "Epoch number 1010/1130\n",
      "Loss on training set: 8.796880838935252, loss on test set: 117.9441285089178\n",
      "Epoch number 1020/1130\n",
      "Loss on training set: 13.905973119891637, loss on test set: 108.67697416043308\n",
      "Epoch number 1030/1130\n",
      "Loss on training set: 9.510329147218012, loss on test set: 119.45942268707341\n",
      "Epoch number 1040/1130\n",
      "Loss on training set: 10.458098361935408, loss on test set: 119.76718714857725\n",
      "Epoch number 1050/1130\n",
      "Loss on training set: 11.395099196044576, loss on test set: 125.42005092679878\n",
      "Epoch number 1060/1130\n",
      "Loss on training set: 13.742596811954831, loss on test set: 132.86972513398212\n",
      "Epoch number 1070/1130\n",
      "Loss on training set: 9.33852340872408, loss on test set: 115.1471815930991\n",
      "Epoch number 1080/1130\n",
      "Loss on training set: 10.792746378707864, loss on test set: 127.57969002860851\n",
      "Epoch number 1090/1130\n",
      "Loss on training set: 10.567963152598617, loss on test set: 124.1602688122118\n",
      "Epoch number 1100/1130\n",
      "Loss on training set: 14.58408499035742, loss on test set: 97.97646835306534\n",
      "Epoch number 1110/1130\n",
      "Loss on training set: 10.40637913426299, loss on test set: 126.36125536433123\n",
      "Epoch number 1120/1130\n",
      "Loss on training set: 15.047457285329994, loss on test set: 104.56524892579546\n",
      "Epoch number 1130/1130\n",
      "Loss on training set: 10.25909952504963, loss on test set: 125.69730970509727\n",
      "Epoch number 10/1000\n",
      "Loss on training set: 8.507599545978476, loss on test set: 110.9584211514736\n",
      "Epoch number 20/1000\n",
      "Loss on training set: 9.287626837225101, loss on test set: 103.62498287236618\n",
      "Epoch number 30/1000\n",
      "Loss on training set: 8.34901650427416, loss on test set: 112.00924909137392\n",
      "Epoch number 40/1000\n",
      "Loss on training set: 9.44058335304857, loss on test set: 105.68036988244843\n",
      "Epoch number 50/1000\n",
      "Loss on training set: 8.48334140450711, loss on test set: 114.75944348069802\n",
      "Epoch number 60/1000\n",
      "Loss on training set: 8.981174948715433, loss on test set: 107.17137486724732\n",
      "Epoch number 70/1000\n",
      "Loss on training set: 8.515291271408648, loss on test set: 107.82672908933947\n",
      "Epoch number 80/1000\n",
      "Loss on training set: 8.723114114622906, loss on test set: 104.36726297964722\n",
      "Epoch number 90/1000\n",
      "Loss on training set: 9.350204006715312, loss on test set: 105.84310434302994\n",
      "Epoch number 100/1000\n",
      "Loss on training set: 8.365972851090712, loss on test set: 106.75427049804176\n",
      "Epoch number 110/1000\n",
      "Loss on training set: 8.51434625583065, loss on test set: 105.14264469547255\n",
      "Epoch number 120/1000\n",
      "Loss on training set: 8.586169368973549, loss on test set: 103.39887952695526\n",
      "Epoch number 130/1000\n",
      "Loss on training set: 8.421374484757834, loss on test set: 107.22714243839582\n",
      "Epoch number 140/1000\n",
      "Loss on training set: 8.464703688422802, loss on test set: 113.52075041101223\n",
      "Epoch number 150/1000\n",
      "Loss on training set: 8.79966806530742, loss on test set: 102.37567287545399\n",
      "Epoch number 160/1000\n",
      "Loss on training set: 8.101589640170626, loss on test set: 108.49201684932214\n",
      "Epoch number 170/1000\n",
      "Loss on training set: 8.939316841827724, loss on test set: 104.0260360273078\n",
      "Epoch number 180/1000\n",
      "Loss on training set: 8.315122285816155, loss on test set: 109.00044994919867\n",
      "Epoch number 190/1000\n",
      "Loss on training set: 8.936011503674841, loss on test set: 116.78800377051468\n",
      "Epoch number 200/1000\n",
      "Loss on training set: 7.987067023557271, loss on test set: 106.74261362151422\n",
      "Epoch number 210/1000\n",
      "Loss on training set: 8.146258106097246, loss on test set: 110.10755818461028\n",
      "Epoch number 220/1000\n",
      "Loss on training set: 9.80735495594666, loss on test set: 118.08269248370861\n",
      "Epoch number 230/1000\n",
      "Loss on training set: 8.606647345702076, loss on test set: 105.05491694760791\n",
      "Epoch number 240/1000\n",
      "Loss on training set: 8.500901400334316, loss on test set: 111.71347182859012\n",
      "Epoch number 250/1000\n",
      "Loss on training set: 8.1294397221078, loss on test set: 105.532899259836\n",
      "Epoch number 260/1000\n",
      "Loss on training set: 8.5126177101184, loss on test set: 112.56612715181109\n",
      "Epoch number 270/1000\n",
      "Loss on training set: 8.248848441854532, loss on test set: 107.26137585495893\n",
      "Epoch number 280/1000\n",
      "Loss on training set: 8.789649846085622, loss on test set: 114.98865800754956\n",
      "Epoch number 290/1000\n",
      "Loss on training set: 7.905111641934239, loss on test set: 108.64230960185222\n",
      "Epoch number 300/1000\n",
      "Loss on training set: 8.363967920602116, loss on test set: 103.60535091589932\n",
      "Epoch number 310/1000\n",
      "Loss on training set: 8.907919168342048, loss on test set: 113.27894415154799\n",
      "Epoch number 320/1000\n",
      "Loss on training set: 8.09159506387471, loss on test set: 102.3553167345779\n",
      "Epoch number 330/1000\n",
      "Loss on training set: 8.627623880497515, loss on test set: 100.99276742257625\n",
      "Epoch number 340/1000\n",
      "Loss on training set: 8.455436498830313, loss on test set: 111.79969314824173\n",
      "Epoch number 350/1000\n",
      "Loss on training set: 8.35620421478543, loss on test set: 107.38599918761376\n",
      "Epoch number 360/1000\n",
      "Loss on training set: 8.427554393032224, loss on test set: 101.76511618509132\n",
      "Epoch number 370/1000\n",
      "Loss on training set: 7.649505544486016, loss on test set: 104.07952318367357\n",
      "Epoch number 380/1000\n",
      "Loss on training set: 7.7905229584745594, loss on test set: 105.53866081997938\n",
      "Epoch number 390/1000\n",
      "Loss on training set: 8.00561410069643, loss on test set: 106.71317011252779\n",
      "Epoch number 400/1000\n",
      "Loss on training set: 8.407953615487964, loss on test set: 99.34419383464515\n",
      "Epoch number 410/1000\n",
      "Loss on training set: 8.232992224526619, loss on test set: 110.06108271028097\n",
      "Epoch number 420/1000\n",
      "Loss on training set: 7.723500740016479, loss on test set: 106.96506635198828\n",
      "Epoch number 430/1000\n",
      "Loss on training set: 8.028761563567423, loss on test set: 109.2401023277113\n",
      "Epoch number 440/1000\n",
      "Loss on training set: 7.633218063553704, loss on test set: 104.43878394110317\n",
      "Epoch number 450/1000\n",
      "Loss on training set: 7.894515407734817, loss on test set: 99.84661666491859\n",
      "Epoch number 460/1000\n",
      "Loss on training set: 8.207485547676903, loss on test set: 98.48256188045603\n",
      "Epoch number 470/1000\n",
      "Loss on training set: 7.748018720929181, loss on test set: 107.67601213354925\n",
      "Epoch number 480/1000\n",
      "Loss on training set: 7.66440206040605, loss on test set: 107.31255546957779\n",
      "Epoch number 490/1000\n",
      "Loss on training set: 7.585477081463182, loss on test set: 104.80179500439192\n",
      "Epoch number 500/1000\n",
      "Loss on training set: 8.200370051657556, loss on test set: 111.90849615183247\n",
      "Epoch number 510/1000\n",
      "Loss on training set: 7.593506274835133, loss on test set: 106.00429253673846\n",
      "Epoch number 520/1000\n",
      "Loss on training set: 7.853146509070813, loss on test set: 99.40920294325414\n",
      "Epoch number 530/1000\n",
      "Loss on training set: 7.520673721177732, loss on test set: 101.33517130116837\n",
      "Epoch number 540/1000\n",
      "Loss on training set: 7.662256101106996, loss on test set: 106.62414468139183\n",
      "Epoch number 550/1000\n",
      "Loss on training set: 7.8520410038865265, loss on test set: 101.15338082166927\n",
      "Epoch number 560/1000\n",
      "Loss on training set: 7.938401227570401, loss on test set: 98.87164076751756\n",
      "Epoch number 570/1000\n",
      "Loss on training set: 7.4450698061275045, loss on test set: 103.76423903965413\n",
      "Epoch number 580/1000\n",
      "Loss on training set: 7.74164160584513, loss on test set: 99.25423278460902\n",
      "Epoch number 590/1000\n",
      "Loss on training set: 7.674246371994954, loss on test set: 99.75346359318414\n",
      "Epoch number 600/1000\n",
      "Loss on training set: 7.717857506233716, loss on test set: 108.81723965323134\n",
      "Epoch number 610/1000\n",
      "Loss on training set: 7.795360655945461, loss on test set: 98.4820400423384\n",
      "Epoch number 620/1000\n",
      "Loss on training set: 7.336930118001291, loss on test set: 103.22509977041753\n",
      "Epoch number 630/1000\n",
      "Loss on training set: 7.312286965723894, loss on test set: 104.15757622771234\n",
      "Epoch number 640/1000\n",
      "Loss on training set: 8.22378784565431, loss on test set: 98.40693917250265\n",
      "Epoch number 650/1000\n",
      "Loss on training set: 8.516451149155897, loss on test set: 98.68913171452613\n",
      "Epoch number 660/1000\n",
      "Loss on training set: 7.515369416811481, loss on test set: 104.6692785456717\n",
      "Epoch number 670/1000\n",
      "Loss on training set: 10.34281275901192, loss on test set: 117.35717086845048\n",
      "Epoch number 680/1000\n",
      "Loss on training set: 8.129421932852804, loss on test set: 101.0600106991797\n",
      "Epoch number 690/1000\n",
      "Loss on training set: 8.49183300921832, loss on test set: 99.16985637654007\n",
      "Epoch number 700/1000\n",
      "Loss on training set: 9.459276792073572, loss on test set: 95.14168293898614\n",
      "Epoch number 710/1000\n",
      "Loss on training set: 7.499139192071745, loss on test set: 108.00797684761471\n",
      "Epoch number 720/1000\n",
      "Loss on training set: 7.679726240409465, loss on test set: 106.31719972843061\n",
      "Epoch number 730/1000\n",
      "Loss on training set: 7.341343520894776, loss on test set: 100.93243083535613\n",
      "Epoch number 740/1000\n",
      "Loss on training set: 7.347469882110005, loss on test set: 104.81905823951588\n",
      "Epoch number 750/1000\n",
      "Loss on training set: 7.212910322901145, loss on test set: 104.4511009041773\n",
      "Epoch number 760/1000\n",
      "Loss on training set: 8.149906098970082, loss on test set: 110.57465734521844\n",
      "Epoch number 770/1000\n",
      "Loss on training set: 7.1764426530056635, loss on test set: 105.02117528189977\n",
      "Epoch number 780/1000\n",
      "Loss on training set: 9.34398806721864, loss on test set: 93.2841956592551\n",
      "Epoch number 790/1000\n",
      "Loss on training set: 7.881199811066926, loss on test set: 96.1236565881475\n",
      "Epoch number 800/1000\n",
      "Loss on training set: 7.724031137221671, loss on test set: 96.8582345989634\n",
      "Epoch number 810/1000\n",
      "Loss on training set: 7.380898004508099, loss on test set: 106.69683330284698\n",
      "Epoch number 820/1000\n",
      "Loss on training set: 7.566555154394886, loss on test set: 100.38708304716751\n",
      "Epoch number 830/1000\n",
      "Loss on training set: 7.100384326388516, loss on test set: 103.1440830418197\n",
      "Epoch number 840/1000\n",
      "Loss on training set: 7.68959797190949, loss on test set: 96.92781852431912\n",
      "Epoch number 850/1000\n",
      "Loss on training set: 7.486618065019295, loss on test set: 106.64393574465409\n",
      "Epoch number 860/1000\n",
      "Loss on training set: 7.3347828749427535, loss on test set: 105.93042589922601\n",
      "Epoch number 870/1000\n",
      "Loss on training set: 7.675424157227184, loss on test set: 106.44885693014949\n",
      "Epoch number 880/1000\n",
      "Loss on training set: 7.4809462576698165, loss on test set: 96.07457399105105\n",
      "Epoch number 890/1000\n",
      "Loss on training set: 7.0889182100473676, loss on test set: 99.16312344825408\n",
      "Epoch number 900/1000\n",
      "Loss on training set: 7.106816033260889, loss on test set: 103.91342251617462\n",
      "Epoch number 910/1000\n",
      "Loss on training set: 7.292240660163614, loss on test set: 104.29752174624646\n",
      "Epoch number 920/1000\n",
      "Loss on training set: 7.511484514914047, loss on test set: 106.84784141244225\n",
      "Epoch number 930/1000\n",
      "Loss on training set: 8.278549608370328, loss on test set: 110.7657698339357\n",
      "Epoch number 940/1000\n",
      "Loss on training set: 7.165831731688096, loss on test set: 98.3039822464445\n",
      "Epoch number 950/1000\n",
      "Loss on training set: 7.214285194631022, loss on test set: 104.50926235257094\n",
      "Epoch number 960/1000\n",
      "Loss on training set: 8.106405887825586, loss on test set: 93.36377483174296\n",
      "Epoch number 970/1000\n",
      "Loss on training set: 7.273456919552461, loss on test set: 101.75653966839519\n",
      "Epoch number 980/1000\n",
      "Loss on training set: 7.244062020403308, loss on test set: 104.39208928066846\n",
      "Epoch number 990/1000\n",
      "Loss on training set: 6.982434406367235, loss on test set: 100.7461267168027\n",
      "Epoch number 1000/1000\n",
      "Loss on training set: 8.108192209051907, loss on test set: 96.5697594413435\n",
      "Epoch number 10/700\n",
      "Loss on training set: 2148.6539174342133, loss on test set: 2609.9166305530925\n",
      "Epoch number 20/700\n",
      "Loss on training set: 1906.7425873596367, loss on test set: 2488.5211593775266\n",
      "Epoch number 30/700\n",
      "Loss on training set: 1533.0943027953058, loss on test set: 2022.1322075229846\n",
      "Epoch number 40/700\n",
      "Loss on training set: 1377.32194277718, loss on test set: 1407.865300542867\n",
      "Epoch number 50/700\n",
      "Loss on training set: 984.6436387930382, loss on test set: 1190.5722509506204\n",
      "Epoch number 60/700\n",
      "Loss on training set: 853.1244914982244, loss on test set: 957.9613465301294\n",
      "Epoch number 70/700\n",
      "Loss on training set: 1801.055876610429, loss on test set: 1506.6739531554056\n",
      "Epoch number 80/700\n",
      "Loss on training set: 873.6572224437066, loss on test set: 1341.9375236433905\n",
      "Epoch number 90/700\n",
      "Loss on training set: 2212.379449687186, loss on test set: 1969.8888139480475\n",
      "Epoch number 100/700\n",
      "Loss on training set: 1215.4391233258898, loss on test set: 1531.68272009301\n",
      "Epoch number 110/700\n",
      "Loss on training set: 1346.1632575005797, loss on test set: 1909.2247523263825\n",
      "Epoch number 120/700\n",
      "Loss on training set: 1039.3443584011275, loss on test set: 1334.7503644040414\n",
      "Epoch number 130/700\n",
      "Loss on training set: 420.9431794029364, loss on test set: 664.4880201615701\n",
      "Epoch number 140/700\n",
      "Loss on training set: 382.7830007002026, loss on test set: 670.7678357281065\n",
      "Epoch number 150/700\n",
      "Loss on training set: 273.8604687978715, loss on test set: 564.5128698025319\n",
      "Epoch number 160/700\n",
      "Loss on training set: 432.2799965829101, loss on test set: 646.6081830266726\n",
      "Epoch number 170/700\n",
      "Loss on training set: 574.5854272823832, loss on test set: 603.3686073853813\n",
      "Epoch number 180/700\n",
      "Loss on training set: 1531.9554211845025, loss on test set: 1611.1014989137234\n",
      "Epoch number 190/700\n",
      "Loss on training set: 403.18140718544225, loss on test set: 611.0079481245701\n",
      "Epoch number 200/700\n",
      "Loss on training set: 208.2135490255825, loss on test set: 519.8384300713938\n",
      "Epoch number 210/700\n",
      "Loss on training set: 1247.9286147555044, loss on test set: 1732.7336883878459\n",
      "Epoch number 220/700\n",
      "Loss on training set: 1743.3595965684951, loss on test set: 1920.8669674860982\n",
      "Epoch number 230/700\n",
      "Loss on training set: 1465.7323848379597, loss on test set: 2139.4242177321075\n",
      "Epoch number 240/700\n",
      "Loss on training set: 1456.7249887944758, loss on test set: 1605.7604104165612\n",
      "Epoch number 250/700\n",
      "Loss on training set: 1366.0834248561473, loss on test set: 1508.3931028334973\n",
      "Epoch number 260/700\n",
      "Loss on training set: 1021.1087727927916, loss on test set: 1241.1207964259456\n",
      "Epoch number 270/700\n",
      "Loss on training set: 615.530563204836, loss on test set: 780.3188082581905\n",
      "Epoch number 280/700\n",
      "Loss on training set: 1102.3308550212682, loss on test set: 893.7732028054746\n",
      "Epoch number 290/700\n",
      "Loss on training set: 335.48565743656036, loss on test set: 672.2970963403542\n",
      "Epoch number 300/700\n",
      "Loss on training set: 304.1209303750719, loss on test set: 556.9138073778701\n",
      "Epoch number 310/700\n",
      "Loss on training set: 406.50074935657096, loss on test set: 743.4153886428627\n",
      "Epoch number 320/700\n",
      "Loss on training set: 257.87161568995396, loss on test set: 522.6197979906482\n",
      "Epoch number 330/700\n",
      "Loss on training set: 303.82335980745137, loss on test set: 667.3180585739001\n",
      "Epoch number 340/700\n",
      "Loss on training set: 376.2321163071539, loss on test set: 497.0696894647324\n",
      "Epoch number 350/700\n",
      "Loss on training set: 218.10031381639948, loss on test set: 498.3050261167994\n",
      "Epoch number 360/700\n",
      "Loss on training set: 595.0758901179399, loss on test set: 612.1269978679348\n",
      "Epoch number 370/700\n",
      "Loss on training set: 494.9103746064341, loss on test set: 728.9367687047966\n",
      "Epoch number 380/700\n",
      "Loss on training set: 380.7657590084626, loss on test set: 645.5906649039366\n",
      "Epoch number 390/700\n",
      "Loss on training set: 326.04211009709445, loss on test set: 578.7951462464248\n",
      "Epoch number 400/700\n",
      "Loss on training set: 293.71306692397354, loss on test set: 455.98172623640363\n",
      "Epoch number 410/700\n",
      "Loss on training set: 205.97609820173722, loss on test set: 510.2366671321528\n",
      "Epoch number 420/700\n",
      "Loss on training set: 286.53768122915426, loss on test set: 638.312899108765\n",
      "Epoch number 430/700\n",
      "Loss on training set: 273.99078088679653, loss on test set: 548.6349428222427\n",
      "Epoch number 440/700\n",
      "Loss on training set: 444.7208583891708, loss on test set: 634.8010239171887\n",
      "Epoch number 450/700\n",
      "Loss on training set: 255.9648728359567, loss on test set: 513.434313843562\n",
      "Epoch number 460/700\n",
      "Loss on training set: 330.5502766735426, loss on test set: 606.2402159949679\n",
      "Epoch number 470/700\n",
      "Loss on training set: 450.187765775249, loss on test set: 649.4849108829741\n",
      "Epoch number 480/700\n",
      "Loss on training set: 552.2202705288041, loss on test set: 664.7696069084691\n",
      "Epoch number 490/700\n",
      "Loss on training set: 171.70654140337555, loss on test set: 446.65970777088586\n",
      "Epoch number 500/700\n",
      "Loss on training set: 356.7613592960444, loss on test set: 527.892118702079\n",
      "Epoch number 510/700\n",
      "Loss on training set: 173.79671904401567, loss on test set: 442.82765316459\n",
      "Epoch number 520/700\n",
      "Loss on training set: 293.3955746705289, loss on test set: 623.3273863493602\n",
      "Epoch number 530/700\n",
      "Loss on training set: 163.79793778096737, loss on test set: 431.6741627183305\n",
      "Epoch number 540/700\n",
      "Loss on training set: 182.92865834749773, loss on test set: 420.9537196365228\n",
      "Epoch number 550/700\n",
      "Loss on training set: 325.84258393213935, loss on test set: 610.4885263302439\n",
      "Epoch number 560/700\n",
      "Loss on training set: 214.83227264400088, loss on test set: 476.69098031992485\n",
      "Epoch number 570/700\n",
      "Loss on training set: 213.30695745654415, loss on test set: 345.04813755632995\n",
      "Epoch number 580/700\n",
      "Loss on training set: 176.21444366386558, loss on test set: 426.7049343469119\n",
      "Epoch number 590/700\n",
      "Loss on training set: 253.11758618375433, loss on test set: 329.14506639546477\n",
      "Epoch number 600/700\n",
      "Loss on training set: 266.8843603357535, loss on test set: 436.24951003529395\n",
      "Epoch number 610/700\n",
      "Loss on training set: 378.96656802859604, loss on test set: 528.3362290058158\n",
      "Epoch number 620/700\n",
      "Loss on training set: 244.23109194671193, loss on test set: 501.5290314672456\n",
      "Epoch number 630/700\n",
      "Loss on training set: 204.64978382141825, loss on test set: 495.1179453171692\n",
      "Epoch number 640/700\n",
      "Loss on training set: 209.87640605188727, loss on test set: 362.9390917415853\n",
      "Epoch number 650/700\n",
      "Loss on training set: 184.55681279283664, loss on test set: 401.6278029997678\n",
      "Epoch number 660/700\n",
      "Loss on training set: 183.01097696930066, loss on test set: 331.88015333024777\n",
      "Epoch number 670/700\n",
      "Loss on training set: 136.05564423651225, loss on test set: 349.1826476100193\n",
      "Epoch number 680/700\n",
      "Loss on training set: 330.0220059117779, loss on test set: 632.338045007928\n",
      "Epoch number 690/700\n",
      "Loss on training set: 279.96176056260896, loss on test set: 584.8693663727324\n",
      "Epoch number 700/700\n",
      "Loss on training set: 284.8183043460998, loss on test set: 462.32259475576217\n",
      "Epoch number 10/1130\n",
      "Loss on training set: 159.42780829714258, loss on test set: 405.2394691447165\n",
      "Epoch number 20/1130\n",
      "Loss on training set: 157.15402482330296, loss on test set: 379.9502068830701\n",
      "Epoch number 30/1130\n",
      "Loss on training set: 154.36004800667064, loss on test set: 390.0491114422106\n",
      "Epoch number 40/1130\n",
      "Loss on training set: 154.5870739286768, loss on test set: 369.54405787547233\n",
      "Epoch number 50/1130\n",
      "Loss on training set: 151.48852331860766, loss on test set: 377.13232625646407\n",
      "Epoch number 60/1130\n",
      "Loss on training set: 149.497338497284, loss on test set: 366.0123255067355\n",
      "Epoch number 70/1130\n",
      "Loss on training set: 159.26278320420005, loss on test set: 344.47179203111074\n",
      "Epoch number 80/1130\n",
      "Loss on training set: 147.37100544350864, loss on test set: 354.7769805738751\n",
      "Epoch number 90/1130\n",
      "Loss on training set: 146.09418714118414, loss on test set: 360.9525716093227\n",
      "Epoch number 100/1130\n",
      "Loss on training set: 144.6009180653444, loss on test set: 349.2772744725246\n",
      "Epoch number 110/1130\n",
      "Loss on training set: 146.70539541589193, loss on test set: 348.2694960672091\n",
      "Epoch number 120/1130\n",
      "Loss on training set: 143.9238466147026, loss on test set: 361.4994187725457\n",
      "Epoch number 130/1130\n",
      "Loss on training set: 142.14747675710197, loss on test set: 347.5598314097891\n",
      "Epoch number 140/1130\n",
      "Loss on training set: 142.44937012820128, loss on test set: 327.1422080030437\n",
      "Epoch number 150/1130\n",
      "Loss on training set: 141.57204866168578, loss on test set: 320.3491515594836\n",
      "Epoch number 160/1130\n",
      "Loss on training set: 139.45664295226746, loss on test set: 326.97388523547875\n",
      "Epoch number 170/1130\n",
      "Loss on training set: 139.06054506152537, loss on test set: 342.96419895396815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m ms_no_reg_build \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_shape\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_train\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneurons_num\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m64\u001B[39m, \u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m1\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mactivations\u001B[39m\u001B[38;5;124m'\u001B[39m: [ReLU(), ReLU(), ReLU(), Linear()]}\n\u001B[0;32m      2\u001B[0m ms_no_reg_fit \u001B[38;5;241m=\u001B[39m [{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m700\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.0003\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mverbose_step\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m10\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregularization_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m      3\u001B[0m                  {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1130\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.0001\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mverbose_step\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m10\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregularization_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m},\n\u001B[0;32m      4\u001B[0m                  {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_train\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_train, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1000\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.00005\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mx_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_x_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_test\u001B[39m\u001B[38;5;124m'\u001B[39m: ms_y_test, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m'\u001B[39m: mse, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mverbose_step\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m10\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mregularization_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m}]\n\u001B[1;32m----> 5\u001B[0m results_train, results_test \u001B[38;5;241m=\u001B[39m \u001B[43mcv_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuild_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mms_no_reg_build\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfit_args\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mms_no_reg_fit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(np\u001B[38;5;241m.\u001B[39mmean(results_train), np\u001B[38;5;241m.\u001B[39mmean(results_test))\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mcv_network\u001B[1;34m(seeds, build_args, fit_args)\u001B[0m\n\u001B[0;32m      7\u001B[0m last_fa \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fa \u001B[38;5;129;01min\u001B[39;00m fit_args:\n\u001B[1;32m----> 9\u001B[0m     nn\u001B[38;5;241m.\u001B[39mfit(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfa)\n\u001B[0;32m     10\u001B[0m     last_fa \u001B[38;5;241m=\u001B[39m fa\n\u001B[0;32m     11\u001B[0m nns\u001B[38;5;241m.\u001B[39mappend(nn)\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN\\network.py:179\u001B[0m, in \u001B[0;36mNN.fit\u001B[1;34m(self, x_train, y_train, batch_size, n_epochs, learning_rate, x_test, y_test, loss, metric, verbose_step, regularization_rate)\u001B[0m\n\u001B[0;32m    176\u001B[0m     y \u001B[38;5;241m=\u001B[39m y_train[idx]\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate_forward(np\u001B[38;5;241m.\u001B[39mreshape(x, (\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m--> 179\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate_backwards_with_regularization\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlambd\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mregularization_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msum_dicts(\n\u001B[0;32m    185\u001B[0m         dict1\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelta_weights, dict2\u001B[38;5;241m=\u001B[39mdelta,\n\u001B[0;32m    186\u001B[0m         dict2_multiplier\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size)\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_layers()\n",
      "File \u001B[1;32m~\\PycharmProjects\\NN\\network.py:78\u001B[0m, in \u001B[0;36mNN.propagate_backwards_with_regularization\u001B[1;34m(self, y_true, x, lambd)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers_num \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     76\u001B[0m     l2 \u001B[38;5;241m=\u001B[39m lambd \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i]\u001B[38;5;241m.\u001B[39mweights\n\u001B[0;32m     77\u001B[0m     delta[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m---> 78\u001B[0m                             \u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m \u001B[38;5;241m-\u001B[39m l2)\n\u001B[0;32m     79\u001B[0m     delta[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbiases\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39minsert(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39merrors[i])\n\u001B[0;32m     81\u001B[0m l2 \u001B[38;5;241m=\u001B[39m lambd \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mweights\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ms_no_reg_build = {'input_shape': ms_x_train.shape, 'neurons_num': [32, 64, 32, 1], 'activations': [ReLU(), ReLU(), ReLU(), Linear()]}\n",
    "ms_no_reg_fit = [{'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 700, 'learning_rate': 0.0003, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test = cv_network(build_args=ms_no_reg_build, fit_args=ms_no_reg_fit)\n",
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results.loc['multimodal-sparse-no-reg'] = [5, 'mse', np.mean(results_train), np.mean(results_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Regularyzacja L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_l2_build = {'input_shape': ms_x_train.shape, 'neurons_num': [32, 64, 32, 1], 'activations': [ReLU(), ReLU(), ReLU(), Linear()]}\n",
    "ms_l2_fit = [{'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 700, 'learning_rate': 0.0003, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01}]\n",
    "results_train, results_test = cv_network(build_args=ms_l2_build, fit_args=ms_l2_fit)\n",
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results.loc['multimodal-sparse-l2-0.01'] = [5, 'mse', round(32.016365343609294, 2), round(122.56518404311655, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_l2_build = {'input_shape': ms_x_train.shape, 'neurons_num': [32, 64, 32, 1], 'activations': [ReLU(), ReLU(), ReLU(), Linear()]}\n",
    "ms_l2_fit = [{'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 700, 'learning_rate': 0.0003, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.01}]\n",
    "results_train, results_test = cv_network(build_args=ms_l2_build, fit_args=ms_l2_fit)\n",
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_l2_build = {'input_shape': ms_x_train.shape, 'neurons_num': [32, 64, 32, 1], 'activations': [ReLU(), ReLU(), ReLU(), Linear()]}\n",
    "ms_l2_fit = [{'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 700, 'learning_rate': 0.0003, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.1},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.1},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0.1}]\n",
    "results_train, results_test = cv_network(build_args=ms_l2_build, fit_args=ms_l2_fit)\n",
    "print(np.mean(results_train, results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ms_l2_build = {'input_shape': ms_x_train.shape, 'neurons_num': [32, 64, 32, 1], 'activations': [ReLU(), ReLU(), ReLU(), Linear()]}\n",
    "ms_l2_fit = [{'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 700, 'learning_rate': 0.0003, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 2},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 2},\n",
    "                 {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 500, 'learning_rate': 0.00005, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 2},\n",
    "             {'x_train': ms_x_train, 'y_train': ms_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00001, 'x_test': ms_x_test, 'y_test': ms_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 2}]\n",
    "results_train, results_test = cv_network(build_args=ms_l2_build, fit_args=ms_l2_fit)\n",
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zbiór rings5-sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r5_x_train, r5_y_train, r5_x_test, r5_y_test = read_classification_data('rings5-sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### No regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/800\n",
      "Loss on training set: 1.9261082659796391 f_score on training set: 0.3727711855338207, loss on test set: 2.5628407785513345 f_score on test set: 0.3181552248403368\n",
      "Epoch number 20/800\n",
      "Loss on training set: 1.2221577764185154 f_score on training set: 0.5115000000000001, loss on test set: 1.8229922850691644 f_score on test set: 0.3956216420906755\n",
      "Epoch number 30/800\n",
      "Loss on training set: 1.5761421875636976 f_score on training set: 0.44269195425205654, loss on test set: 2.0530622759569277 f_score on test set: 0.38752793111203576\n",
      "Epoch number 40/800\n",
      "Loss on training set: 1.3409371959500316 f_score on training set: 0.49179621054621053, loss on test set: 2.0961867818776994 f_score on test set: 0.37244297567564594\n",
      "Epoch number 50/800\n",
      "Loss on training set: 1.2465652142745771 f_score on training set: 0.42170140060429373, loss on test set: 1.955733974903992 f_score on test set: 0.34164350675329574\n",
      "Epoch number 60/800\n",
      "Loss on training set: 1.1655248708676107 f_score on training set: 0.43277575286418324, loss on test set: 1.640453478317324 f_score on test set: 0.36391336861848145\n",
      "Epoch number 70/800\n",
      "Loss on training set: 1.0128245419849593 f_score on training set: 0.4447265078844026, loss on test set: 1.562814265849999 f_score on test set: 0.4112600909744591\n",
      "Epoch number 80/800\n",
      "Loss on training set: 1.5827011592480167 f_score on training set: 0.4286760894987271, loss on test set: 1.976673187535312 f_score on test set: 0.38789158431471277\n",
      "Epoch number 90/800\n",
      "Loss on training set: 1.090919049044604 f_score on training set: 0.4141449250114105, loss on test set: 1.6023690638673975 f_score on test set: 0.37913252145827553\n",
      "Epoch number 100/800\n",
      "Loss on training set: 1.0466768103382453 f_score on training set: 0.5032952719405532, loss on test set: 1.6486315312419138 f_score on test set: 0.3962605822042891\n",
      "Epoch number 110/800\n",
      "Loss on training set: 0.9544733337425043 f_score on training set: 0.4667022847510652, loss on test set: 1.5535908041972828 f_score on test set: 0.38230225324662226\n",
      "Epoch number 120/800\n",
      "Loss on training set: 0.9328215272083566 f_score on training set: 0.4644988669399194, loss on test set: 1.6067107794246331 f_score on test set: 0.3855846247109018\n",
      "Epoch number 130/800\n",
      "Loss on training set: 1.527175980410118 f_score on training set: 0.4064959458966744, loss on test set: 2.128763843715471 f_score on test set: 0.30701737489620967\n",
      "Epoch number 140/800\n",
      "Loss on training set: 0.782418255864918 f_score on training set: 0.4775988274060768, loss on test set: 1.5145195034028494 f_score on test set: 0.39124738810348203\n",
      "Epoch number 150/800\n",
      "Loss on training set: 1.0644612961180964 f_score on training set: 0.4691612713039079, loss on test set: 1.7436509946048915 f_score on test set: 0.39678271752533045\n",
      "Epoch number 160/800\n",
      "Loss on training set: 0.7839221641320271 f_score on training set: 0.5306228379104067, loss on test set: 1.5088373263758816 f_score on test set: 0.37988163488007004\n",
      "Epoch number 170/800\n",
      "Loss on training set: 1.045139312355562 f_score on training set: 0.4278637678442788, loss on test set: 1.68982610531206 f_score on test set: 0.35757305763326463\n",
      "Epoch number 180/800\n",
      "Loss on training set: 0.9044533971366981 f_score on training set: 0.5327707980882121, loss on test set: 1.780387834983501 f_score on test set: 0.3666592014318041\n",
      "Epoch number 190/800\n",
      "Loss on training set: 1.0881684178751332 f_score on training set: 0.4676288952442409, loss on test set: 1.4767764465550486 f_score on test set: 0.4268479102717229\n",
      "Epoch number 200/800\n",
      "Loss on training set: 0.7393712469679924 f_score on training set: 0.5324921198695717, loss on test set: 1.4615186871760284 f_score on test set: 0.4042538677135278\n",
      "Epoch number 210/800\n",
      "Loss on training set: 0.6600353038126834 f_score on training set: 0.5730004149857476, loss on test set: 1.420399084619781 f_score on test set: 0.39893060340400754\n",
      "Epoch number 220/800\n",
      "Loss on training set: 1.1602962820072398 f_score on training set: 0.4773166780225604, loss on test set: 1.8105591403537489 f_score on test set: 0.32674257466090184\n",
      "Epoch number 230/800\n",
      "Loss on training set: 0.5048663779789263 f_score on training set: 0.5649420095517893, loss on test set: 1.1542336337701553 f_score on test set: 0.42855174149350284\n",
      "Epoch number 240/800\n",
      "Loss on training set: 1.371158671417676 f_score on training set: 0.47655963313605537, loss on test set: 1.7572127247399094 f_score on test set: 0.3854470013663029\n",
      "Epoch number 250/800\n",
      "Loss on training set: 0.9852148740561347 f_score on training set: 0.5054082247495669, loss on test set: 1.6873900250150153 f_score on test set: 0.3588000043488145\n",
      "Epoch number 260/800\n",
      "Loss on training set: 0.8211445075625918 f_score on training set: 0.5172630869300628, loss on test set: 1.631083289761543 f_score on test set: 0.34303111853361307\n",
      "Epoch number 270/800\n",
      "Loss on training set: 1.2896229421165117 f_score on training set: 0.424599823717254, loss on test set: 1.9432939669935971 f_score on test set: 0.2968539024779721\n",
      "Epoch number 280/800\n",
      "Loss on training set: 0.616095168664061 f_score on training set: 0.6754117277342666, loss on test set: 1.4114665069375083 f_score on test set: 0.43151056244242875\n",
      "Epoch number 290/800\n",
      "Loss on training set: 0.8720617321053987 f_score on training set: 0.5530091166363594, loss on test set: 1.4869079791516115 f_score on test set: 0.39044112026097577\n",
      "Epoch number 300/800\n",
      "Loss on training set: 0.5914456416072822 f_score on training set: 0.6361925502807602, loss on test set: 1.149861223008482 f_score on test set: 0.46381231895876285\n",
      "Epoch number 310/800\n",
      "Loss on training set: 0.629845524919492 f_score on training set: 0.6275140681893413, loss on test set: 1.3174090548513024 f_score on test set: 0.3949625614825364\n",
      "Epoch number 320/800\n",
      "Loss on training set: 0.4696595151961434 f_score on training set: 0.6180259193357933, loss on test set: 1.304536175016647 f_score on test set: 0.4329055884261178\n",
      "Epoch number 330/800\n",
      "Loss on training set: 0.7119364117126122 f_score on training set: 0.5958597938552238, loss on test set: 1.5394370839480733 f_score on test set: 0.4073678791556902\n",
      "Epoch number 340/800\n",
      "Loss on training set: 0.5488480301785971 f_score on training set: 0.6531941523907556, loss on test set: 1.2745049354024736 f_score on test set: 0.4370587168101653\n",
      "Epoch number 350/800\n",
      "Loss on training set: 0.9397868478847615 f_score on training set: 0.5370150111326581, loss on test set: 1.481648740687777 f_score on test set: 0.4296517475565467\n",
      "Epoch number 360/800\n",
      "Loss on training set: 0.4118278645441575 f_score on training set: 0.6035167958448041, loss on test set: 1.182938015420767 f_score on test set: 0.46371999236296807\n",
      "Epoch number 370/800\n",
      "Loss on training set: 0.6536605662688058 f_score on training set: 0.6499911352114053, loss on test set: 1.5282308439262022 f_score on test set: 0.42018050795915685\n",
      "Epoch number 380/800\n",
      "Loss on training set: 0.7424257689755589 f_score on training set: 0.6046634076321376, loss on test set: 1.3144206454059528 f_score on test set: 0.4559022589266062\n",
      "Epoch number 390/800\n",
      "Loss on training set: 0.5234601934353563 f_score on training set: 0.6365020139919441, loss on test set: 1.2201717753038985 f_score on test set: 0.4270354608293306\n",
      "Epoch number 400/800\n",
      "Loss on training set: 0.692776646715889 f_score on training set: 0.5867784264368704, loss on test set: 1.1388638894602696 f_score on test set: 0.4473448793073789\n",
      "Epoch number 410/800\n",
      "Loss on training set: 0.6643367323288104 f_score on training set: 0.5903447672222376, loss on test set: 1.312700040813035 f_score on test set: 0.4182484323420638\n",
      "Epoch number 420/800\n",
      "Loss on training set: 0.6672055309921749 f_score on training set: 0.5510242653762536, loss on test set: 1.7295012010476836 f_score on test set: 0.28331608419279736\n",
      "Epoch number 430/800\n",
      "Loss on training set: 1.0224932623523422 f_score on training set: 0.5546612288078102, loss on test set: 1.5162075112216211 f_score on test set: 0.414782682813823\n",
      "Epoch number 440/800\n",
      "Loss on training set: 0.5549492045511707 f_score on training set: 0.6534268943907497, loss on test set: 1.2431193695404676 f_score on test set: 0.43901551780957265\n",
      "Epoch number 450/800\n",
      "Loss on training set: 0.5488246320402288 f_score on training set: 0.6570500213901346, loss on test set: 1.3569397717046758 f_score on test set: 0.40809461028819466\n",
      "Epoch number 460/800\n",
      "Loss on training set: 0.5028797398326165 f_score on training set: 0.6375500407335322, loss on test set: 1.1326258613081128 f_score on test set: 0.4641345695686964\n",
      "Epoch number 470/800\n",
      "Loss on training set: 0.5930034890886104 f_score on training set: 0.6367559684347218, loss on test set: 1.5001590576608814 f_score on test set: 0.4139201456876984\n",
      "Epoch number 480/800\n",
      "Loss on training set: 0.6010733184621008 f_score on training set: 0.6473824395124262, loss on test set: 1.23690249965043 f_score on test set: 0.4421762393099056\n",
      "Epoch number 490/800\n",
      "Loss on training set: 0.5054266096924703 f_score on training set: 0.6301961425302387, loss on test set: 1.1760962485523392 f_score on test set: 0.46600096061183927\n",
      "Epoch number 500/800\n",
      "Loss on training set: 0.3357454910294717 f_score on training set: 0.6483757382234129, loss on test set: 1.206603834762889 f_score on test set: 0.4008945909105113\n",
      "Epoch number 510/800\n",
      "Loss on training set: 0.4563040000144172 f_score on training set: 0.6628039405688998, loss on test set: 1.1154557498430078 f_score on test set: 0.44722513417858195\n",
      "Epoch number 520/800\n",
      "Loss on training set: 0.5127112272258127 f_score on training set: 0.6549042171243338, loss on test set: 1.2675504766755257 f_score on test set: 0.4332073674606912\n",
      "Epoch number 530/800\n",
      "Loss on training set: 0.7146013090136262 f_score on training set: 0.61483251714373, loss on test set: 1.3438491747673917 f_score on test set: 0.44068785029542457\n",
      "Epoch number 540/800\n",
      "Loss on training set: 0.5539060421946699 f_score on training set: 0.5606567191673575, loss on test set: 1.3365715247949095 f_score on test set: 0.3850349706685671\n",
      "Epoch number 550/800\n",
      "Loss on training set: 0.4961413999006286 f_score on training set: 0.6506686658296791, loss on test set: 1.3807573764628311 f_score on test set: 0.4248042141349143\n",
      "Epoch number 560/800\n",
      "Loss on training set: 0.42684180642440933 f_score on training set: 0.6545779378944484, loss on test set: 1.1415963143586894 f_score on test set: 0.4294404031335728\n",
      "Epoch number 570/800\n",
      "Loss on training set: 0.3567634951761339 f_score on training set: 0.6620712620712621, loss on test set: 1.1073407258206185 f_score on test set: 0.44269779202583087\n",
      "Epoch number 580/800\n",
      "Loss on training set: 0.5074013563972681 f_score on training set: 0.679414742593576, loss on test set: 1.172617848999948 f_score on test set: 0.44816849241579393\n",
      "Epoch number 590/800\n",
      "Loss on training set: 0.27277081509040024 f_score on training set: 0.7253650878548364, loss on test set: 1.0625139156793466 f_score on test set: 0.4384262454004855\n",
      "Epoch number 600/800\n",
      "Loss on training set: 0.5354059548369269 f_score on training set: 0.6407681267494157, loss on test set: 1.1748753099848603 f_score on test set: 0.4363638923479747\n",
      "Epoch number 610/800\n",
      "Loss on training set: 0.5454666263398115 f_score on training set: 0.6510055827764464, loss on test set: 1.0608900683889397 f_score on test set: 0.4839633834840579\n",
      "Epoch number 620/800\n",
      "Loss on training set: 0.3395604792018532 f_score on training set: 0.7465687679415298, loss on test set: 0.9510014028532178 f_score on test set: 0.501866389866637\n",
      "Epoch number 630/800\n",
      "Loss on training set: 0.5416824412205548 f_score on training set: 0.6928431251365044, loss on test set: 1.1295894122432584 f_score on test set: 0.4757246690415631\n",
      "Epoch number 640/800\n",
      "Loss on training set: 0.7047410993308226 f_score on training set: 0.6063314275210719, loss on test set: 1.304707552735891 f_score on test set: 0.44026759471456367\n",
      "Epoch number 650/800\n",
      "Loss on training set: 0.4071802418346786 f_score on training set: 0.6588362377061844, loss on test set: 1.0341025573363807 f_score on test set: 0.4535981286698428\n",
      "Epoch number 660/800\n",
      "Loss on training set: 0.716739235252141 f_score on training set: 0.6246720160121769, loss on test set: 1.4475059867012736 f_score on test set: 0.42589866503101403\n",
      "Epoch number 670/800\n",
      "Loss on training set: 0.5013385241571924 f_score on training set: 0.6229554277275637, loss on test set: 1.173912457229685 f_score on test set: 0.4378458974465102\n",
      "Epoch number 680/800\n",
      "Loss on training set: 0.637524239937854 f_score on training set: 0.6369978631112021, loss on test set: 1.3294678326898206 f_score on test set: 0.4507124881115996\n",
      "Epoch number 690/800\n",
      "Loss on training set: 0.29227682951441664 f_score on training set: 0.7354176146859075, loss on test set: 1.0813148930287564 f_score on test set: 0.42770727276464393\n",
      "Epoch number 700/800\n",
      "Loss on training set: 0.5619744264594768 f_score on training set: 0.6636748807120418, loss on test set: 1.4624083942058663 f_score on test set: 0.3547243072831722\n",
      "Epoch number 710/800\n",
      "Loss on training set: 0.7177520990503228 f_score on training set: 0.5759938199068634, loss on test set: 1.3773650843507204 f_score on test set: 0.3958613232503529\n",
      "Epoch number 720/800\n",
      "Loss on training set: 0.7278276666535027 f_score on training set: 0.5635730071401636, loss on test set: 1.2081299528621205 f_score on test set: 0.4476324094648772\n",
      "Epoch number 730/800\n",
      "Loss on training set: 0.311273611746824 f_score on training set: 0.6778349643951148, loss on test set: 1.1858216284043777 f_score on test set: 0.3993168564319934\n",
      "Epoch number 740/800\n",
      "Loss on training set: 0.4301300619109727 f_score on training set: 0.6452505382715624, loss on test set: 1.1679094227112254 f_score on test set: 0.41982245747225727\n",
      "Epoch number 750/800\n",
      "Loss on training set: 0.3911526296600941 f_score on training set: 0.701670882227199, loss on test set: 1.122506318436978 f_score on test set: 0.4722718379753503\n",
      "Epoch number 760/800\n",
      "Loss on training set: 0.4452604083433233 f_score on training set: 0.6753815533040097, loss on test set: 1.1358212347941747 f_score on test set: 0.45109741192633296\n",
      "Epoch number 770/800\n",
      "Loss on training set: 0.4143964157886264 f_score on training set: 0.6849221934586909, loss on test set: 1.2598340309663225 f_score on test set: 0.3933956616904642\n",
      "Epoch number 780/800\n",
      "Loss on training set: 0.4745733944514854 f_score on training set: 0.638685467378167, loss on test set: 1.0119564379441466 f_score on test set: 0.46102454196311743\n",
      "Epoch number 790/800\n",
      "Loss on training set: 0.33177816355503525 f_score on training set: 0.743248214014288, loss on test set: 1.0907523606931575 f_score on test set: 0.4708151609956629\n",
      "Epoch number 800/800\n",
      "Loss on training set: 0.32518442785839263 f_score on training set: 0.6909356192276108, loss on test set: 1.0207279679032857 f_score on test set: 0.4550115550561015\n",
      "Epoch number 10/400\n",
      "Loss on training set: 0.17856647350431856 f_score on training set: 0.7874653490708389, loss on test set: 0.8966274034368525 f_score on test set: 0.486535105900058\n",
      "Epoch number 20/400\n",
      "Loss on training set: 0.17973515962521872 f_score on training set: 0.8077304607537167, loss on test set: 0.9444876620312449 f_score on test set: 0.48063337048763666\n",
      "Epoch number 30/400\n",
      "Loss on training set: 0.1653628868605287 f_score on training set: 0.818155138589079, loss on test set: 0.9326521886100915 f_score on test set: 0.48209236669971856\n",
      "Epoch number 40/400\n",
      "Loss on training set: 0.15639266284773365 f_score on training set: 0.8584578129094017, loss on test set: 0.8632348145279921 f_score on test set: 0.508717441791132\n",
      "Epoch number 50/400\n",
      "Loss on training set: 0.16328934707827886 f_score on training set: 0.8515651518453424, loss on test set: 0.9033511246026299 f_score on test set: 0.4800103335275599\n",
      "Epoch number 60/400\n",
      "Loss on training set: 0.15753095511219728 f_score on training set: 0.854314270568004, loss on test set: 0.8840295830879833 f_score on test set: 0.4932902678121157\n",
      "Epoch number 70/400\n",
      "Loss on training set: 0.1674888148346892 f_score on training set: 0.8027724867724868, loss on test set: 0.8381738297705886 f_score on test set: 0.5171173713342712\n",
      "Epoch number 80/400\n",
      "Loss on training set: 0.1558494867854039 f_score on training set: 0.8421827972690825, loss on test set: 0.8725698000144831 f_score on test set: 0.48444340392358753\n",
      "Epoch number 90/400\n",
      "Loss on training set: 0.16241450781043912 f_score on training set: 0.8434890226844248, loss on test set: 0.8666401609376693 f_score on test set: 0.49974317601001667\n",
      "Epoch number 100/400\n",
      "Loss on training set: 0.1588654621556011 f_score on training set: 0.8553182112005641, loss on test set: 0.8824278189639874 f_score on test set: 0.48211493713135317\n",
      "Epoch number 110/400\n",
      "Loss on training set: 0.15353624168476163 f_score on training set: 0.8601273365859248, loss on test set: 0.8842961148514525 f_score on test set: 0.4983716649806148\n",
      "Epoch number 120/400\n",
      "Loss on training set: 0.1623564686896883 f_score on training set: 0.838663101604278, loss on test set: 0.9024000999553747 f_score on test set: 0.4836637144415989\n",
      "Epoch number 130/400\n",
      "Loss on training set: 0.15075681377987502 f_score on training set: 0.8873351891915644, loss on test set: 0.8665243447834183 f_score on test set: 0.4936901967406137\n",
      "Epoch number 140/400\n",
      "Loss on training set: 0.14709324670912666 f_score on training set: 0.8570526604641217, loss on test set: 0.8731914815789098 f_score on test set: 0.4953512964443214\n",
      "Epoch number 150/400\n",
      "Loss on training set: 0.1598613020174813 f_score on training set: 0.8146969243433361, loss on test set: 0.8578108444465197 f_score on test set: 0.5079081881125749\n",
      "Epoch number 160/400\n",
      "Loss on training set: 0.1500794866823459 f_score on training set: 0.8725201586164891, loss on test set: 0.8600688813403407 f_score on test set: 0.5005308516881505\n",
      "Epoch number 170/400\n",
      "Loss on training set: 0.16546776591364382 f_score on training set: 0.8147324477858566, loss on test set: 0.8622509263139344 f_score on test set: 0.4942923057560267\n",
      "Epoch number 180/400\n",
      "Loss on training set: 0.15148280537852038 f_score on training set: 0.8527826093164547, loss on test set: 0.877498669495845 f_score on test set: 0.4952529108150916\n",
      "Epoch number 190/400\n",
      "Loss on training set: 0.15399438712770339 f_score on training set: 0.8617958677327549, loss on test set: 0.8597535922876801 f_score on test set: 0.49359963304129284\n",
      "Epoch number 200/400\n",
      "Loss on training set: 0.15407478717320458 f_score on training set: 0.8361430093068722, loss on test set: 0.8471748536776513 f_score on test set: 0.5041815893596651\n",
      "Epoch number 210/400\n",
      "Loss on training set: 0.15884285684860375 f_score on training set: 0.8494131462407323, loss on test set: 0.8941928988728227 f_score on test set: 0.48348008392634667\n",
      "Epoch number 220/400\n",
      "Loss on training set: 0.16082275230055873 f_score on training set: 0.8320326750559309, loss on test set: 0.8822143147573017 f_score on test set: 0.49372168321477417\n",
      "Epoch number 230/400\n",
      "Loss on training set: 0.14759398203861102 f_score on training set: 0.8669587594073773, loss on test set: 0.8680970927520688 f_score on test set: 0.5052767780790582\n",
      "Epoch number 240/400\n",
      "Loss on training set: 0.1584751876705904 f_score on training set: 0.8222027471037882, loss on test set: 0.8537428277053372 f_score on test set: 0.5034252727460278\n",
      "Epoch number 250/400\n",
      "Loss on training set: 0.14495720094968534 f_score on training set: 0.8678307768299358, loss on test set: 0.8554254927369952 f_score on test set: 0.5021147294514379\n",
      "Epoch number 260/400\n",
      "Loss on training set: 0.15668163233866625 f_score on training set: 0.8311081504544484, loss on test set: 0.8809856119516507 f_score on test set: 0.4852102803567215\n",
      "Epoch number 270/400\n",
      "Loss on training set: 0.1607857716789784 f_score on training set: 0.8212466548468413, loss on test set: 0.8731045921911927 f_score on test set: 0.4773042950187134\n",
      "Epoch number 280/400\n",
      "Loss on training set: 0.15479328095998612 f_score on training set: 0.8589581914695674, loss on test set: 0.8898489109444662 f_score on test set: 0.49351322191403446\n",
      "Epoch number 290/400\n",
      "Loss on training set: 0.15820628084902189 f_score on training set: 0.8453895789513862, loss on test set: 0.8603653803351963 f_score on test set: 0.5057427245782755\n",
      "Epoch number 300/400\n",
      "Loss on training set: 0.14750298332941267 f_score on training set: 0.8640688611102185, loss on test set: 0.8517239729264394 f_score on test set: 0.5031936710499448\n",
      "Epoch number 310/400\n",
      "Loss on training set: 0.16126316855663703 f_score on training set: 0.839580808369207, loss on test set: 0.8973105323488025 f_score on test set: 0.48141188227297493\n",
      "Epoch number 320/400\n",
      "Loss on training set: 0.1464976458931988 f_score on training set: 0.8529470843362676, loss on test set: 0.8700053744127921 f_score on test set: 0.5018914618939961\n",
      "Epoch number 330/400\n",
      "Loss on training set: 0.15127313610674636 f_score on training set: 0.8441735174395402, loss on test set: 0.880076897616859 f_score on test set: 0.48768472566309534\n",
      "Epoch number 340/400\n",
      "Loss on training set: 0.15686972273706032 f_score on training set: 0.8544531635100809, loss on test set: 0.8967739918964238 f_score on test set: 0.47926758058241964\n",
      "Epoch number 350/400\n",
      "Loss on training set: 0.15262360568844352 f_score on training set: 0.85458564326696, loss on test set: 0.8638332415154235 f_score on test set: 0.49804469918746774\n",
      "Epoch number 360/400\n",
      "Loss on training set: 0.15235018489059765 f_score on training set: 0.8485821543292806, loss on test set: 0.8891753547217952 f_score on test set: 0.4849309134847823\n",
      "Epoch number 370/400\n",
      "Loss on training set: 0.14618257650246197 f_score on training set: 0.8537056364217074, loss on test set: 0.8485302587174152 f_score on test set: 0.49996813579681637\n",
      "Epoch number 380/400\n",
      "Loss on training set: 0.1512483595359222 f_score on training set: 0.8449336870026525, loss on test set: 0.8838265771157785 f_score on test set: 0.48427647486034736\n",
      "Epoch number 390/400\n",
      "Loss on training set: 0.15631158076078783 f_score on training set: 0.8459578083687673, loss on test set: 0.8400244185384412 f_score on test set: 0.5148310741474179\n",
      "Epoch number 400/400\n",
      "Loss on training set: 0.14732443908046133 f_score on training set: 0.850210970464135, loss on test set: 0.860112556814599 f_score on test set: 0.4956558577966173\n",
      "Epoch number 10/800\n",
      "Loss on training set: 2.4222065439466323 f_score on training set: 0.40841991341991346, loss on test set: 3.001462407851293 f_score on test set: 0.3062675722294485\n",
      "Epoch number 20/800\n",
      "Loss on training set: 1.305808448415563 f_score on training set: 0.5088447410973324, loss on test set: 1.9270527493957346 f_score on test set: 0.40616578843634926\n",
      "Epoch number 30/800\n",
      "Loss on training set: 1.5389879306150382 f_score on training set: 0.4068806462331831, loss on test set: 2.056055745624685 f_score on test set: 0.3501746157311692\n",
      "Epoch number 40/800\n",
      "Loss on training set: 1.4122318242327978 f_score on training set: 0.4754031063299004, loss on test set: 2.3679983778113702 f_score on test set: 0.34745341916763517\n",
      "Epoch number 50/800\n",
      "Loss on training set: 1.7423623119773384 f_score on training set: 0.4180233261084325, loss on test set: 2.32164062646215 f_score on test set: 0.3197074523090665\n",
      "Epoch number 60/800\n",
      "Loss on training set: 1.1045581022615085 f_score on training set: 0.5696870857402168, loss on test set: 1.8879387062777193 f_score on test set: 0.38792544433035214\n",
      "Epoch number 70/800\n",
      "Loss on training set: 0.8731949975117239 f_score on training set: 0.5893526545887153, loss on test set: 1.7470505819380104 f_score on test set: 0.39664103451728583\n",
      "Epoch number 80/800\n",
      "Loss on training set: 0.8332078683418299 f_score on training set: 0.6192825946467981, loss on test set: 1.7698149647892438 f_score on test set: 0.376619007316749\n",
      "Epoch number 90/800\n",
      "Loss on training set: 1.1095864961278066 f_score on training set: 0.5260458869719954, loss on test set: 1.6577833677274052 f_score on test set: 0.43581384002141194\n",
      "Epoch number 100/800\n",
      "Loss on training set: 0.9742223560219018 f_score on training set: 0.6030432598129457, loss on test set: 1.842301814883641 f_score on test set: 0.4006402117806794\n",
      "Epoch number 110/800\n",
      "Loss on training set: 0.8983970377058383 f_score on training set: 0.572459515792849, loss on test set: 1.7522277708264613 f_score on test set: 0.3867232059671261\n",
      "Epoch number 120/800\n",
      "Loss on training set: 1.5264015953381214 f_score on training set: 0.4626903271224493, loss on test set: 2.166114566538948 f_score on test set: 0.37182845545618665\n",
      "Epoch number 130/800\n",
      "Loss on training set: 1.109832432719164 f_score on training set: 0.5415192179735097, loss on test set: 1.8151698405647216 f_score on test set: 0.38585234651529987\n",
      "Epoch number 140/800\n",
      "Loss on training set: 1.671603110469753 f_score on training set: 0.4592754146360034, loss on test set: 2.2783935993327966 f_score on test set: 0.36229142187656876\n",
      "Epoch number 150/800\n",
      "Loss on training set: 1.0193534453487159 f_score on training set: 0.5560760667903525, loss on test set: 1.938797329934281 f_score on test set: 0.3840228408349237\n",
      "Epoch number 160/800\n",
      "Loss on training set: 0.9086934301368995 f_score on training set: 0.5865769201695974, loss on test set: 1.7721163703403804 f_score on test set: 0.3647956959094905\n",
      "Epoch number 170/800\n",
      "Loss on training set: 0.8618621887532918 f_score on training set: 0.5495311523725317, loss on test set: 1.445726689532087 f_score on test set: 0.4307971920044045\n",
      "Epoch number 180/800\n",
      "Loss on training set: 0.7154181102148843 f_score on training set: 0.5971721753354188, loss on test set: 1.7907499180397015 f_score on test set: 0.36825244802839646\n",
      "Epoch number 190/800\n",
      "Loss on training set: 0.6011347519181093 f_score on training set: 0.6560779665586572, loss on test set: 1.5062387641124069 f_score on test set: 0.4142324357348669\n",
      "Epoch number 200/800\n",
      "Loss on training set: 0.9616888254194059 f_score on training set: 0.5678353562385677, loss on test set: 1.486586026692242 f_score on test set: 0.4443456957729078\n",
      "Epoch number 210/800\n",
      "Loss on training set: 0.8981251031729612 f_score on training set: 0.552521592738984, loss on test set: 1.7389329197097416 f_score on test set: 0.38837638428248916\n",
      "Epoch number 220/800\n",
      "Loss on training set: 1.0292088052317474 f_score on training set: 0.5108768258261159, loss on test set: 1.7964889566385915 f_score on test set: 0.37179869967636586\n",
      "Epoch number 230/800\n",
      "Loss on training set: 0.6490013851190296 f_score on training set: 0.6549800563070047, loss on test set: 1.592452008303856 f_score on test set: 0.4074253666340704\n",
      "Epoch number 240/800\n",
      "Loss on training set: 0.8470892539894678 f_score on training set: 0.6754360070273471, loss on test set: 1.7523677940084883 f_score on test set: 0.41808454008276613\n",
      "Epoch number 250/800\n",
      "Loss on training set: 0.6970088804092717 f_score on training set: 0.6465926429759625, loss on test set: 1.4986530499481379 f_score on test set: 0.431874261471636\n",
      "Epoch number 260/800\n",
      "Loss on training set: 0.6693006889904979 f_score on training set: 0.6501722000525803, loss on test set: 1.5586412371834253 f_score on test set: 0.4333253085100257\n",
      "Epoch number 270/800\n",
      "Loss on training set: 0.686342290192851 f_score on training set: 0.5819486468196486, loss on test set: 1.4450902398885828 f_score on test set: 0.4396000078602074\n",
      "Epoch number 280/800\n",
      "Loss on training set: 1.1099495109279907 f_score on training set: 0.5496614542663876, loss on test set: 1.9520364520495255 f_score on test set: 0.315985108270289\n",
      "Epoch number 290/800\n",
      "Loss on training set: 0.6758837962952595 f_score on training set: 0.711153112283474, loss on test set: 1.5805334946949001 f_score on test set: 0.4337043115261361\n",
      "Epoch number 300/800\n",
      "Loss on training set: 0.8118629040154506 f_score on training set: 0.6100914112135619, loss on test set: 1.5532955232953314 f_score on test set: 0.4123746661386237\n",
      "Epoch number 310/800\n",
      "Loss on training set: 0.6285841080310404 f_score on training set: 0.702749076229865, loss on test set: 1.609481399244694 f_score on test set: 0.43745456948460687\n",
      "Epoch number 320/800\n",
      "Loss on training set: 0.9560248329989623 f_score on training set: 0.5523195157480871, loss on test set: 1.8639961230865305 f_score on test set: 0.37450411360741237\n",
      "Epoch number 330/800\n",
      "Loss on training set: 0.5147489879946484 f_score on training set: 0.61385121736657, loss on test set: 1.3815360150269074 f_score on test set: 0.4506226099681604\n",
      "Epoch number 340/800\n",
      "Loss on training set: 0.6855250770795459 f_score on training set: 0.6654024495767209, loss on test set: 1.526127249581384 f_score on test set: 0.4522306068095722\n",
      "Epoch number 350/800\n",
      "Loss on training set: 1.1961280174907183 f_score on training set: 0.5377606928667281, loss on test set: 1.8904976685602672 f_score on test set: 0.42714579569754324\n",
      "Epoch number 360/800\n",
      "Loss on training set: 0.5075854920312306 f_score on training set: 0.6664107901395888, loss on test set: 1.3740976475075997 f_score on test set: 0.4371206923070457\n",
      "Epoch number 370/800\n",
      "Loss on training set: 0.44048998181425114 f_score on training set: 0.7108507212531981, loss on test set: 1.2582528122285745 f_score on test set: 0.4726418114457323\n",
      "Epoch number 380/800\n",
      "Loss on training set: 0.8405644029288833 f_score on training set: 0.5881806578847417, loss on test set: 1.6347398534119686 f_score on test set: 0.40814616108144897\n",
      "Epoch number 390/800\n",
      "Loss on training set: 0.7422268006835718 f_score on training set: 0.7072434160239038, loss on test set: 1.4414008710368746 f_score on test set: 0.44921799605012236\n",
      "Epoch number 400/800\n",
      "Loss on training set: 0.5938393195549061 f_score on training set: 0.7054265960835664, loss on test set: 1.529482317210644 f_score on test set: 0.4394179256125067\n",
      "Epoch number 410/800\n",
      "Loss on training set: 0.7188258494635482 f_score on training set: 0.6509586970608892, loss on test set: 1.7267424299537693 f_score on test set: 0.39588496556268105\n",
      "Epoch number 420/800\n",
      "Loss on training set: 0.7694179200944153 f_score on training set: 0.6312782344731122, loss on test set: 1.7742772662721698 f_score on test set: 0.3945915864177357\n",
      "Epoch number 430/800\n",
      "Loss on training set: 0.8148314841354577 f_score on training set: 0.6234223398438778, loss on test set: 1.4325612950669049 f_score on test set: 0.4585897270714172\n",
      "Epoch number 440/800\n",
      "Loss on training set: 0.5126080385853717 f_score on training set: 0.6636172105297078, loss on test set: 1.546068400909694 f_score on test set: 0.3530170327029231\n",
      "Epoch number 450/800\n",
      "Loss on training set: 0.8333888574214673 f_score on training set: 0.566228999486906, loss on test set: 1.7491684876485083 f_score on test set: 0.37633092299665133\n",
      "Epoch number 460/800\n",
      "Loss on training set: 0.7943506582375014 f_score on training set: 0.6366876020154709, loss on test set: 1.561373834113294 f_score on test set: 0.4312246549990402\n",
      "Epoch number 470/800\n",
      "Loss on training set: 0.6792617638225797 f_score on training set: 0.6255343056543357, loss on test set: 1.62226000468455 f_score on test set: 0.43446955962260214\n",
      "Epoch number 480/800\n",
      "Loss on training set: 0.7265507407535208 f_score on training set: 0.6705039297272307, loss on test set: 1.4456931757699383 f_score on test set: 0.46905926113607666\n",
      "Epoch number 490/800\n",
      "Loss on training set: 0.39782149411381806 f_score on training set: 0.7767261961592834, loss on test set: 1.2994451177882111 f_score on test set: 0.46902031752842743\n",
      "Epoch number 500/800\n",
      "Loss on training set: 0.4952396111054351 f_score on training set: 0.7147233707833555, loss on test set: 1.302519576270007 f_score on test set: 0.4733825586507578\n",
      "Epoch number 510/800\n",
      "Loss on training set: 0.5428157650075423 f_score on training set: 0.6012190381295387, loss on test set: 1.2495296105027396 f_score on test set: 0.47079756555728997\n",
      "Epoch number 520/800\n",
      "Loss on training set: 0.7517957399607114 f_score on training set: 0.6117170680939638, loss on test set: 1.678631702836139 f_score on test set: 0.431477608688947\n",
      "Epoch number 530/800\n",
      "Loss on training set: 0.4693408618769404 f_score on training set: 0.6939392728633839, loss on test set: 1.1695754332311836 f_score on test set: 0.4890889868514073\n",
      "Epoch number 540/800\n",
      "Loss on training set: 0.5487957664501786 f_score on training set: 0.7287613509361718, loss on test set: 1.4174905889733949 f_score on test set: 0.46512807667939954\n",
      "Epoch number 550/800\n",
      "Loss on training set: 0.5787565463609458 f_score on training set: 0.6440160414298345, loss on test set: 1.6138002086629508 f_score on test set: 0.41902435336017535\n",
      "Epoch number 560/800\n",
      "Loss on training set: 0.44071146191235216 f_score on training set: 0.7165503875968992, loss on test set: 1.3862354147909537 f_score on test set: 0.4453720823542365\n",
      "Epoch number 570/800\n",
      "Loss on training set: 0.5502250276146273 f_score on training set: 0.6802715861625843, loss on test set: 1.4752355119293346 f_score on test set: 0.4294193717846563\n",
      "Epoch number 580/800\n",
      "Loss on training set: 0.5964683147045438 f_score on training set: 0.6462399913703917, loss on test set: 1.3706970589212684 f_score on test set: 0.44318501183828335\n",
      "Epoch number 590/800\n",
      "Loss on training set: 0.41632376119990216 f_score on training set: 0.7054102640644673, loss on test set: 1.2111852578129674 f_score on test set: 0.5013721324058353\n",
      "Epoch number 600/800\n",
      "Loss on training set: 0.8032889069366365 f_score on training set: 0.5417267714107001, loss on test set: 1.575070925206531 f_score on test set: 0.40965970391025336\n",
      "Epoch number 610/800\n",
      "Loss on training set: 0.4992699987629795 f_score on training set: 0.6718143067503682, loss on test set: 1.1497128300624826 f_score on test set: 0.473460093947385\n",
      "Epoch number 620/800\n",
      "Loss on training set: 0.6619576340763608 f_score on training set: 0.6547620351361314, loss on test set: 1.5078647445836122 f_score on test set: 0.42453175300159296\n",
      "Epoch number 630/800\n",
      "Loss on training set: 0.7876140188996579 f_score on training set: 0.6149598229671147, loss on test set: 1.362528998852826 f_score on test set: 0.44947962244053036\n",
      "Epoch number 640/800\n",
      "Loss on training set: 0.9068413818491199 f_score on training set: 0.5870897559319043, loss on test set: 1.658018296937392 f_score on test set: 0.3859037257938463\n",
      "Epoch number 650/800\n",
      "Loss on training set: 0.43627106102015795 f_score on training set: 0.6225337038179437, loss on test set: 1.2577763711846857 f_score on test set: 0.42348513660267045\n",
      "Epoch number 660/800\n",
      "Loss on training set: 0.5113449604269172 f_score on training set: 0.7238494342096345, loss on test set: 1.4086573286046888 f_score on test set: 0.4211456726882328\n",
      "Epoch number 670/800\n",
      "Loss on training set: 1.0609001032078405 f_score on training set: 0.486752352386025, loss on test set: 1.8691497907961747 f_score on test set: 0.35665930276113694\n",
      "Epoch number 680/800\n",
      "Loss on training set: 0.4218734389208418 f_score on training set: 0.7054432247414704, loss on test set: 1.3323471599006231 f_score on test set: 0.4124502073530026\n",
      "Epoch number 690/800\n",
      "Loss on training set: 0.4865209773648488 f_score on training set: 0.7251434885729097, loss on test set: 1.26858728025669 f_score on test set: 0.46280950568133444\n",
      "Epoch number 700/800\n",
      "Loss on training set: 0.39137785884721327 f_score on training set: 0.7283415016411913, loss on test set: 1.1783069399038855 f_score on test set: 0.5023120530949716\n",
      "Epoch number 710/800\n",
      "Loss on training set: 0.8172560440033512 f_score on training set: 0.632597832774689, loss on test set: 1.476357931804386 f_score on test set: 0.43993654913437863\n",
      "Epoch number 720/800\n",
      "Loss on training set: 0.34423767623407264 f_score on training set: 0.7803373830152216, loss on test set: 1.2235316821960345 f_score on test set: 0.4826930013906559\n",
      "Epoch number 730/800\n",
      "Loss on training set: 0.4537689238750897 f_score on training set: 0.7512011389255577, loss on test set: 1.276982046414041 f_score on test set: 0.47957169909153274\n",
      "Epoch number 740/800\n",
      "Loss on training set: 0.6384955120259469 f_score on training set: 0.7034550856633597, loss on test set: 1.367449342125846 f_score on test set: 0.4761376077251226\n",
      "Epoch number 750/800\n",
      "Loss on training set: 0.32952823124229474 f_score on training set: 0.7639275907810148, loss on test set: 1.1491057297465181 f_score on test set: 0.5033555318010041\n",
      "Epoch number 760/800\n",
      "Loss on training set: 0.37474993832486475 f_score on training set: 0.73634160112926, loss on test set: 1.2022215606646263 f_score on test set: 0.48744618591222805\n",
      "Epoch number 770/800\n",
      "Loss on training set: 0.3536345464287595 f_score on training set: 0.7325526282296042, loss on test set: 1.2778754052025114 f_score on test set: 0.46603552553328925\n",
      "Epoch number 780/800\n",
      "Loss on training set: 0.3910793538067171 f_score on training set: 0.7567467735379659, loss on test set: 1.2945796719549998 f_score on test set: 0.43545414206052885\n",
      "Epoch number 790/800\n",
      "Loss on training set: 0.5451203362568028 f_score on training set: 0.6370922701089287, loss on test set: 1.2942626979099388 f_score on test set: 0.4576789335990538\n",
      "Epoch number 800/800\n",
      "Loss on training set: 0.3647625681761165 f_score on training set: 0.8027109179472975, loss on test set: 1.2811574204149092 f_score on test set: 0.4489240858074584\n",
      "Epoch number 10/400\n",
      "Loss on training set: 0.18579859752812342 f_score on training set: 0.8598945054273501, loss on test set: 1.0185419495857808 f_score on test set: 0.48664796670656835\n",
      "Epoch number 20/400\n",
      "Loss on training set: 0.19254906281111425 f_score on training set: 0.8302505862589586, loss on test set: 1.012945068800375 f_score on test set: 0.5006611470057621\n",
      "Epoch number 30/400\n",
      "Loss on training set: 0.2033979363102326 f_score on training set: 0.808640372544675, loss on test set: 0.9897413010265309 f_score on test set: 0.508737485134338\n",
      "Epoch number 40/400\n",
      "Loss on training set: 0.1892826941418021 f_score on training set: 0.8539117258584673, loss on test set: 1.0061656275604998 f_score on test set: 0.4892032997086159\n",
      "Epoch number 50/400\n",
      "Loss on training set: 0.18912462014594944 f_score on training set: 0.8546775847980668, loss on test set: 1.0215387291167195 f_score on test set: 0.4990436525343674\n",
      "Epoch number 60/400\n",
      "Loss on training set: 0.1826574570195174 f_score on training set: 0.8660520703503668, loss on test set: 0.9601220174784261 f_score on test set: 0.5137687334981704\n",
      "Epoch number 70/400\n",
      "Loss on training set: 0.19290913211874663 f_score on training set: 0.852823923830522, loss on test set: 1.0007183312342016 f_score on test set: 0.48438391244412937\n",
      "Epoch number 80/400\n",
      "Loss on training set: 0.1739081489346254 f_score on training set: 0.8382299356379812, loss on test set: 0.9479120018240915 f_score on test set: 0.5187135698253932\n",
      "Epoch number 90/400\n",
      "Loss on training set: 0.17963579535020466 f_score on training set: 0.854474060374011, loss on test set: 0.9987378116641081 f_score on test set: 0.47965987787783465\n",
      "Epoch number 100/400\n",
      "Loss on training set: 0.18230717439717903 f_score on training set: 0.8637650116946771, loss on test set: 0.9538366495628514 f_score on test set: 0.5051992933192863\n",
      "Epoch number 110/400\n",
      "Loss on training set: 0.16110923882630418 f_score on training set: 0.8394980843309235, loss on test set: 0.9583100467722101 f_score on test set: 0.48431180070080004\n",
      "Epoch number 120/400\n",
      "Loss on training set: 0.17609328309579236 f_score on training set: 0.825382624865771, loss on test set: 0.9170219171844243 f_score on test set: 0.5277224570324516\n",
      "Epoch number 130/400\n",
      "Loss on training set: 0.17435918715787374 f_score on training set: 0.8591481856210251, loss on test set: 0.990706833645708 f_score on test set: 0.48573860823660076\n",
      "Epoch number 140/400\n",
      "Loss on training set: 0.16845315165119865 f_score on training set: 0.8542381199120501, loss on test set: 0.9611571402607315 f_score on test set: 0.49500794704809953\n",
      "Epoch number 150/400\n",
      "Loss on training set: 0.14994255060099929 f_score on training set: 0.8552782994146831, loss on test set: 0.9255562551326381 f_score on test set: 0.5073674784470852\n",
      "Epoch number 160/400\n",
      "Loss on training set: 0.1490512258883444 f_score on training set: 0.8749351921705407, loss on test set: 0.9218685680004007 f_score on test set: 0.49817561628585394\n",
      "Epoch number 170/400\n",
      "Loss on training set: 0.14985688708455974 f_score on training set: 0.87465544332211, loss on test set: 0.8893184188106168 f_score on test set: 0.5324598427367473\n",
      "Epoch number 180/400\n",
      "Loss on training set: 0.14943719700847607 f_score on training set: 0.8492817236682232, loss on test set: 0.891124248071132 f_score on test set: 0.5282343294069203\n",
      "Epoch number 190/400\n",
      "Loss on training set: 0.13913874251162672 f_score on training set: 0.8612312364659931, loss on test set: 0.9120211149443789 f_score on test set: 0.49983239801408663\n",
      "Epoch number 200/400\n",
      "Loss on training set: 0.17269485629142312 f_score on training set: 0.8203128023500946, loss on test set: 0.96535018881781 f_score on test set: 0.4737131891481152\n",
      "Epoch number 210/400\n",
      "Loss on training set: 0.1440679485014597 f_score on training set: 0.8549708383183031, loss on test set: 0.9286621312690629 f_score on test set: 0.48628924058434914\n",
      "Epoch number 220/400\n",
      "Loss on training set: 0.1478950584154099 f_score on training set: 0.8518918508392193, loss on test set: 0.8964934334057906 f_score on test set: 0.5112148051098518\n",
      "Epoch number 230/400\n",
      "Loss on training set: 0.14282174327343342 f_score on training set: 0.8671104217560459, loss on test set: 0.8862753080720835 f_score on test set: 0.5215055608964511\n",
      "Epoch number 240/400\n",
      "Loss on training set: 0.15199159975733384 f_score on training set: 0.8639933122547528, loss on test set: 0.9142690373852301 f_score on test set: 0.4945474768268707\n",
      "Epoch number 250/400\n",
      "Loss on training set: 0.19048153743412113 f_score on training set: 0.8334081532512906, loss on test set: 0.9169981794351123 f_score on test set: 0.5014828449191928\n",
      "Epoch number 260/400\n",
      "Loss on training set: 0.1321692988860096 f_score on training set: 0.8712784531428034, loss on test set: 0.9007493519108623 f_score on test set: 0.5056574901190197\n",
      "Epoch number 270/400\n",
      "Loss on training set: 0.1493724507595494 f_score on training set: 0.8737885297403484, loss on test set: 0.9013446982171345 f_score on test set: 0.5142054429813556\n",
      "Epoch number 280/400\n",
      "Loss on training set: 0.15365116132486795 f_score on training set: 0.8679675813285692, loss on test set: 0.922438344709266 f_score on test set: 0.49053211934419677\n",
      "Epoch number 290/400\n",
      "Loss on training set: 0.2138347543344088 f_score on training set: 0.8182936955869341, loss on test set: 0.9697879666876507 f_score on test set: 0.471928715013852\n",
      "Epoch number 300/400\n",
      "Loss on training set: 0.13471565275624733 f_score on training set: 0.8796348595064899, loss on test set: 0.8888647791516585 f_score on test set: 0.5205975319062962\n",
      "Epoch number 310/400\n",
      "Loss on training set: 0.16509702165951098 f_score on training set: 0.8187661583116946, loss on test set: 0.9403480878633629 f_score on test set: 0.47344794389981437\n",
      "Epoch number 320/400\n",
      "Loss on training set: 0.1456997279324212 f_score on training set: 0.8588407583061776, loss on test set: 0.9025306588052834 f_score on test set: 0.5099701124602711\n",
      "Epoch number 330/400\n",
      "Loss on training set: 0.13355820838772112 f_score on training set: 0.8615450245425619, loss on test set: 0.8941095103803235 f_score on test set: 0.5100821971827237\n",
      "Epoch number 340/400\n",
      "Loss on training set: 0.15390386567265468 f_score on training set: 0.8624887968485185, loss on test set: 0.9291673509528299 f_score on test set: 0.5002682406878523\n",
      "Epoch number 350/400\n",
      "Loss on training set: 0.16510951971289703 f_score on training set: 0.8594502865555498, loss on test set: 0.8918116467427429 f_score on test set: 0.5195678589560369\n",
      "Epoch number 360/400\n",
      "Loss on training set: 0.1369072683694334 f_score on training set: 0.8801498775937359, loss on test set: 0.9002853389905057 f_score on test set: 0.5120226607384569\n",
      "Epoch number 370/400\n",
      "Loss on training set: 0.1455582165440346 f_score on training set: 0.8671096345514951, loss on test set: 0.9085507349989715 f_score on test set: 0.5107516199174211\n",
      "Epoch number 380/400\n",
      "Loss on training set: 0.136451854105492 f_score on training set: 0.8857726125044968, loss on test set: 0.8797895005937122 f_score on test set: 0.5232604774267454\n",
      "Epoch number 390/400\n",
      "Loss on training set: 0.14267911501255626 f_score on training set: 0.8559666332999666, loss on test set: 0.895200292945715 f_score on test set: 0.508879329656804\n",
      "Epoch number 400/400\n",
      "Loss on training set: 0.1417397124092133 f_score on training set: 0.8650990038171065, loss on test set: 0.8922408921174098 f_score on test set: 0.5204235106833756\n",
      "Epoch number 10/800\n",
      "Loss on training set: 1.8963540270091848 f_score on training set: 0.49826374047893035, loss on test set: 2.8603386683248195 f_score on test set: 0.35743392349482195\n",
      "Epoch number 20/800\n",
      "Loss on training set: 1.768859188542177 f_score on training set: 0.4768146568086914, loss on test set: 2.4084579108885893 f_score on test set: 0.4229935917514201\n",
      "Epoch number 30/800\n",
      "Loss on training set: 1.3468114040493802 f_score on training set: 0.5391411218927101, loss on test set: 2.1846301094700156 f_score on test set: 0.42017714929225936\n",
      "Epoch number 40/800\n",
      "Loss on training set: 1.148759132195354 f_score on training set: 0.47221628961662193, loss on test set: 1.8944697272805846 f_score on test set: 0.38446673620971067\n",
      "Epoch number 50/800\n",
      "Loss on training set: 1.711302460834185 f_score on training set: 0.47645152551895736, loss on test set: 2.624078478563452 f_score on test set: 0.3438117572970181\n",
      "Epoch number 60/800\n",
      "Loss on training set: 2.032910508924076 f_score on training set: 0.4670241484115653, loss on test set: 2.759917819540833 f_score on test set: 0.337121354957529\n",
      "Epoch number 70/800\n",
      "Loss on training set: 1.1833307994587343 f_score on training set: 0.5051290240028855, loss on test set: 2.114680753040092 f_score on test set: 0.31359122014807367\n",
      "Epoch number 80/800\n",
      "Loss on training set: 1.2625481819414177 f_score on training set: 0.4801043042978526, loss on test set: 2.0075390932519115 f_score on test set: 0.41629391765465695\n",
      "Epoch number 90/800\n",
      "Loss on training set: 1.091582139099911 f_score on training set: 0.4884792151195313, loss on test set: 1.8702136382502452 f_score on test set: 0.3694710312003974\n",
      "Epoch number 100/800\n",
      "Loss on training set: 1.298500731605654 f_score on training set: 0.4543500250734293, loss on test set: 2.236718147453191 f_score on test set: 0.3689789697065384\n",
      "Epoch number 110/800\n",
      "Loss on training set: 1.1792835953845973 f_score on training set: 0.545248392241601, loss on test set: 2.065245055107824 f_score on test set: 0.39791926282671186\n",
      "Epoch number 120/800\n",
      "Loss on training set: 1.4667414149928781 f_score on training set: 0.37460062214490447, loss on test set: 2.1912646666918545 f_score on test set: 0.28552982037401725\n",
      "Epoch number 130/800\n",
      "Loss on training set: 0.6754542063722339 f_score on training set: 0.6211096304872588, loss on test set: 1.7230936865652016 f_score on test set: 0.3928862195411705\n",
      "Epoch number 140/800\n",
      "Loss on training set: 0.8441868339212546 f_score on training set: 0.577555846419954, loss on test set: 1.8460614990251571 f_score on test set: 0.3791362513591156\n",
      "Epoch number 150/800\n",
      "Loss on training set: 1.0206330525196343 f_score on training set: 0.6001274088230609, loss on test set: 1.9638991884934214 f_score on test set: 0.41672203526713797\n",
      "Epoch number 160/800\n",
      "Loss on training set: 1.2034384324984162 f_score on training set: 0.5155629105542123, loss on test set: 2.013564285273287 f_score on test set: 0.3525054135454679\n",
      "Epoch number 170/800\n",
      "Loss on training set: 0.9278854489972476 f_score on training set: 0.564767788805663, loss on test set: 2.096667333391985 f_score on test set: 0.35276153625879203\n",
      "Epoch number 180/800\n",
      "Loss on training set: 1.030190944156795 f_score on training set: 0.5701724035431901, loss on test set: 1.9865221896455045 f_score on test set: 0.4045878840112067\n",
      "Epoch number 190/800\n",
      "Loss on training set: 0.6421068532895554 f_score on training set: 0.6085547785547787, loss on test set: 1.6085826264706478 f_score on test set: 0.42183777800940275\n",
      "Epoch number 200/800\n",
      "Loss on training set: 0.6834264613826253 f_score on training set: 0.5429282669589576, loss on test set: 1.5609673419050323 f_score on test set: 0.4269795359304877\n",
      "Epoch number 210/800\n",
      "Loss on training set: 0.7596826604531559 f_score on training set: 0.5570342133526393, loss on test set: 1.740953681425997 f_score on test set: 0.3396508627518625\n",
      "Epoch number 220/800\n",
      "Loss on training set: 0.7838398433061291 f_score on training set: 0.5339718424576436, loss on test set: 1.6959293492890215 f_score on test set: 0.39052999066943045\n",
      "Epoch number 230/800\n",
      "Loss on training set: 0.7864582346184166 f_score on training set: 0.46050112591460773, loss on test set: 1.7909393578038275 f_score on test set: 0.31002658716356746\n",
      "Epoch number 240/800\n",
      "Loss on training set: 1.1642015215723789 f_score on training set: 0.47005409225204614, loss on test set: 2.1909245891006437 f_score on test set: 0.34369669396638464\n",
      "Epoch number 250/800\n",
      "Loss on training set: 1.237134772068828 f_score on training set: 0.464487042328544, loss on test set: 2.144509395645694 f_score on test set: 0.32597710255312673\n",
      "Epoch number 260/800\n",
      "Loss on training set: 0.7901459189503524 f_score on training set: 0.48129683108275806, loss on test set: 1.752803224694073 f_score on test set: 0.3610962211972208\n",
      "Epoch number 270/800\n",
      "Loss on training set: 0.748959795418257 f_score on training set: 0.5551308760329488, loss on test set: 1.6483461011107923 f_score on test set: 0.36066296952351073\n",
      "Epoch number 280/800\n",
      "Loss on training set: 0.6564685408400969 f_score on training set: 0.5598474279343845, loss on test set: 1.5049016016450594 f_score on test set: 0.4002659066313688\n",
      "Epoch number 290/800\n",
      "Loss on training set: 0.9045810190463747 f_score on training set: 0.6173479589013983, loss on test set: 1.8803329686030037 f_score on test set: 0.41853232490523257\n",
      "Epoch number 300/800\n",
      "Loss on training set: 1.0004700893620768 f_score on training set: 0.48366483590364184, loss on test set: 1.809697605459052 f_score on test set: 0.37045975076147813\n",
      "Epoch number 310/800\n",
      "Loss on training set: 1.1484309953538774 f_score on training set: 0.46817137814262866, loss on test set: 1.9970363764909373 f_score on test set: 0.3954200513494474\n",
      "Epoch number 320/800\n",
      "Loss on training set: 0.7149185447862437 f_score on training set: 0.6022164967933087, loss on test set: 1.6292053586350477 f_score on test set: 0.3982322195841959\n",
      "Epoch number 330/800\n",
      "Loss on training set: 0.8890375681224245 f_score on training set: 0.6309544478215376, loss on test set: 1.952931287829252 f_score on test set: 0.3637632688764683\n",
      "Epoch number 340/800\n",
      "Loss on training set: 0.9827438931288143 f_score on training set: 0.5535166861151396, loss on test set: 1.9595618316143864 f_score on test set: 0.3416610099571712\n",
      "Epoch number 350/800\n",
      "Loss on training set: 0.8839842466799903 f_score on training set: 0.5721989883421591, loss on test set: 1.9664225176373256 f_score on test set: 0.3639334454041413\n",
      "Epoch number 360/800\n",
      "Loss on training set: 1.0040749633286399 f_score on training set: 0.48834960396771904, loss on test set: 1.7208730095734492 f_score on test set: 0.3792576140349629\n",
      "Epoch number 370/800\n",
      "Loss on training set: 0.44686448541144175 f_score on training set: 0.660561242839724, loss on test set: 1.4335918545001767 f_score on test set: 0.44041756366453466\n",
      "Epoch number 380/800\n",
      "Loss on training set: 0.870569374905991 f_score on training set: 0.5145487102008842, loss on test set: 1.719934821897739 f_score on test set: 0.3525470498136464\n",
      "Epoch number 390/800\n",
      "Loss on training set: 1.1466072990927652 f_score on training set: 0.5916666378898487, loss on test set: 2.1540511070914032 f_score on test set: 0.3586938888801876\n",
      "Epoch number 400/800\n",
      "Loss on training set: 1.203063081525375 f_score on training set: 0.5104717894191579, loss on test set: 2.0362848281305586 f_score on test set: 0.3425431234009684\n",
      "Epoch number 410/800\n",
      "Loss on training set: 0.6633245351800426 f_score on training set: 0.5628231814162848, loss on test set: 1.4501993383598213 f_score on test set: 0.43645494595602313\n",
      "Epoch number 420/800\n",
      "Loss on training set: 0.806200323917361 f_score on training set: 0.5167124827800074, loss on test set: 1.7172201020027424 f_score on test set: 0.3383185901256046\n",
      "Epoch number 430/800\n",
      "Loss on training set: 0.9680823805635445 f_score on training set: 0.5430189091505584, loss on test set: 1.7966441001378066 f_score on test set: 0.42676916659306907\n",
      "Epoch number 440/800\n",
      "Loss on training set: 0.7815009688181085 f_score on training set: 0.6138291866796937, loss on test set: 1.48958997215552 f_score on test set: 0.4192050260106957\n",
      "Epoch number 450/800\n",
      "Loss on training set: 0.5617912705191206 f_score on training set: 0.49777021811905525, loss on test set: 1.3654599636067084 f_score on test set: 0.40571095973106247\n",
      "Epoch number 460/800\n",
      "Loss on training set: 0.9268527380301285 f_score on training set: 0.4962878787878788, loss on test set: 1.7567851739123679 f_score on test set: 0.38075747171799895\n",
      "Epoch number 470/800\n",
      "Loss on training set: 0.6836569829091163 f_score on training set: 0.6346718622685832, loss on test set: 1.5693981399797061 f_score on test set: 0.419626569579226\n",
      "Epoch number 480/800\n",
      "Loss on training set: 0.7836475695042501 f_score on training set: 0.6135558343389359, loss on test set: 1.4708344857874862 f_score on test set: 0.43858255148958664\n",
      "Epoch number 490/800\n",
      "Loss on training set: 0.8757590491520338 f_score on training set: 0.4219597615499255, loss on test set: 1.7296390892846851 f_score on test set: 0.3279817061834935\n",
      "Epoch number 500/800\n",
      "Loss on training set: 0.4401968869502969 f_score on training set: 0.6435981411970026, loss on test set: 1.3613177134321435 f_score on test set: 0.4471374855611642\n",
      "Epoch number 510/800\n",
      "Loss on training set: 0.83267820483869 f_score on training set: 0.5839699802542269, loss on test set: 1.6494928472452681 f_score on test set: 0.4030195040674897\n",
      "Epoch number 520/800\n",
      "Loss on training set: 0.6580693806971185 f_score on training set: 0.6681262359920898, loss on test set: 1.6788345594899103 f_score on test set: 0.38330684413841987\n",
      "Epoch number 530/800\n",
      "Loss on training set: 0.4902164116830806 f_score on training set: 0.6096234944728594, loss on test set: 1.4196235695354016 f_score on test set: 0.4145753050557556\n",
      "Epoch number 540/800\n",
      "Loss on training set: 1.1545108173346663 f_score on training set: 0.630448225609516, loss on test set: 1.70047932002755 f_score on test set: 0.4437534320060258\n",
      "Epoch number 550/800\n",
      "Loss on training set: 0.7074629513535458 f_score on training set: 0.5938786487729528, loss on test set: 1.6027883284907296 f_score on test set: 0.3900950212949806\n",
      "Epoch number 560/800\n",
      "Loss on training set: 0.8026607642467088 f_score on training set: 0.6127825713723096, loss on test set: 1.7856816309501509 f_score on test set: 0.3899478920148064\n",
      "Epoch number 570/800\n",
      "Loss on training set: 0.5473584920019705 f_score on training set: 0.6237291292679698, loss on test set: 1.4514245977513742 f_score on test set: 0.4244715688567346\n",
      "Epoch number 580/800\n",
      "Loss on training set: 0.49608796491003143 f_score on training set: 0.5979299777087427, loss on test set: 1.4214381608725442 f_score on test set: 0.37970195224065195\n",
      "Epoch number 590/800\n",
      "Loss on training set: 0.7422443836746337 f_score on training set: 0.5962298839486597, loss on test set: 1.5025201886853554 f_score on test set: 0.4119661909095113\n",
      "Epoch number 600/800\n",
      "Loss on training set: 0.5412461506379437 f_score on training set: 0.6117430673457838, loss on test set: 1.3298933256815977 f_score on test set: 0.41991688712264386\n",
      "Epoch number 610/800\n",
      "Loss on training set: 0.7469630210316179 f_score on training set: 0.567187794357186, loss on test set: 1.466242470562851 f_score on test set: 0.3996191203476165\n",
      "Epoch number 620/800\n",
      "Loss on training set: 0.6318147183562701 f_score on training set: 0.6274296681820707, loss on test set: 1.5313838661148562 f_score on test set: 0.3621003718639266\n",
      "Epoch number 630/800\n",
      "Loss on training set: 0.5407178693831987 f_score on training set: 0.6172365960741675, loss on test set: 1.5157053311604995 f_score on test set: 0.365264004352623\n",
      "Epoch number 640/800\n",
      "Loss on training set: 0.7633857288963821 f_score on training set: 0.5180967540030922, loss on test set: 1.5873736322601377 f_score on test set: 0.3820113059485946\n",
      "Epoch number 650/800\n",
      "Loss on training set: 0.7283655874323094 f_score on training set: 0.629211518166742, loss on test set: 1.4852050718177758 f_score on test set: 0.41569483567203885\n",
      "Epoch number 660/800\n",
      "Loss on training set: 0.5181283480566498 f_score on training set: 0.6360642048162429, loss on test set: 1.4342131101111504 f_score on test set: 0.40650161672994783\n",
      "Epoch number 670/800\n",
      "Loss on training set: 0.5884201662424112 f_score on training set: 0.6356134063546389, loss on test set: 1.4396481673038062 f_score on test set: 0.4432162088400325\n",
      "Epoch number 680/800\n",
      "Loss on training set: 0.6808304983837533 f_score on training set: 0.5319113424504498, loss on test set: 1.6466794971038412 f_score on test set: 0.3128364178324227\n",
      "Epoch number 690/800\n",
      "Loss on training set: 0.30820648445340554 f_score on training set: 0.6921296637156636, loss on test set: 1.1636010207283463 f_score on test set: 0.4591456126771007\n",
      "Epoch number 700/800\n",
      "Loss on training set: 0.7845199673244367 f_score on training set: 0.5865182311412356, loss on test set: 1.6313675628034856 f_score on test set: 0.3881444639441247\n",
      "Epoch number 710/800\n",
      "Loss on training set: 0.5731240811227004 f_score on training set: 0.6724389928187119, loss on test set: 1.525694648228782 f_score on test set: 0.38766767156415693\n",
      "Epoch number 720/800\n",
      "Loss on training set: 0.8520904563732491 f_score on training set: 0.5693536818143217, loss on test set: 1.75513682137566 f_score on test set: 0.36505733797686746\n",
      "Epoch number 730/800\n",
      "Loss on training set: 0.6078050640545701 f_score on training set: 0.6582011085475908, loss on test set: 1.396354489905851 f_score on test set: 0.45281268125895086\n",
      "Epoch number 740/800\n",
      "Loss on training set: 0.49215136670621845 f_score on training set: 0.6530505014111572, loss on test set: 1.3397536010010023 f_score on test set: 0.4266316153216603\n",
      "Epoch number 750/800\n",
      "Loss on training set: 0.44457906426855937 f_score on training set: 0.6461074343497891, loss on test set: 1.283715540714192 f_score on test set: 0.45228412018832537\n",
      "Epoch number 760/800\n",
      "Loss on training set: 1.0320692236164626 f_score on training set: 0.4848609343347073, loss on test set: 1.8085727287303748 f_score on test set: 0.3663698514777891\n",
      "Epoch number 770/800\n",
      "Loss on training set: 0.5759817624588917 f_score on training set: 0.6853871088225927, loss on test set: 1.451104671865724 f_score on test set: 0.3840526041188244\n",
      "Epoch number 780/800\n",
      "Loss on training set: 0.7635006306022839 f_score on training set: 0.5732841997736279, loss on test set: 1.420231559246354 f_score on test set: 0.4173928453669666\n",
      "Epoch number 790/800\n",
      "Loss on training set: 0.4827395180590187 f_score on training set: 0.6583985630059608, loss on test set: 1.3210798177714276 f_score on test set: 0.45699938021886927\n",
      "Epoch number 800/800\n",
      "Loss on training set: 0.6394941209456455 f_score on training set: 0.5606596428375014, loss on test set: 1.499657948329045 f_score on test set: 0.4197948484100592\n",
      "Epoch number 10/400\n",
      "Loss on training set: 0.24043775271010367 f_score on training set: 0.7379217021899949, loss on test set: 1.1358551464142579 f_score on test set: 0.45875167080295254\n",
      "Epoch number 20/400\n",
      "Loss on training set: 0.23810712576567472 f_score on training set: 0.7160755115628499, loss on test set: 1.08456550956098 f_score on test set: 0.4365948595469085\n",
      "Epoch number 30/400\n",
      "Loss on training set: 0.21808783968952025 f_score on training set: 0.7416027697408267, loss on test set: 1.0717026122770608 f_score on test set: 0.46216119521206295\n",
      "Epoch number 40/400\n",
      "Loss on training set: 0.21914160950091457 f_score on training set: 0.7246672637464039, loss on test set: 1.0748528847953822 f_score on test set: 0.477298652146776\n",
      "Epoch number 50/400\n",
      "Loss on training set: 0.21011690463468266 f_score on training set: 0.7537443236310657, loss on test set: 1.0940980731226921 f_score on test set: 0.4590381728281261\n",
      "Epoch number 60/400\n",
      "Loss on training set: 0.2652608014450087 f_score on training set: 0.7150249368039898, loss on test set: 1.0937686380765188 f_score on test set: 0.4478275536965982\n",
      "Epoch number 70/400\n",
      "Loss on training set: 0.20371782032347516 f_score on training set: 0.7571737659864956, loss on test set: 1.0921834263093597 f_score on test set: 0.48205377340509864\n",
      "Epoch number 80/400\n",
      "Loss on training set: 0.19884749109081576 f_score on training set: 0.7324599941788269, loss on test set: 1.0764701666730583 f_score on test set: 0.4669899924128264\n",
      "Epoch number 90/400\n",
      "Loss on training set: 0.2030976180745176 f_score on training set: 0.7413604362304903, loss on test set: 1.0942361349145555 f_score on test set: 0.4488329381346866\n",
      "Epoch number 100/400\n",
      "Loss on training set: 0.22165220186386425 f_score on training set: 0.7601871875167057, loss on test set: 1.0844160070815707 f_score on test set: 0.47268960702204116\n",
      "Epoch number 110/400\n",
      "Loss on training set: 0.20334562658629887 f_score on training set: 0.7510752540240955, loss on test set: 1.0650533826232604 f_score on test set: 0.4758006647693196\n",
      "Epoch number 120/400\n",
      "Loss on training set: 0.23841663603899899 f_score on training set: 0.7679557112357784, loss on test set: 1.1052368012155949 f_score on test set: 0.46992632032456716\n",
      "Epoch number 130/400\n",
      "Loss on training set: 0.19582369077240647 f_score on training set: 0.7551122667635575, loss on test set: 1.0729092349854323 f_score on test set: 0.4624281982877321\n",
      "Epoch number 140/400\n",
      "Loss on training set: 0.2233084021671849 f_score on training set: 0.7022086720867208, loss on test set: 1.1012332454793459 f_score on test set: 0.4063791708266522\n",
      "Epoch number 150/400\n",
      "Loss on training set: 0.20738829090436992 f_score on training set: 0.7462994525799403, loss on test set: 1.06681822618412 f_score on test set: 0.46568838255223965\n",
      "Epoch number 160/400\n",
      "Loss on training set: 0.20936556831143577 f_score on training set: 0.7761608304983986, loss on test set: 1.0757926314250725 f_score on test set: 0.4445528998838586\n",
      "Epoch number 170/400\n",
      "Loss on training set: 0.19425062169985075 f_score on training set: 0.7626879813626801, loss on test set: 1.0648648691582603 f_score on test set: 0.46424861888265095\n",
      "Epoch number 180/400\n",
      "Loss on training set: 0.211042154337076 f_score on training set: 0.7762315724018702, loss on test set: 1.0838780425707704 f_score on test set: 0.4486978588353378\n",
      "Epoch number 190/400\n",
      "Loss on training set: 0.20002770450144747 f_score on training set: 0.7782678531300854, loss on test set: 1.0737484828966704 f_score on test set: 0.4456163480253361\n",
      "Epoch number 200/400\n",
      "Loss on training set: 0.19303856571003203 f_score on training set: 0.7691806048270264, loss on test set: 1.0756863611924956 f_score on test set: 0.49076815252081435\n",
      "Epoch number 210/400\n",
      "Loss on training set: 0.21465020938211893 f_score on training set: 0.7533409596265873, loss on test set: 1.0972115002676222 f_score on test set: 0.4841845304671673\n",
      "Epoch number 220/400\n",
      "Loss on training set: 0.21186803427287867 f_score on training set: 0.7125014586980077, loss on test set: 1.0677214819877137 f_score on test set: 0.48043329866457674\n",
      "Epoch number 230/400\n",
      "Loss on training set: 0.3114810871854022 f_score on training set: 0.7163040024598986, loss on test set: 1.1019595918365774 f_score on test set: 0.45811655087071307\n",
      "Epoch number 240/400\n",
      "Loss on training set: 0.1971009166100282 f_score on training set: 0.7272250568088645, loss on test set: 1.0647067243004615 f_score on test set: 0.46216032118249756\n",
      "Epoch number 250/400\n",
      "Loss on training set: 0.1919508099013643 f_score on training set: 0.7598889857110827, loss on test set: 1.0551342595444086 f_score on test set: 0.45743670803170644\n",
      "Epoch number 260/400\n",
      "Loss on training set: 0.21470188029962867 f_score on training set: 0.7523512183660257, loss on test set: 1.1119975644063067 f_score on test set: 0.4517131352238133\n",
      "Epoch number 270/400\n",
      "Loss on training set: 0.20355158102412305 f_score on training set: 0.7460098305468751, loss on test set: 1.0760100934500165 f_score on test set: 0.4926990266854715\n",
      "Epoch number 280/400\n",
      "Loss on training set: 0.2118134659207753 f_score on training set: 0.7615922173065031, loss on test set: 1.0869451042398328 f_score on test set: 0.505136963973349\n",
      "Epoch number 290/400\n",
      "Loss on training set: 0.20578138765596984 f_score on training set: 0.7103794759970186, loss on test set: 1.0814452864022819 f_score on test set: 0.430649149990682\n",
      "Epoch number 300/400\n",
      "Loss on training set: 0.24370936539289773 f_score on training set: 0.7458518197657071, loss on test set: 1.0610441760319973 f_score on test set: 0.4761088481319007\n",
      "Epoch number 310/400\n",
      "Loss on training set: 0.1925403398086982 f_score on training set: 0.7393754284930756, loss on test set: 1.0634758921786958 f_score on test set: 0.4603635815332315\n",
      "Epoch number 320/400\n",
      "Loss on training set: 0.1894819479972771 f_score on training set: 0.7880591923951689, loss on test set: 1.0583820497049334 f_score on test set: 0.47432403472808965\n",
      "Epoch number 330/400\n",
      "Loss on training set: 0.19955483966920085 f_score on training set: 0.7329157564328377, loss on test set: 1.0575488510617514 f_score on test set: 0.4938934988112422\n",
      "Epoch number 340/400\n",
      "Loss on training set: 0.1942897006151262 f_score on training set: 0.7392496897890566, loss on test set: 1.0705819112906403 f_score on test set: 0.4432947184198614\n",
      "Epoch number 350/400\n",
      "Loss on training set: 0.19080140403325083 f_score on training set: 0.7803881179977388, loss on test set: 1.0668472828675986 f_score on test set: 0.4829754136628112\n",
      "Epoch number 360/400\n",
      "Loss on training set: 0.2112073119101413 f_score on training set: 0.7679588688398907, loss on test set: 1.10751945205492 f_score on test set: 0.470836538787721\n",
      "Epoch number 370/400\n",
      "Loss on training set: 0.22003604807892216 f_score on training set: 0.7434509497788188, loss on test set: 1.064963587740907 f_score on test set: 0.48362163809615044\n",
      "Epoch number 380/400\n",
      "Loss on training set: 0.20092756954395316 f_score on training set: 0.7682379041301712, loss on test set: 1.0793187697151247 f_score on test set: 0.4798291121759422\n",
      "Epoch number 390/400\n",
      "Loss on training set: 0.18554157154890466 f_score on training set: 0.7624792666957662, loss on test set: 1.0602072112243752 f_score on test set: 0.4682971985287445\n",
      "Epoch number 400/400\n",
      "Loss on training set: 0.23648527157599492 f_score on training set: 0.7252599542698553, loss on test set: 1.0610348392734803 f_score on test set: 0.4643791424553167\n",
      "Epoch number 10/800\n",
      "Loss on training set: 2.3585845808130173 f_score on training set: 0.35087807572006874, loss on test set: 2.573564443128464 f_score on test set: 0.3405814610073233\n",
      "Epoch number 20/800\n",
      "Loss on training set: 1.5056194575136346 f_score on training set: 0.39775256457792607, loss on test set: 1.999811042231067 f_score on test set: 0.3479597068485233\n",
      "Epoch number 30/800\n",
      "Loss on training set: 1.6162358534083912 f_score on training set: 0.5099972755039479, loss on test set: 2.509258695847923 f_score on test set: 0.3416449736231384\n",
      "Epoch number 40/800\n",
      "Loss on training set: 1.630125010491495 f_score on training set: 0.3734051883934788, loss on test set: 2.3502805185811058 f_score on test set: 0.27003510954381144\n",
      "Epoch number 50/800\n",
      "Loss on training set: 1.2185297744494328 f_score on training set: 0.3920027103460946, loss on test set: 1.954313038776163 f_score on test set: 0.3309012580323407\n",
      "Epoch number 60/800\n",
      "Loss on training set: 1.3009096432272482 f_score on training set: 0.4175096067313616, loss on test set: 2.166610007092728 f_score on test set: 0.3395146416271568\n",
      "Epoch number 70/800\n",
      "Loss on training set: 1.4109551125947979 f_score on training set: 0.39792022270609806, loss on test set: 1.8687251657170767 f_score on test set: 0.3726874451225176\n",
      "Epoch number 80/800\n",
      "Loss on training set: 1.1178687709675021 f_score on training set: 0.46014477965944095, loss on test set: 1.7595457183503942 f_score on test set: 0.38701065802367485\n",
      "Epoch number 90/800\n",
      "Loss on training set: 1.088730369869775 f_score on training set: 0.47608084776347437, loss on test set: 2.2106413098443043 f_score on test set: 0.25003896692651756\n",
      "Epoch number 100/800\n",
      "Loss on training set: 1.4489701597133597 f_score on training set: 0.4171549832578886, loss on test set: 2.112771094716464 f_score on test set: 0.37079903985150686\n",
      "Epoch number 110/800\n",
      "Loss on training set: 1.5539051431428994 f_score on training set: 0.4400790579189851, loss on test set: 2.3805834542376925 f_score on test set: 0.3640951456566568\n",
      "Epoch number 120/800\n",
      "Loss on training set: 0.8682967525780101 f_score on training set: 0.5337229985650251, loss on test set: 1.838067398311947 f_score on test set: 0.349639281567139\n",
      "Epoch number 130/800\n",
      "Loss on training set: 1.0367835976338373 f_score on training set: 0.5179340831514745, loss on test set: 1.7934437944473298 f_score on test set: 0.3722437447707139\n",
      "Epoch number 140/800\n",
      "Loss on training set: 1.1388047054550499 f_score on training set: 0.46611922960760166, loss on test set: 1.8989498171848227 f_score on test set: 0.3734642264111695\n",
      "Epoch number 150/800\n",
      "Loss on training set: 0.9112556168710816 f_score on training set: 0.4897656034674713, loss on test set: 1.7556023677605586 f_score on test set: 0.3493965291040696\n",
      "Epoch number 160/800\n",
      "Loss on training set: 1.2100636849399045 f_score on training set: 0.4430019020385274, loss on test set: 1.8982834100964479 f_score on test set: 0.36398274997215885\n",
      "Epoch number 170/800\n",
      "Loss on training set: 0.9684200980689684 f_score on training set: 0.5156950621064429, loss on test set: 1.7436281781454739 f_score on test set: 0.3771472845775755\n",
      "Epoch number 180/800\n",
      "Loss on training set: 1.4697539771320098 f_score on training set: 0.321829268292683, loss on test set: 2.2099377198426495 f_score on test set: 0.3055450824213873\n",
      "Epoch number 190/800\n",
      "Loss on training set: 1.2739578336624244 f_score on training set: 0.44837226729829355, loss on test set: 2.465012234837045 f_score on test set: 0.2560621719586452\n",
      "Epoch number 200/800\n",
      "Loss on training set: 1.2588597397186863 f_score on training set: 0.4497977933571218, loss on test set: 1.8338002762949115 f_score on test set: 0.4071144111771441\n",
      "Epoch number 210/800\n",
      "Loss on training set: 1.4563155112322244 f_score on training set: 0.3768643146223613, loss on test set: 2.0434940560389476 f_score on test set: 0.34884886641178947\n",
      "Epoch number 220/800\n",
      "Loss on training set: 1.12693615340654 f_score on training set: 0.5367423362930352, loss on test set: 2.0037055845184417 f_score on test set: 0.33815141788269626\n",
      "Epoch number 230/800\n",
      "Loss on training set: 1.1397751185981881 f_score on training set: 0.4371581981462532, loss on test set: 1.7830829731573155 f_score on test set: 0.37977752686424593\n",
      "Epoch number 240/800\n",
      "Loss on training set: 1.0736277410642396 f_score on training set: 0.5564933527259734, loss on test set: 1.8799032911634825 f_score on test set: 0.38061132315397617\n",
      "Epoch number 250/800\n",
      "Loss on training set: 0.5567123424560848 f_score on training set: 0.5451024086009647, loss on test set: 1.3888996512546534 f_score on test set: 0.4252753686854514\n",
      "Epoch number 260/800\n",
      "Loss on training set: 1.0833066429689568 f_score on training set: 0.4887011325444898, loss on test set: 1.9538875917454033 f_score on test set: 0.34540793605483167\n",
      "Epoch number 270/800\n",
      "Loss on training set: 0.742839406702125 f_score on training set: 0.5975171609681431, loss on test set: 1.5503058324555348 f_score on test set: 0.3803047196543773\n",
      "Epoch number 280/800\n",
      "Loss on training set: 0.8728170312427747 f_score on training set: 0.4934751312396942, loss on test set: 1.6988163654561401 f_score on test set: 0.3559251569041661\n",
      "Epoch number 290/800\n",
      "Loss on training set: 0.7916021101165759 f_score on training set: 0.4873834662069956, loss on test set: 1.5981774714850323 f_score on test set: 0.37817464581162147\n",
      "Epoch number 300/800\n",
      "Loss on training set: 0.942695558526457 f_score on training set: 0.48378223084105426, loss on test set: 1.8054795951011524 f_score on test set: 0.33488133549847315\n",
      "Epoch number 310/800\n",
      "Loss on training set: 0.8857070237217822 f_score on training set: 0.4754723405110541, loss on test set: 1.7505269540023844 f_score on test set: 0.36563367232720967\n",
      "Epoch number 320/800\n",
      "Loss on training set: 0.8048511499543137 f_score on training set: 0.5989648440587408, loss on test set: 1.6603766325922087 f_score on test set: 0.4131435572631466\n",
      "Epoch number 330/800\n",
      "Loss on training set: 1.7918530506505808 f_score on training set: 0.39097660237492304, loss on test set: 2.0358966800200724 f_score on test set: 0.40721901826331824\n",
      "Epoch number 340/800\n",
      "Loss on training set: 0.573913800879329 f_score on training set: 0.5755578106465475, loss on test set: 1.4518502757366858 f_score on test set: 0.39308756173088166\n",
      "Epoch number 350/800\n",
      "Loss on training set: 0.8751040803394262 f_score on training set: 0.5885807408737967, loss on test set: 1.642027274163515 f_score on test set: 0.4123353339678158\n",
      "Epoch number 360/800\n",
      "Loss on training set: 0.5905933210416634 f_score on training set: 0.6276063103027059, loss on test set: 1.5170602728325169 f_score on test set: 0.41328439017590146\n",
      "Epoch number 370/800\n",
      "Loss on training set: 0.7399274750709005 f_score on training set: 0.4581579261841051, loss on test set: 1.5856293700552402 f_score on test set: 0.36257143977883916\n",
      "Epoch number 380/800\n",
      "Loss on training set: 0.7593473345037696 f_score on training set: 0.5158044936284373, loss on test set: 1.4244592134497904 f_score on test set: 0.41856762574948103\n",
      "Epoch number 390/800\n",
      "Loss on training set: 0.7662043666013406 f_score on training set: 0.5525908111190198, loss on test set: 1.6449784842315318 f_score on test set: 0.3801449430033615\n",
      "Epoch number 400/800\n",
      "Loss on training set: 1.0747741882391098 f_score on training set: 0.4363642846954785, loss on test set: 1.7649350704618068 f_score on test set: 0.3769580767338873\n",
      "Epoch number 410/800\n",
      "Loss on training set: 0.6904872144664939 f_score on training set: 0.5898399667828419, loss on test set: 1.6117595042753214 f_score on test set: 0.38726052494778707\n",
      "Epoch number 420/800\n",
      "Loss on training set: 0.492510853351845 f_score on training set: 0.6329840839797868, loss on test set: 1.381503898659323 f_score on test set: 0.4268171493788414\n",
      "Epoch number 430/800\n",
      "Loss on training set: 1.2240068238745785 f_score on training set: 0.499597673290323, loss on test set: 1.7584420935353156 f_score on test set: 0.41224298605139154\n",
      "Epoch number 440/800\n",
      "Loss on training set: 0.5668624498188559 f_score on training set: 0.6124889007700439, loss on test set: 1.4393359648153161 f_score on test set: 0.40266130165728875\n",
      "Epoch number 450/800\n",
      "Loss on training set: 0.5295929317837839 f_score on training set: 0.6380125601918116, loss on test set: 1.400918462095423 f_score on test set: 0.4240289465868797\n",
      "Epoch number 460/800\n",
      "Loss on training set: 0.8350581382552582 f_score on training set: 0.5210419406850705, loss on test set: 1.6214985436721072 f_score on test set: 0.3780487800168161\n",
      "Epoch number 470/800\n",
      "Loss on training set: 0.8845541847417243 f_score on training set: 0.49791592019540337, loss on test set: 1.5394188662109245 f_score on test set: 0.39407187502287105\n",
      "Epoch number 480/800\n",
      "Loss on training set: 0.5431794139941292 f_score on training set: 0.5933619922470097, loss on test set: 1.384735512551701 f_score on test set: 0.42518217036448575\n",
      "Epoch number 490/800\n",
      "Loss on training set: 0.7474540162839448 f_score on training set: 0.4983620829926077, loss on test set: 1.6637801465609872 f_score on test set: 0.36300320556012206\n",
      "Epoch number 500/800\n",
      "Loss on training set: 0.6382498759521079 f_score on training set: 0.547125096242328, loss on test set: 1.5390089105371327 f_score on test set: 0.386881040188373\n",
      "Epoch number 510/800\n",
      "Loss on training set: 0.5586051344992857 f_score on training set: 0.5731106509391709, loss on test set: 1.5024485027959613 f_score on test set: 0.3834682726308281\n",
      "Epoch number 520/800\n",
      "Loss on training set: 0.6202735689897309 f_score on training set: 0.5792856258788595, loss on test set: 1.3904910461498878 f_score on test set: 0.4168422439395323\n",
      "Epoch number 530/800\n",
      "Loss on training set: 0.648154892507871 f_score on training set: 0.5295797157462236, loss on test set: 1.4420485292508798 f_score on test set: 0.4230010111340897\n",
      "Epoch number 540/800\n",
      "Loss on training set: 0.5748835288725638 f_score on training set: 0.6092819447429376, loss on test set: 1.3150096545026906 f_score on test set: 0.4074933277642208\n",
      "Epoch number 550/800\n",
      "Loss on training set: 0.5606219908955691 f_score on training set: 0.5440297104025916, loss on test set: 1.5531568378218372 f_score on test set: 0.3766624353438309\n",
      "Epoch number 560/800\n",
      "Loss on training set: 0.5950775782596265 f_score on training set: 0.5304703296416597, loss on test set: 1.37168356033899 f_score on test set: 0.4196162214783535\n",
      "Epoch number 570/800\n",
      "Loss on training set: 1.0814801870706363 f_score on training set: 0.5208854121529726, loss on test set: 1.7274475316622069 f_score on test set: 0.40118164704926385\n",
      "Epoch number 580/800\n",
      "Loss on training set: 0.508265792575372 f_score on training set: 0.6327490460903492, loss on test set: 1.4427904899588186 f_score on test set: 0.41634425406688874\n",
      "Epoch number 590/800\n",
      "Loss on training set: 0.5061060405663917 f_score on training set: 0.660320910973085, loss on test set: 1.3239554202084252 f_score on test set: 0.4268698671252901\n",
      "Epoch number 600/800\n",
      "Loss on training set: 0.6878001404668699 f_score on training set: 0.5731178168799667, loss on test set: 1.4738353442357972 f_score on test set: 0.37942183954833936\n",
      "Epoch number 610/800\n",
      "Loss on training set: 0.6407054551421393 f_score on training set: 0.5795805720558359, loss on test set: 1.325642686546731 f_score on test set: 0.455520658191969\n",
      "Epoch number 620/800\n",
      "Loss on training set: 0.5271195212393076 f_score on training set: 0.5648912898492731, loss on test set: 1.3494908182833745 f_score on test set: 0.40613424244485025\n",
      "Epoch number 630/800\n",
      "Loss on training set: 0.6262008127305335 f_score on training set: 0.5735947880502336, loss on test set: 1.5022678482630318 f_score on test set: 0.36269132522634295\n",
      "Epoch number 640/800\n",
      "Loss on training set: 0.6167856393137798 f_score on training set: 0.5312745891386669, loss on test set: 1.233818614152801 f_score on test set: 0.45573697017610765\n",
      "Epoch number 650/800\n",
      "Loss on training set: 0.6816604269485488 f_score on training set: 0.5722067305443722, loss on test set: 1.6478333757219648 f_score on test set: 0.3896954078438089\n",
      "Epoch number 660/800\n",
      "Loss on training set: 0.6348219486367112 f_score on training set: 0.6440713453213452, loss on test set: 1.3252572709627277 f_score on test set: 0.42893974761414855\n",
      "Epoch number 670/800\n",
      "Loss on training set: 0.8087961075537171 f_score on training set: 0.623742215048111, loss on test set: 1.4323309944712734 f_score on test set: 0.43929650607839466\n",
      "Epoch number 680/800\n",
      "Loss on training set: 0.5985128591599365 f_score on training set: 0.6669003082527672, loss on test set: 1.5562538419359275 f_score on test set: 0.38594600009610236\n",
      "Epoch number 690/800\n",
      "Loss on training set: 0.4442441236783593 f_score on training set: 0.6368955720768865, loss on test set: 1.1800615831107502 f_score on test set: 0.4819152950612723\n",
      "Epoch number 700/800\n",
      "Loss on training set: 0.7341599032090547 f_score on training set: 0.5487933634992459, loss on test set: 1.5314767760168633 f_score on test set: 0.39136831391121324\n",
      "Epoch number 710/800\n",
      "Loss on training set: 0.5240669035152964 f_score on training set: 0.6178116423912283, loss on test set: 1.4404749947878845 f_score on test set: 0.4078465940544536\n",
      "Epoch number 720/800\n",
      "Loss on training set: 0.9814447244659081 f_score on training set: 0.5012296881862099, loss on test set: 1.5601555837384424 f_score on test set: 0.40672114709682977\n",
      "Epoch number 730/800\n",
      "Loss on training set: 0.37023520604859833 f_score on training set: 0.6782913766443567, loss on test set: 1.1775245095903013 f_score on test set: 0.4623172346809263\n",
      "Epoch number 740/800\n",
      "Loss on training set: 0.6476096735592148 f_score on training set: 0.6565184862099323, loss on test set: 1.4727899280348806 f_score on test set: 0.40315133701205097\n",
      "Epoch number 750/800\n",
      "Loss on training set: 0.4494715941056354 f_score on training set: 0.6193682609043649, loss on test set: 1.2367612360910487 f_score on test set: 0.43717224655002956\n",
      "Epoch number 760/800\n",
      "Loss on training set: 0.3973441375826538 f_score on training set: 0.6695465356689868, loss on test set: 1.1388829412971504 f_score on test set: 0.44500902367617756\n",
      "Epoch number 770/800\n",
      "Loss on training set: 0.6037895973219558 f_score on training set: 0.6092378980577738, loss on test set: 1.236429731855765 f_score on test set: 0.45642804486575594\n",
      "Epoch number 780/800\n",
      "Loss on training set: 0.591523231729607 f_score on training set: 0.6398461353344612, loss on test set: 1.4609043588194603 f_score on test set: 0.3866241004475593\n",
      "Epoch number 790/800\n",
      "Loss on training set: 0.522951812283285 f_score on training set: 0.6647215518263623, loss on test set: 1.1710329739098655 f_score on test set: 0.4749317257502462\n",
      "Epoch number 800/800\n",
      "Loss on training set: 0.4050843907710361 f_score on training set: 0.6172644003324517, loss on test set: 1.1541217049375012 f_score on test set: 0.45083814992931603\n",
      "Epoch number 10/400\n",
      "Loss on training set: 0.2229755689661593 f_score on training set: 0.7842219451989385, loss on test set: 1.0449127273086256 f_score on test set: 0.4770348998987038\n",
      "Epoch number 20/400\n",
      "Loss on training set: 0.2103369086572235 f_score on training set: 0.7365018293753925, loss on test set: 1.037166436696804 f_score on test set: 0.47689239632850633\n",
      "Epoch number 30/400\n",
      "Loss on training set: 0.20445223576423568 f_score on training set: 0.7850239460851356, loss on test set: 1.043454945314797 f_score on test set: 0.4750357521666228\n",
      "Epoch number 40/400\n",
      "Loss on training set: 0.20751863869724693 f_score on training set: 0.8099690571606827, loss on test set: 1.0263392948704153 f_score on test set: 0.474493647377022\n",
      "Epoch number 50/400\n",
      "Loss on training set: 0.21604428196270503 f_score on training set: 0.7811517548066527, loss on test set: 1.0372946563189616 f_score on test set: 0.48268306253140036\n",
      "Epoch number 60/400\n",
      "Loss on training set: 0.21384890356677433 f_score on training set: 0.7633573743329842, loss on test set: 1.0374588023520257 f_score on test set: 0.4797224656139974\n",
      "Epoch number 70/400\n",
      "Loss on training set: 0.19936296286591215 f_score on training set: 0.8102057487480063, loss on test set: 1.0341860755018515 f_score on test set: 0.48032494656414493\n",
      "Epoch number 80/400\n",
      "Loss on training set: 0.20030629680579995 f_score on training set: 0.8048538356654515, loss on test set: 1.0225563950423966 f_score on test set: 0.48774395507034685\n",
      "Epoch number 90/400\n",
      "Loss on training set: 0.1892065814543955 f_score on training set: 0.7888344966020372, loss on test set: 1.0104147152084753 f_score on test set: 0.48696339968993035\n",
      "Epoch number 100/400\n",
      "Loss on training set: 0.2116597601163494 f_score on training set: 0.7395551462917489, loss on test set: 1.018231212363533 f_score on test set: 0.48444283569506436\n",
      "Epoch number 110/400\n",
      "Loss on training set: 0.19461253866009456 f_score on training set: 0.7573033597108022, loss on test set: 1.0011577287724944 f_score on test set: 0.49059104374929396\n",
      "Epoch number 120/400\n",
      "Loss on training set: 0.20951867014070144 f_score on training set: 0.7837342061391199, loss on test set: 0.9939069236749845 f_score on test set: 0.4932412651505455\n",
      "Epoch number 130/400\n",
      "Loss on training set: 0.2228382994149059 f_score on training set: 0.73993578894569, loss on test set: 1.0336165158031498 f_score on test set: 0.4712842442940984\n",
      "Epoch number 140/400\n",
      "Loss on training set: 0.20502731636850474 f_score on training set: 0.8007672260725127, loss on test set: 1.0209175580979926 f_score on test set: 0.48049451107871666\n",
      "Epoch number 150/400\n",
      "Loss on training set: 0.20918597915081175 f_score on training set: 0.7553072036016988, loss on test set: 1.032859091545779 f_score on test set: 0.47879207174779936\n",
      "Epoch number 160/400\n",
      "Loss on training set: 0.24054358573862492 f_score on training set: 0.752860026607654, loss on test set: 1.0581535594591225 f_score on test set: 0.45662086721788386\n",
      "Epoch number 170/400\n",
      "Loss on training set: 0.18625839448972747 f_score on training set: 0.8044220288514539, loss on test set: 0.9881761898388041 f_score on test set: 0.49318529859653215\n",
      "Epoch number 180/400\n",
      "Loss on training set: 0.20682350475784572 f_score on training set: 0.7345607901753229, loss on test set: 1.0017856072997438 f_score on test set: 0.4741676191108871\n",
      "Epoch number 190/400\n",
      "Loss on training set: 0.23721624721007556 f_score on training set: 0.7117224501530828, loss on test set: 0.9822854004813067 f_score on test set: 0.495013272458525\n",
      "Epoch number 200/400\n",
      "Loss on training set: 0.1935337671644552 f_score on training set: 0.7692495655453684, loss on test set: 1.0038061708856374 f_score on test set: 0.4861041718540803\n",
      "Epoch number 210/400\n",
      "Loss on training set: 0.19650599009513076 f_score on training set: 0.76982628367381, loss on test set: 0.9949064055744988 f_score on test set: 0.5071408455501518\n",
      "Epoch number 220/400\n",
      "Loss on training set: 0.18368367964288468 f_score on training set: 0.7775802327526464, loss on test set: 0.9779565889069681 f_score on test set: 0.49620832105974794\n",
      "Epoch number 230/400\n",
      "Loss on training set: 0.21950252354195238 f_score on training set: 0.7471835317343934, loss on test set: 0.9876686514001259 f_score on test set: 0.5131682321252267\n",
      "Epoch number 240/400\n",
      "Loss on training set: 0.19767785824927017 f_score on training set: 0.8254936120789779, loss on test set: 0.9999479405388864 f_score on test set: 0.49450133613156927\n",
      "Epoch number 250/400\n",
      "Loss on training set: 0.21371704316181442 f_score on training set: 0.764910238548648, loss on test set: 0.978425021143867 f_score on test set: 0.4963773346061009\n",
      "Epoch number 260/400\n",
      "Loss on training set: 0.20395187916718255 f_score on training set: 0.7902888565582827, loss on test set: 1.0105762742239845 f_score on test set: 0.5004386058387352\n",
      "Epoch number 270/400\n",
      "Loss on training set: 0.21949337248571907 f_score on training set: 0.7994389248120592, loss on test set: 0.9761346086255113 f_score on test set: 0.5037885404732712\n",
      "Epoch number 280/400\n",
      "Loss on training set: 0.18008509902701292 f_score on training set: 0.7775814031020087, loss on test set: 0.9788259273075404 f_score on test set: 0.4866777398216\n",
      "Epoch number 290/400\n",
      "Loss on training set: 0.18623101343364606 f_score on training set: 0.819966341584488, loss on test set: 0.989972312142922 f_score on test set: 0.4992036733459958\n",
      "Epoch number 300/400\n",
      "Loss on training set: 0.1940403370656748 f_score on training set: 0.7957408350145495, loss on test set: 0.9951447934296522 f_score on test set: 0.48889671574423355\n",
      "Epoch number 310/400\n",
      "Loss on training set: 0.18046592817786053 f_score on training set: 0.8202843623977043, loss on test set: 0.9826649954308515 f_score on test set: 0.4933609041421218\n",
      "Epoch number 320/400\n",
      "Loss on training set: 0.1862789907189852 f_score on training set: 0.8302560012918692, loss on test set: 0.97429389077796 f_score on test set: 0.4987748069780178\n",
      "Epoch number 330/400\n",
      "Loss on training set: 0.19293202481961394 f_score on training set: 0.8138823485658278, loss on test set: 0.9801611301366465 f_score on test set: 0.5062831618820008\n",
      "Epoch number 340/400\n",
      "Loss on training set: 0.20463242501986695 f_score on training set: 0.7704740111147433, loss on test set: 1.011886261466935 f_score on test set: 0.4774909235945485\n",
      "Epoch number 350/400\n",
      "Loss on training set: 0.18390988178863107 f_score on training set: 0.8105253779790137, loss on test set: 0.9768985453657262 f_score on test set: 0.5104551439969945\n",
      "Epoch number 360/400\n",
      "Loss on training set: 0.18193126230892712 f_score on training set: 0.8100646034162017, loss on test set: 0.987399439770681 f_score on test set: 0.48672550374509427\n",
      "Epoch number 370/400\n",
      "Loss on training set: 0.18232608145245757 f_score on training set: 0.8301208149206721, loss on test set: 0.9748390258612897 f_score on test set: 0.5100481777675374\n",
      "Epoch number 380/400\n",
      "Loss on training set: 0.1973240980696071 f_score on training set: 0.7828258261999003, loss on test set: 0.9934523516661319 f_score on test set: 0.4904006421991372\n",
      "Epoch number 390/400\n",
      "Loss on training set: 0.19199360930849843 f_score on training set: 0.7853972494919579, loss on test set: 0.9901729392816291 f_score on test set: 0.4880271631754236\n",
      "Epoch number 400/400\n",
      "Loss on training set: 0.2038476231899557 f_score on training set: 0.7746022826622583, loss on test set: 0.9770815174190397 f_score on test set: 0.5114254240395469\n",
      "Epoch number 10/800\n",
      "Loss on training set: 2.544790157574099 f_score on training set: 0.4046464785817755, loss on test set: 2.5606631442509613 f_score on test set: 0.39305660736123904\n",
      "Epoch number 20/800\n",
      "Loss on training set: 1.7978832882362576 f_score on training set: 0.4503500405931494, loss on test set: 2.4424727463476557 f_score on test set: 0.354564264811813\n",
      "Epoch number 30/800\n",
      "Loss on training set: 2.1833219323580026 f_score on training set: 0.4302873920032353, loss on test set: 2.514975776504326 f_score on test set: 0.3457272773002224\n",
      "Epoch number 40/800\n",
      "Loss on training set: 2.382827949070963 f_score on training set: 0.34828733791105476, loss on test set: 2.6676976489458846 f_score on test set: 0.38958325351899664\n",
      "Epoch number 50/800\n",
      "Loss on training set: 1.2382105460946005 f_score on training set: 0.4467758936364352, loss on test set: 2.141374795141233 f_score on test set: 0.28366748807490544\n",
      "Epoch number 60/800\n",
      "Loss on training set: 1.119477568120497 f_score on training set: 0.5401712234682428, loss on test set: 1.785890139201486 f_score on test set: 0.3866298323967726\n",
      "Epoch number 70/800\n",
      "Loss on training set: 0.917500487112288 f_score on training set: 0.584295952579216, loss on test set: 1.6766103926092073 f_score on test set: 0.4035418667648215\n",
      "Epoch number 80/800\n",
      "Loss on training set: 1.4227505208668643 f_score on training set: 0.42058479532163745, loss on test set: 1.820414290373681 f_score on test set: 0.37975859604601114\n",
      "Epoch number 90/800\n",
      "Loss on training set: 1.4682606416658275 f_score on training set: 0.5114100089105325, loss on test set: 2.3972100749550505 f_score on test set: 0.3814548723223367\n",
      "Epoch number 100/800\n",
      "Loss on training set: 1.0598986643905424 f_score on training set: 0.5041313276336881, loss on test set: 1.6871488627777973 f_score on test set: 0.42999514875711675\n",
      "Epoch number 110/800\n",
      "Loss on training set: 0.7520930561290136 f_score on training set: 0.5852948689764181, loss on test set: 1.520064356990757 f_score on test set: 0.4567823296869862\n",
      "Epoch number 120/800\n",
      "Loss on training set: 1.211968428640902 f_score on training set: 0.5121914092955082, loss on test set: 2.049067449690747 f_score on test set: 0.3653050855138617\n",
      "Epoch number 130/800\n",
      "Loss on training set: 0.9250855234591027 f_score on training set: 0.5462991507702812, loss on test set: 1.8248533864328813 f_score on test set: 0.38805967636929756\n",
      "Epoch number 140/800\n",
      "Loss on training set: 1.217716123932229 f_score on training set: 0.49453990048521, loss on test set: 1.6302342830797176 f_score on test set: 0.45776364043581125\n",
      "Epoch number 150/800\n",
      "Loss on training set: 1.1820749595577351 f_score on training set: 0.5150917824608805, loss on test set: 1.9677625480726515 f_score on test set: 0.3694150902315642\n",
      "Epoch number 160/800\n",
      "Loss on training set: 1.5578140639361895 f_score on training set: 0.4890273988161807, loss on test set: 2.325719151039963 f_score on test set: 0.3656156597327794\n",
      "Epoch number 170/800\n",
      "Loss on training set: 0.8622416174274936 f_score on training set: 0.5388247492948498, loss on test set: 1.9285073705245406 f_score on test set: 0.3603860238129028\n",
      "Epoch number 180/800\n",
      "Loss on training set: 0.9750461249508867 f_score on training set: 0.5126157292260268, loss on test set: 1.7640156177061412 f_score on test set: 0.4164821285804146\n",
      "Epoch number 190/800\n",
      "Loss on training set: 0.8838855411980346 f_score on training set: 0.5233227793847808, loss on test set: 1.6190118545387093 f_score on test set: 0.40504835177286963\n",
      "Epoch number 200/800\n",
      "Loss on training set: 1.1951162801381223 f_score on training set: 0.49439125668160616, loss on test set: 1.6166525811704635 f_score on test set: 0.41245132310977095\n",
      "Epoch number 210/800\n",
      "Loss on training set: 1.3724158387300132 f_score on training set: 0.5224332360477385, loss on test set: 2.076873731336666 f_score on test set: 0.38051659348046435\n",
      "Epoch number 220/800\n",
      "Loss on training set: 0.7085060588413141 f_score on training set: 0.5960709147005576, loss on test set: 1.4188038066160533 f_score on test set: 0.45218965856203236\n",
      "Epoch number 230/800\n",
      "Loss on training set: 1.521817215401856 f_score on training set: 0.532299622452388, loss on test set: 2.178154668789953 f_score on test set: 0.39139893222197564\n",
      "Epoch number 240/800\n",
      "Loss on training set: 0.8346647686677439 f_score on training set: 0.593966614961931, loss on test set: 1.6151013929652434 f_score on test set: 0.4295075342160593\n",
      "Epoch number 250/800\n",
      "Loss on training set: 0.5826264034280961 f_score on training set: 0.6283884879065601, loss on test set: 1.4839395509344036 f_score on test set: 0.4368777235451904\n",
      "Epoch number 260/800\n",
      "Loss on training set: 0.5469632128331355 f_score on training set: 0.6544893994893995, loss on test set: 1.3291329630095337 f_score on test set: 0.4610166814909444\n",
      "Epoch number 270/800\n",
      "Loss on training set: 0.5001111634314569 f_score on training set: 0.6404801819363222, loss on test set: 1.335696349528887 f_score on test set: 0.4475369312507031\n",
      "Epoch number 280/800\n",
      "Loss on training set: 0.6843831915781526 f_score on training set: 0.6016548268863664, loss on test set: 1.4879870564724829 f_score on test set: 0.4375864940767122\n",
      "Epoch number 290/800\n",
      "Loss on training set: 0.8257419203801384 f_score on training set: 0.5583910674859024, loss on test set: 1.4271522908323628 f_score on test set: 0.4405436424669182\n",
      "Epoch number 300/800\n",
      "Loss on training set: 1.1568860568289883 f_score on training set: 0.46007410458368897, loss on test set: 1.4608633538901776 f_score on test set: 0.44877956288620974\n",
      "Epoch number 310/800\n",
      "Loss on training set: 1.1249913159431384 f_score on training set: 0.552577760626275, loss on test set: 1.922317991832432 f_score on test set: 0.36157199912429644\n",
      "Epoch number 320/800\n",
      "Loss on training set: 0.919237062103351 f_score on training set: 0.5626622718849138, loss on test set: 1.6356805776136738 f_score on test set: 0.42340439796173274\n",
      "Epoch number 330/800\n",
      "Loss on training set: 1.0423882499183723 f_score on training set: 0.589276121879425, loss on test set: 1.711869907229768 f_score on test set: 0.405304296728008\n",
      "Epoch number 340/800\n",
      "Loss on training set: 0.6026249862911937 f_score on training set: 0.6466991553680144, loss on test set: 1.3604175849546323 f_score on test set: 0.4375475083846563\n",
      "Epoch number 350/800\n",
      "Loss on training set: 0.748467893513416 f_score on training set: 0.5868144395704425, loss on test set: 1.5557902796548062 f_score on test set: 0.4112225582648165\n",
      "Epoch number 360/800\n",
      "Loss on training set: 1.2026451734329722 f_score on training set: 0.4808975315076262, loss on test set: 1.6778537805276126 f_score on test set: 0.38659964210512837\n",
      "Epoch number 370/800\n",
      "Loss on training set: 0.8290272115667793 f_score on training set: 0.6234828390000804, loss on test set: 1.725867278622558 f_score on test set: 0.4314425291443708\n",
      "Epoch number 380/800\n",
      "Loss on training set: 0.9356983326702629 f_score on training set: 0.6115667882052938, loss on test set: 1.6512331269718012 f_score on test set: 0.38576742666803115\n",
      "Epoch number 390/800\n",
      "Loss on training set: 0.9666875620645571 f_score on training set: 0.6020099701475234, loss on test set: 1.6707692082617596 f_score on test set: 0.38903403102745976\n",
      "Epoch number 400/800\n",
      "Loss on training set: 0.7621984506135345 f_score on training set: 0.6085065411683434, loss on test set: 1.4971639707020454 f_score on test set: 0.42756114679339235\n",
      "Epoch number 410/800\n",
      "Loss on training set: 0.5427489788810003 f_score on training set: 0.6639513635140262, loss on test set: 1.2775951626570463 f_score on test set: 0.45685162145905667\n",
      "Epoch number 420/800\n",
      "Loss on training set: 1.0564490052361752 f_score on training set: 0.5879235527186405, loss on test set: 1.6809046082041441 f_score on test set: 0.37457515509963707\n",
      "Epoch number 430/800\n",
      "Loss on training set: 1.2792756977109105 f_score on training set: 0.5245272448502076, loss on test set: 1.7794962876012912 f_score on test set: 0.3909421877593712\n",
      "Epoch number 440/800\n",
      "Loss on training set: 0.6396719318938242 f_score on training set: 0.6298692866704916, loss on test set: 1.3279083346970253 f_score on test set: 0.4330048355746189\n",
      "Epoch number 450/800\n",
      "Loss on training set: 1.1623812027367295 f_score on training set: 0.5437417069093432, loss on test set: 1.5794765342098294 f_score on test set: 0.44426600058356625\n",
      "Epoch number 460/800\n",
      "Loss on training set: 0.8075027031178371 f_score on training set: 0.6258619047945373, loss on test set: 1.4723646844007052 f_score on test set: 0.44571661154077097\n",
      "Epoch number 470/800\n",
      "Loss on training set: 0.5991596796111488 f_score on training set: 0.6136306686500258, loss on test set: 1.2858601141790653 f_score on test set: 0.42417023323452774\n",
      "Epoch number 480/800\n",
      "Loss on training set: 0.6784954268421867 f_score on training set: 0.6680548834581073, loss on test set: 1.2259036889066004 f_score on test set: 0.46100134058890213\n",
      "Epoch number 490/800\n",
      "Loss on training set: 0.8382078173151902 f_score on training set: 0.5988566309614197, loss on test set: 1.6624510584474528 f_score on test set: 0.3527998618095625\n",
      "Epoch number 500/800\n",
      "Loss on training set: 0.4211093230428256 f_score on training set: 0.6357498666344576, loss on test set: 1.2319745184393367 f_score on test set: 0.4260204308186059\n",
      "Epoch number 510/800\n",
      "Loss on training set: 0.7894320227718344 f_score on training set: 0.6154418640236785, loss on test set: 1.448294146735944 f_score on test set: 0.42052831932531665\n",
      "Epoch number 520/800\n",
      "Loss on training set: 0.5765332412660803 f_score on training set: 0.6823497758842104, loss on test set: 1.3984438194124993 f_score on test set: 0.43989354617606335\n",
      "Epoch number 530/800\n",
      "Loss on training set: 0.9991453685696714 f_score on training set: 0.6085611968239298, loss on test set: 1.627888591666956 f_score on test set: 0.3924252201571784\n",
      "Epoch number 540/800\n",
      "Loss on training set: 0.7903157325225623 f_score on training set: 0.6232430376737307, loss on test set: 1.4458946818378773 f_score on test set: 0.41888370184742757\n",
      "Epoch number 550/800\n",
      "Loss on training set: 0.5109397045726525 f_score on training set: 0.6489837197196054, loss on test set: 1.3287288039475191 f_score on test set: 0.4086327703747258\n",
      "Epoch number 560/800\n",
      "Loss on training set: 0.5807951248432771 f_score on training set: 0.6731954953003643, loss on test set: 1.2902098108500692 f_score on test set: 0.45726463573657034\n",
      "Epoch number 570/800\n",
      "Loss on training set: 0.5856105737893184 f_score on training set: 0.6449025506173114, loss on test set: 1.2426189605947817 f_score on test set: 0.4196474587443801\n",
      "Epoch number 580/800\n",
      "Loss on training set: 0.8325244240468352 f_score on training set: 0.5832632881201428, loss on test set: 1.57923982990086 f_score on test set: 0.3694697518235768\n",
      "Epoch number 590/800\n",
      "Loss on training set: 0.5245535467681169 f_score on training set: 0.679374290076102, loss on test set: 1.2539338755778535 f_score on test set: 0.46851726694896595\n",
      "Epoch number 600/800\n",
      "Loss on training set: 0.35647712371157264 f_score on training set: 0.7181577968962801, loss on test set: 1.1064553494185763 f_score on test set: 0.48678055906453277\n",
      "Epoch number 610/800\n",
      "Loss on training set: 0.6623466826028261 f_score on training set: 0.660715227750012, loss on test set: 1.3086316123367976 f_score on test set: 0.4432342459822485\n",
      "Epoch number 620/800\n",
      "Loss on training set: 0.5956762720782692 f_score on training set: 0.6409501651910311, loss on test set: 1.3667003890572584 f_score on test set: 0.45492449070889596\n",
      "Epoch number 630/800\n",
      "Loss on training set: 0.3483711847696535 f_score on training set: 0.6609280103568261, loss on test set: 1.1147481342690655 f_score on test set: 0.4483726680622281\n",
      "Epoch number 640/800\n",
      "Loss on training set: 0.6269318949106776 f_score on training set: 0.6305212966682743, loss on test set: 1.1842316190084676 f_score on test set: 0.4221992079040644\n",
      "Epoch number 650/800\n",
      "Loss on training set: 0.3325431484649668 f_score on training set: 0.760053860666839, loss on test set: 1.0997613047585426 f_score on test set: 0.4863669549534113\n",
      "Epoch number 660/800\n",
      "Loss on training set: 0.6268777249318201 f_score on training set: 0.6048104196955179, loss on test set: 1.1648588768684531 f_score on test set: 0.4500598025146471\n",
      "Epoch number 670/800\n",
      "Loss on training set: 0.6954033704719338 f_score on training set: 0.6483955969229461, loss on test set: 1.3412248902287347 f_score on test set: 0.4499293464465486\n",
      "Epoch number 680/800\n",
      "Loss on training set: 1.0877567256857241 f_score on training set: 0.5890895481889357, loss on test set: 1.9465273059216077 f_score on test set: 0.35920863171612516\n",
      "Epoch number 690/800\n",
      "Loss on training set: 0.3750412828092779 f_score on training set: 0.676485316619345, loss on test set: 1.2214437739753843 f_score on test set: 0.43927147891398816\n",
      "Epoch number 700/800\n",
      "Loss on training set: 0.5264572900296453 f_score on training set: 0.67975733492706, loss on test set: 1.191218435730134 f_score on test set: 0.4511274419286474\n",
      "Epoch number 710/800\n",
      "Loss on training set: 0.4507614852839916 f_score on training set: 0.6780926561361343, loss on test set: 1.1526073519552804 f_score on test set: 0.46093596269027465\n",
      "Epoch number 720/800\n",
      "Loss on training set: 0.4112757340532163 f_score on training set: 0.6935984754875311, loss on test set: 1.196468204062229 f_score on test set: 0.4418426049070676\n",
      "Epoch number 730/800\n",
      "Loss on training set: 0.5982010456554613 f_score on training set: 0.6767012950619509, loss on test set: 1.2090442043295213 f_score on test set: 0.4600973806040678\n",
      "Epoch number 740/800\n",
      "Loss on training set: 0.5382645367800355 f_score on training set: 0.7272389945426362, loss on test set: 1.2819188600598548 f_score on test set: 0.4570524040811275\n",
      "Epoch number 750/800\n",
      "Loss on training set: 1.0844095060307064 f_score on training set: 0.5870802915567657, loss on test set: 1.647223776958353 f_score on test set: 0.4168929578404763\n",
      "Epoch number 760/800\n",
      "Loss on training set: 0.45843769765612746 f_score on training set: 0.6840320498215234, loss on test set: 1.2351362158116805 f_score on test set: 0.45410376405014063\n",
      "Epoch number 770/800\n",
      "Loss on training set: 0.5079874894154772 f_score on training set: 0.6562115688948889, loss on test set: 1.326658704579003 f_score on test set: 0.3805346194442665\n",
      "Epoch number 780/800\n",
      "Loss on training set: 0.44957978722055364 f_score on training set: 0.6837535014005602, loss on test set: 1.2092746460694286 f_score on test set: 0.4300715190482321\n",
      "Epoch number 790/800\n",
      "Loss on training set: 0.5987606101087426 f_score on training set: 0.7513333861720959, loss on test set: 1.1870050078741399 f_score on test set: 0.49232949452521785\n",
      "Epoch number 800/800\n",
      "Loss on training set: 0.7992206848227277 f_score on training set: 0.6099292253680574, loss on test set: 1.4156336327370798 f_score on test set: 0.436384036348497\n",
      "Epoch number 10/400\n",
      "Loss on training set: 0.2115773798263832 f_score on training set: 0.7999703667225855, loss on test set: 0.9716132341270252 f_score on test set: 0.48668473663478445\n",
      "Epoch number 20/400\n",
      "Loss on training set: 0.20822884018890675 f_score on training set: 0.8293364528637126, loss on test set: 0.9540545461261091 f_score on test set: 0.5046470044595337\n",
      "Epoch number 30/400\n",
      "Loss on training set: 0.2289066234034246 f_score on training set: 0.8078383648707208, loss on test set: 0.9329103701389615 f_score on test set: 0.5027758265312767\n",
      "Epoch number 40/400\n",
      "Loss on training set: 0.1991124757128372 f_score on training set: 0.8371238710379649, loss on test set: 0.9350601992185906 f_score on test set: 0.5035458530625799\n",
      "Epoch number 50/400\n",
      "Loss on training set: 0.25131839296908864 f_score on training set: 0.8299077134614022, loss on test set: 0.9429875391010539 f_score on test set: 0.508763506371857\n",
      "Epoch number 60/400\n",
      "Loss on training set: 0.17660882323205257 f_score on training set: 0.8395529738610925, loss on test set: 0.8826140075037685 f_score on test set: 0.5243795164092889\n",
      "Epoch number 70/400\n",
      "Loss on training set: 0.18953335150751002 f_score on training set: 0.8027268179015482, loss on test set: 0.9211163188288798 f_score on test set: 0.49373304331822493\n",
      "Epoch number 80/400\n",
      "Loss on training set: 0.1853969649493001 f_score on training set: 0.8238011187958689, loss on test set: 0.9053057068541015 f_score on test set: 0.5043555430340344\n",
      "Epoch number 90/400\n",
      "Loss on training set: 0.1797032595903719 f_score on training set: 0.8138590346698912, loss on test set: 0.8965700076916251 f_score on test set: 0.5056278944198015\n",
      "Epoch number 100/400\n",
      "Loss on training set: 0.18217963578119234 f_score on training set: 0.825959175959176, loss on test set: 0.9091822518448793 f_score on test set: 0.5209171319645808\n",
      "Epoch number 110/400\n",
      "Loss on training set: 0.18874546123585734 f_score on training set: 0.8161965811965812, loss on test set: 0.8944403073680087 f_score on test set: 0.5131322292496707\n",
      "Epoch number 120/400\n",
      "Loss on training set: 0.19648868294397762 f_score on training set: 0.791972088216006, loss on test set: 0.9204544436414842 f_score on test set: 0.48987115332121245\n",
      "Epoch number 130/400\n",
      "Loss on training set: 0.184678049002405 f_score on training set: 0.8302836169502836, loss on test set: 0.8877147679593107 f_score on test set: 0.52046938603004\n",
      "Epoch number 140/400\n",
      "Loss on training set: 0.17132462489691197 f_score on training set: 0.834248483193631, loss on test set: 0.9069570498591567 f_score on test set: 0.49714049906072216\n",
      "Epoch number 150/400\n",
      "Loss on training set: 0.19964980897146592 f_score on training set: 0.8112360847408501, loss on test set: 0.9044506511012341 f_score on test set: 0.5107368525691095\n",
      "Epoch number 160/400\n",
      "Loss on training set: 0.1675446209082367 f_score on training set: 0.8348398174841524, loss on test set: 0.8752047699317543 f_score on test set: 0.5184551456891223\n",
      "Epoch number 170/400\n",
      "Loss on training set: 0.20558328768011236 f_score on training set: 0.8255406763355918, loss on test set: 0.8673235169728177 f_score on test set: 0.5296716193101336\n",
      "Epoch number 180/400\n",
      "Loss on training set: 0.18289431818452156 f_score on training set: 0.8165514191413936, loss on test set: 0.8642557323564203 f_score on test set: 0.5280630641847373\n",
      "Epoch number 190/400\n",
      "Loss on training set: 0.20994440524534877 f_score on training set: 0.8025968856617969, loss on test set: 0.9230504332903445 f_score on test set: 0.4810376998975127\n",
      "Epoch number 200/400\n",
      "Loss on training set: 0.16266438135536485 f_score on training set: 0.8511315387568428, loss on test set: 0.8845130070097388 f_score on test set: 0.5225663781197485\n",
      "Epoch number 210/400\n",
      "Loss on training set: 0.16938204339179885 f_score on training set: 0.8023439087123152, loss on test set: 0.8964238601969797 f_score on test set: 0.5092590934580389\n",
      "Epoch number 220/400\n",
      "Loss on training set: 0.1600946729687208 f_score on training set: 0.8240694931625235, loss on test set: 0.8814776689939358 f_score on test set: 0.5161743246141568\n",
      "Epoch number 230/400\n",
      "Loss on training set: 0.17612256489131714 f_score on training set: 0.8477345936664656, loss on test set: 0.8926680669164209 f_score on test set: 0.5074087630673392\n",
      "Epoch number 240/400\n",
      "Loss on training set: 0.1832959084038608 f_score on training set: 0.8482963791739213, loss on test set: 0.8981052505844316 f_score on test set: 0.5090649614083942\n",
      "Epoch number 250/400\n",
      "Loss on training set: 0.18208236039058703 f_score on training set: 0.8336274791696477, loss on test set: 0.8862187511252337 f_score on test set: 0.5105364356232927\n",
      "Epoch number 260/400\n",
      "Loss on training set: 0.18380949648478803 f_score on training set: 0.8219422551112275, loss on test set: 0.9227860790826249 f_score on test set: 0.4973837585453396\n",
      "Epoch number 270/400\n",
      "Loss on training set: 0.18961607293647068 f_score on training set: 0.8025343743789545, loss on test set: 0.9161652381669494 f_score on test set: 0.4802964167124101\n",
      "Epoch number 280/400\n",
      "Loss on training set: 0.17590627889746407 f_score on training set: 0.8585538702566307, loss on test set: 0.8916048494296092 f_score on test set: 0.5101358246487127\n",
      "Epoch number 290/400\n",
      "Loss on training set: 0.16900997507985038 f_score on training set: 0.820189630951258, loss on test set: 0.8757383981571244 f_score on test set: 0.5224799923272769\n",
      "Epoch number 300/400\n",
      "Loss on training set: 0.15635466177754057 f_score on training set: 0.8439429581999244, loss on test set: 0.8800385898217743 f_score on test set: 0.5083408870011086\n",
      "Epoch number 310/400\n",
      "Loss on training set: 0.17691518874259668 f_score on training set: 0.8175685425685425, loss on test set: 0.8976639028558612 f_score on test set: 0.514582630605471\n",
      "Epoch number 320/400\n",
      "Loss on training set: 0.16870749466693633 f_score on training set: 0.8162948620818777, loss on test set: 0.8894533425985012 f_score on test set: 0.5104406465149294\n",
      "Epoch number 330/400\n",
      "Loss on training set: 0.18406897690326862 f_score on training set: 0.8054942417603516, loss on test set: 0.9321275369695522 f_score on test set: 0.48483827151919684\n",
      "Epoch number 340/400\n",
      "Loss on training set: 0.16939925891828506 f_score on training set: 0.8382165389511426, loss on test set: 0.8782685015643242 f_score on test set: 0.5175309193903047\n",
      "Epoch number 350/400\n",
      "Loss on training set: 0.15231420851027905 f_score on training set: 0.8441517269030301, loss on test set: 0.8790981917251278 f_score on test set: 0.5098990317437362\n",
      "Epoch number 360/400\n",
      "Loss on training set: 0.17451825004844998 f_score on training set: 0.8437018193967744, loss on test set: 0.8914926761360507 f_score on test set: 0.5101195164229619\n",
      "Epoch number 370/400\n",
      "Loss on training set: 0.16030993719614936 f_score on training set: 0.8290855064384476, loss on test set: 0.8817491048108977 f_score on test set: 0.5182073230419653\n",
      "Epoch number 380/400\n",
      "Loss on training set: 0.1916031256280488 f_score on training set: 0.8382244669934984, loss on test set: 0.8786004825266729 f_score on test set: 0.5176379052860566\n",
      "Epoch number 390/400\n",
      "Loss on training set: 0.17637639980713105 f_score on training set: 0.8255489896717967, loss on test set: 0.8713231064820147 f_score on test set: 0.5326549688615929\n",
      "Epoch number 400/400\n",
      "Loss on training set: 0.16809495153916548 f_score on training set: 0.8394523389612608, loss on test set: 0.8711564974919426 f_score on test set: 0.5186455937937458\n"
     ]
    }
   ],
   "source": [
    "r5_no_reg_build = {'input_shape': r5_x_train.shape, 'neurons_num': [40, 40, 5], 'activations': [ReLU(), ReLU(), Softmax()]}\n",
    "r5_no_reg_fit = [{'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 800, 'learning_rate': 0.00005, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 400, 'learning_rate': 0.00001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test, r5_no_reg_nns = cv_network(build_args=r5_no_reg_build, fit_args=r5_no_reg_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8109249100349232 0.5021059057537205\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def average_loss(nns):\n",
    "    n = len(nns) - 1\n",
    "    sum_loss_train = np.array(nns[1].history['loss_train'])\n",
    "    sum_loss_test = np.array(nns[1].history['loss_test'])\n",
    "    for i in range(2, len(nns)):\n",
    "        sum_loss_train = np.add(sum_loss_train, np.array(nns[i].history['loss_train']))\n",
    "        sum_loss_test = np.add(sum_loss_test, np.array(nns[i].history['loss_test']))\n",
    "    return sum_loss_train / n, sum_loss_test / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum_loss_train = np.array(r5_no_reg_nns[1].history['loss_train'])\n",
    "sum_loss_test = np.array(r5_no_reg_nns[1].history['loss_test'])\n",
    "for i in range(2, 5):\n",
    "    sum_loss_train = np.add(sum_loss_train, np.array(r5_no_reg_nns[i].history['loss_train']))\n",
    "    sum_loss_test = np.add(sum_loss_test, np.array(r5_no_reg_nns[i].history['loss_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sum_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "avg_loss_r5_no_reg = average_loss(r5_no_reg_nns)\n",
    "avg_loss_r5_l2 = average_loss(r5_l2_nns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(1200), avg_loss_r5_no_reg[0])\n",
    "plt.ylim(0, 5)\n",
    "plt.title('Uśredniona funkcja straty bez regularyzacji')\n",
    "plt.plot(np.arange(1200), avg_loss_r5_no_reg[1], c='red')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(270), avg_loss_r5_l2[0])\n",
    "plt.ylim(0, 5)\n",
    "plt.title('Uśredniona funkcja straty z regularyzacją L2')\n",
    "plt.plot(np.arange(270), avg_loss_r5_l2[1], c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "r5_l2_build = {'input_shape': r5_x_train.shape, 'neurons_num': [40, 40, 5], 'activations': [ReLU(), ReLU(), Softmax()]}\n",
    "r5_l2_fit = [{'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 800, 'learning_rate': 0.00005, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0.1},\n",
    "                 {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 400, 'learning_rate': 0.00001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0.1}]\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test = cv_network(build_args=r5_l2_build, fit_args=r5_l2_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/70\n",
      "Loss on training set: 1.6241255570698956 f_score on training set: 0.36836806747498085, loss on test set: 2.009509933282919 f_score on test set: 0.3144414344798424\n",
      "Epoch number 20/70\n",
      "Loss on training set: 0.7417570005821565 f_score on training set: 0.4763483665943171, loss on test set: 1.058472179391965 f_score on test set: 0.381883067062043\n",
      "Epoch number 30/70\n",
      "Loss on training set: 0.5443219182930841 f_score on training set: 0.4916899154308436, loss on test set: 0.7626522253325697 f_score on test set: 0.42544298323462953\n",
      "Epoch number 40/70\n",
      "Loss on training set: 0.40817020338236754 f_score on training set: 0.48223407168640436, loss on test set: 0.5714530161097913 f_score on test set: 0.36179913651470763\n",
      "Epoch number 50/70\n",
      "Loss on training set: 0.33919715451711846 f_score on training set: 0.48005407546266576, loss on test set: 0.45888246748233413 f_score on test set: 0.3850065612773246\n",
      "Epoch number 60/70\n",
      "Loss on training set: 0.32218682344544025 f_score on training set: 0.5506037141467239, loss on test set: 0.3783017190514168 f_score on test set: 0.4683507788596075\n",
      "Epoch number 70/70\n",
      "Loss on training set: 0.33065293792787676 f_score on training set: 0.5470720696236099, loss on test set: 0.3751320812486763 f_score on test set: 0.4717414581120593\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.33190582589153106 f_score on training set: 0.560611991186933, loss on test set: 0.3752150833534144 f_score on test set: 0.4738608125221944\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.3332750492610857 f_score on training set: 0.5666748281382428, loss on test set: 0.37513228359031264 f_score on test set: 0.4798951038685078\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.33470691915995104 f_score on training set: 0.5608948493273567, loss on test set: 0.3750465968111107 f_score on test set: 0.4833103154000016\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.3360722193951725 f_score on training set: 0.555063186479929, loss on test set: 0.3754012921032123 f_score on test set: 0.4848188283794899\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.33743250849916545 f_score on training set: 0.5671497024603969, loss on test set: 0.3749509632333598 f_score on test set: 0.48337692979161545\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.3388087904869025 f_score on training set: 0.5487655563878079, loss on test set: 0.3755889749952554 f_score on test set: 0.48673092055847245\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.3401465343716307 f_score on training set: 0.539934980461043, loss on test set: 0.3759141923754909 f_score on test set: 0.4858483962606093\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.34150189281029364 f_score on training set: 0.5376854256854258, loss on test set: 0.37609709713145795 f_score on test set: 0.4882649142443205\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.3428054319004543 f_score on training set: 0.5361054561054561, loss on test set: 0.37624971113060474 f_score on test set: 0.48660441359598544\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.3441236184350542 f_score on training set: 0.5361054561054561, loss on test set: 0.3767216621018842 f_score on test set: 0.48685112892344534\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.34543099995168286 f_score on training set: 0.5403575003575004, loss on test set: 0.3769679884250259 f_score on test set: 0.4885548941429878\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.34673473503292745 f_score on training set: 0.5411161379645104, loss on test set: 0.3773138259336504 f_score on test set: 0.48954187691045287\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.34802975856814916 f_score on training set: 0.5411161379645104, loss on test set: 0.3777054434313215 f_score on test set: 0.48772149797454756\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.3492978378982516 f_score on training set: 0.5411161379645104, loss on test set: 0.3781821306195855 f_score on test set: 0.48896824390651694\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.3505845205186954 f_score on training set: 0.5341766043181138, loss on test set: 0.3786305153952396 f_score on test set: 0.48816083665931775\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.3518493994463403 f_score on training set: 0.5341766043181138, loss on test set: 0.3790367780283462 f_score on test set: 0.4884944642192182\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.3530721709813042 f_score on training set: 0.5413705982127034, loss on test set: 0.37941859066984396 f_score on test set: 0.49047534443394486\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.3543181720455981 f_score on training set: 0.5339550587242278, loss on test set: 0.38012429024183086 f_score on test set: 0.4912967724700946\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.3555672402903582 f_score on training set: 0.5339550587242278, loss on test set: 0.38058268928646294 f_score on test set: 0.4911969453766858\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.3568846441911911 f_score on training set: 0.5339550587242278, loss on test set: 0.38118026907758895 f_score on test set: 0.4901012710567714\n",
      "Epoch number 10/70\n",
      "Loss on training set: 1.8927701221183904 f_score on training set: 0.3813948816731946, loss on test set: 2.3441745466397124 f_score on test set: 0.2739300946736061\n",
      "Epoch number 20/70\n",
      "Loss on training set: 0.5870726321683842 f_score on training set: 0.5897458187179097, loss on test set: 1.0552299091371067 f_score on test set: 0.4320461755601623\n",
      "Epoch number 30/70\n",
      "Loss on training set: 0.5310165353364916 f_score on training set: 0.475484180067604, loss on test set: 0.6769916080727304 f_score on test set: 0.41161028390467236\n",
      "Epoch number 40/70\n",
      "Loss on training set: 0.3924243638831965 f_score on training set: 0.5634772924862684, loss on test set: 0.6141564746674972 f_score on test set: 0.3925880510289709\n",
      "Epoch number 50/70\n",
      "Loss on training set: 0.3660313505710265 f_score on training set: 0.6068334590440613, loss on test set: 0.44770038234139403 f_score on test set: 0.4467674615969849\n",
      "Epoch number 60/70\n",
      "Loss on training set: 0.3044287083398547 f_score on training set: 0.5975949364184658, loss on test set: 0.3841106543136861 f_score on test set: 0.4903743248980763\n",
      "Epoch number 70/70\n",
      "Loss on training set: 0.3210883058078036 f_score on training set: 0.6059228937225052, loss on test set: 0.39709660343525327 f_score on test set: 0.45245339821701\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.3177163069185611 f_score on training set: 0.6335009658965327, loss on test set: 0.37601613471584017 f_score on test set: 0.4770253317769143\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.3196201083167989 f_score on training set: 0.622969751631705, loss on test set: 0.3756716246333445 f_score on test set: 0.48040391691604295\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.3215476816252164 f_score on training set: 0.638040993197138, loss on test set: 0.3763351431291473 f_score on test set: 0.47678657790833145\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.3234092760566566 f_score on training set: 0.6322144075021313, loss on test set: 0.37593300963644044 f_score on test set: 0.4825021459093473\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.3252950461722723 f_score on training set: 0.6316391941391941, loss on test set: 0.37637252051322256 f_score on test set: 0.4845869949740662\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.32720048296391135 f_score on training set: 0.6360412662879948, loss on test set: 0.37567244823116547 f_score on test set: 0.4912761923599146\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.32910635809866684 f_score on training set: 0.6352327091712968, loss on test set: 0.37678296381623827 f_score on test set: 0.48785315241205546\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.33093438915563317 f_score on training set: 0.6234124176814706, loss on test set: 0.3771447589284615 f_score on test set: 0.48641361694625196\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.3328205581867517 f_score on training set: 0.6172279513625327, loss on test set: 0.37705337939435096 f_score on test set: 0.48969553690742784\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.33465039843573513 f_score on training set: 0.6171980152299509, loss on test set: 0.3777964376032688 f_score on test set: 0.4862309181751911\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.33648613062084526 f_score on training set: 0.6176378530964048, loss on test set: 0.3778158937797527 f_score on test set: 0.49034263701170694\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.3383305395959299 f_score on training set: 0.5880715970000739, loss on test set: 0.3785972704630736 f_score on test set: 0.48461136323220655\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.340120933907114 f_score on training set: 0.574459317558709, loss on test set: 0.379438248884162 f_score on test set: 0.4855793618088492\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.3419177275922899 f_score on training set: 0.561620017054752, loss on test set: 0.379746285091418 f_score on test set: 0.48153429152913757\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.3437076537053416 f_score on training set: 0.554232295864336, loss on test set: 0.38067304736252805 f_score on test set: 0.48211316963762385\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.3454476669686861 f_score on training set: 0.5541822708424359, loss on test set: 0.381070622435967 f_score on test set: 0.48223801565700886\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.34713489576518497 f_score on training set: 0.5309712066107319, loss on test set: 0.3816302326846102 f_score on test set: 0.4843830157793656\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.3488397082126719 f_score on training set: 0.5390524855950036, loss on test set: 0.3823134126097137 f_score on test set: 0.4873032864439279\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.3505587392816411 f_score on training set: 0.5263574294542313, loss on test set: 0.38335515196694625 f_score on test set: 0.4785326534990718\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.3522304844408235 f_score on training set: 0.5263574294542313, loss on test set: 0.3840345266742229 f_score on test set: 0.4752824580057827\n",
      "Epoch number 10/70\n",
      "Loss on training set: 1.5591839191949917 f_score on training set: 0.4401414269518265, loss on test set: 2.2649401180474333 f_score on test set: 0.3654636701300914\n",
      "Epoch number 20/70\n",
      "Loss on training set: 1.1448803197171815 f_score on training set: 0.5116362217641303, loss on test set: 1.5279228668502993 f_score on test set: 0.42579482687021486\n",
      "Epoch number 30/70\n",
      "Loss on training set: 0.47891865397196426 f_score on training set: 0.5387917317124478, loss on test set: 0.8510844819755399 f_score on test set: 0.39216170505753467\n",
      "Epoch number 40/70\n",
      "Loss on training set: 0.3798946325540254 f_score on training set: 0.5747462179160292, loss on test set: 0.567421885435513 f_score on test set: 0.43772945358340715\n",
      "Epoch number 50/70\n",
      "Loss on training set: 0.3595061507713039 f_score on training set: 0.5453680874615233, loss on test set: 0.5497244815741155 f_score on test set: 0.40250282157260225\n",
      "Epoch number 60/70\n",
      "Loss on training set: 0.3735640157381076 f_score on training set: 0.5423456138177725, loss on test set: 0.4745576070466181 f_score on test set: 0.41832650166477675\n",
      "Epoch number 70/70\n",
      "Loss on training set: 0.3390525253932385 f_score on training set: 0.5794083037217725, loss on test set: 0.40864030709576815 f_score on test set: 0.4562791899704594\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.33487461940014296 f_score on training set: 0.5854410744603895, loss on test set: 0.40386086535672483 f_score on test set: 0.4417639657512149\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.3364024991788784 f_score on training set: 0.5854410744603895, loss on test set: 0.4034388305619068 f_score on test set: 0.4345711597933465\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.3379362379111127 f_score on training set: 0.5797163120567376, loss on test set: 0.4027952331456061 f_score on test set: 0.43257112719866986\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.33955551473895756 f_score on training set: 0.585855780485942, loss on test set: 0.40161331799393213 f_score on test set: 0.43464074340267184\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.3412398961921774 f_score on training set: 0.5800890285415029, loss on test set: 0.40123979452176745 f_score on test set: 0.42417109025669125\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.34282343592017606 f_score on training set: 0.5750173820339297, loss on test set: 0.4007762683492787 f_score on test set: 0.42590016844192846\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.34448380079939406 f_score on training set: 0.5698514272611646, loss on test set: 0.40037644292847413 f_score on test set: 0.4227535524146898\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.34617770930780717 f_score on training set: 0.5640685768075503, loss on test set: 0.3999214502906 f_score on test set: 0.4246596510621886\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.34783013885798386 f_score on training set: 0.557963458110517, loss on test set: 0.39968578188571985 f_score on test set: 0.42194214131426644\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.3494814280210577 f_score on training set: 0.5466231690393378, loss on test set: 0.39937892735878183 f_score on test set: 0.42111999323930527\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.3511567032188749 f_score on training set: 0.5289406838391558, loss on test set: 0.39915624283096157 f_score on test set: 0.42193931085389796\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.35281806785985736 f_score on training set: 0.5242556557506723, loss on test set: 0.399257970021819 f_score on test set: 0.4218135963403481\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.3544106355919346 f_score on training set: 0.5346876481112031, loss on test set: 0.39906801482832704 f_score on test set: 0.41955131532238304\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.35600603344686194 f_score on training set: 0.5281663456544413, loss on test set: 0.399293153793183 f_score on test set: 0.41697058955342087\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.3575813981332137 f_score on training set: 0.5346876481112031, loss on test set: 0.3992797469031962 f_score on test set: 0.41105625743561136\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.3591531911843298 f_score on training set: 0.5279597629014648, loss on test set: 0.3994007111582071 f_score on test set: 0.40578407538249256\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.36069656197734207 f_score on training set: 0.5175114986614918, loss on test set: 0.39950027454824183 f_score on test set: 0.4033873665005282\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.3621990316795536 f_score on training set: 0.5175114986614918, loss on test set: 0.399637825896133 f_score on test set: 0.3982864765939066\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.3636193401544431 f_score on training set: 0.5175114986614918, loss on test set: 0.39991681137846147 f_score on test set: 0.39446684340462873\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.3650173976254481 f_score on training set: 0.5031202602214997, loss on test set: 0.4000613263118354 f_score on test set: 0.3918772039106233\n",
      "Epoch number 10/70\n",
      "Loss on training set: 1.3871528717751267 f_score on training set: 0.36079105942220996, loss on test set: 1.822604084487759 f_score on test set: 0.3237575043673103\n",
      "Epoch number 20/70\n",
      "Loss on training set: 0.7361123012697742 f_score on training set: 0.5048865626281113, loss on test set: 1.1420159669801058 f_score on test set: 0.35042448709522733\n",
      "Epoch number 30/70\n",
      "Loss on training set: 0.5609139303983 f_score on training set: 0.534722018647952, loss on test set: 0.8905293554186181 f_score on test set: 0.36996494575052036\n",
      "Epoch number 40/70\n",
      "Loss on training set: 0.44884655090506453 f_score on training set: 0.45446293888927924, loss on test set: 0.6755640137681533 f_score on test set: 0.35190745967413817\n",
      "Epoch number 50/70\n",
      "Loss on training set: 0.3466930362115315 f_score on training set: 0.45310134310134315, loss on test set: 0.4897773607675053 f_score on test set: 0.38258496590380553\n",
      "Epoch number 60/70\n",
      "Loss on training set: 0.34124678373579687 f_score on training set: 0.5525667137414125, loss on test set: 0.44126909943617676 f_score on test set: 0.3804320375418956\n",
      "Epoch number 70/70\n",
      "Loss on training set: 0.3496278189287563 f_score on training set: 0.5633876009348672, loss on test set: 0.40306681924743504 f_score on test set: 0.4075053459088399\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.3482840987107989 f_score on training set: 0.5352722418824114, loss on test set: 0.407014183001773 f_score on test set: 0.38737481900609927\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.34936493501918875 f_score on training set: 0.5203171078515668, loss on test set: 0.405839688014713 f_score on test set: 0.3851975311027683\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.350521056523892 f_score on training set: 0.5249995090272468, loss on test set: 0.404937557437156 f_score on test set: 0.3839393085355745\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.3516937704352192 f_score on training set: 0.5305086569454385, loss on test set: 0.4036093825355142 f_score on test set: 0.38348502490098013\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.3529117827507042 f_score on training set: 0.5295824366481491, loss on test set: 0.4032726420085341 f_score on test set: 0.3856891017182853\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.35405139671767166 f_score on training set: 0.5306420569100622, loss on test set: 0.40230223261011977 f_score on test set: 0.38373049996362807\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.35525819845366174 f_score on training set: 0.5306420569100622, loss on test set: 0.40156103139933524 f_score on test set: 0.38784474501524574\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.35651068473508046 f_score on training set: 0.5296207934415413, loss on test set: 0.40132447074366184 f_score on test set: 0.3898034056696814\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.3577553102738856 f_score on training set: 0.5296722084284081, loss on test set: 0.400487330252279 f_score on test set: 0.39070016195841617\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.3589866015702203 f_score on training set: 0.5222767172998884, loss on test set: 0.40003431515119137 f_score on test set: 0.3920597406808075\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.36015226401804434 f_score on training set: 0.5183796756041046, loss on test set: 0.39943074160933983 f_score on test set: 0.3911421681870029\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.3612800344526887 f_score on training set: 0.5053891190638926, loss on test set: 0.39935202259686575 f_score on test set: 0.39192179639298236\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.36244292942379136 f_score on training set: 0.5053891190638926, loss on test set: 0.39904823538702294 f_score on test set: 0.38895285111119876\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.3635587096658992 f_score on training set: 0.49798078442664234, loss on test set: 0.3986154415217397 f_score on test set: 0.38757906111884977\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.3646522973701265 f_score on training set: 0.4823102606097386, loss on test set: 0.39829771725923074 f_score on test set: 0.3855502553994702\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.36573114087146164 f_score on training set: 0.4767565704018476, loss on test set: 0.39832653722338063 f_score on test set: 0.38409285738803745\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.3668807963635561 f_score on training set: 0.4767565704018476, loss on test set: 0.3983106012872051 f_score on test set: 0.3797493774542596\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.3680476892293959 f_score on training set: 0.47724388861680483, loss on test set: 0.3981236716769215 f_score on test set: 0.38015149232139095\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.3692269608193428 f_score on training set: 0.4684738021736372, loss on test set: 0.3980274830609623 f_score on test set: 0.37918808601973364\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.37039240190954564 f_score on training set: 0.4725160344511636, loss on test set: 0.3980401982811572 f_score on test set: 0.37751988906029577\n",
      "Epoch number 10/70\n",
      "Loss on training set: 1.921495937179737 f_score on training set: 0.4146179257891896, loss on test set: 2.0730576059443813 f_score on test set: 0.38098968614106987\n",
      "Epoch number 20/70\n",
      "Loss on training set: 0.9084908375803336 f_score on training set: 0.5332571764450321, loss on test set: 1.3166767803660042 f_score on test set: 0.39206278708693665\n",
      "Epoch number 30/70\n",
      "Loss on training set: 1.0324568063920465 f_score on training set: 0.4878426836321573, loss on test set: 0.9789681582234326 f_score on test set: 0.3800951690731239\n",
      "Epoch number 40/70\n",
      "Loss on training set: 0.48419319383514103 f_score on training set: 0.39349628312582685, loss on test set: 0.5529482349889797 f_score on test set: 0.40607392447492136\n",
      "Epoch number 50/70\n",
      "Loss on training set: 0.31074166824102395 f_score on training set: 0.571994430212252, loss on test set: 0.44591717716192697 f_score on test set: 0.42606608046215055\n",
      "Epoch number 60/70\n",
      "Loss on training set: 0.30765404085510406 f_score on training set: 0.5588205610471829, loss on test set: 0.38079792304156224 f_score on test set: 0.47361306893646593\n",
      "Epoch number 70/70\n",
      "Loss on training set: 0.32245272695133087 f_score on training set: 0.5600529261398827, loss on test set: 0.37849713094307336 f_score on test set: 0.4699329892639315\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.321082962329118 f_score on training set: 0.554782136468183, loss on test set: 0.37453542381091126 f_score on test set: 0.48815455179572553\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.3227543639843313 f_score on training set: 0.5493280894115155, loss on test set: 0.37438286200157495 f_score on test set: 0.48664575125383236\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.324500664617813 f_score on training set: 0.5450322288526007, loss on test set: 0.3741247761449172 f_score on test set: 0.48796498869411414\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.3263484858034082 f_score on training set: 0.539769605753584, loss on test set: 0.3739962979162679 f_score on test set: 0.48694951918256485\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.32809035050817387 f_score on training set: 0.5309129126568558, loss on test set: 0.3738963947300847 f_score on test set: 0.484264633190672\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.329888723089411 f_score on training set: 0.5220141362386133, loss on test set: 0.3744769925491201 f_score on test set: 0.4814261555920999\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.3316949366675124 f_score on training set: 0.5265452795002885, loss on test set: 0.3743967039496065 f_score on test set: 0.4796327750050916\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.3334592131001997 f_score on training set: 0.5216265984527015, loss on test set: 0.37471676520371494 f_score on test set: 0.47909154204740456\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.3352478540428448 f_score on training set: 0.5216265984527015, loss on test set: 0.37495856350948686 f_score on test set: 0.4784580253975934\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.33699385073433613 f_score on training set: 0.5261106547014637, loss on test set: 0.37530695844605205 f_score on test set: 0.4791154673315509\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.3387851537104799 f_score on training set: 0.5261106547014637, loss on test set: 0.37550570912104053 f_score on test set: 0.47907991883913026\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.3405861744264326 f_score on training set: 0.5213326120486288, loss on test set: 0.375874450733628 f_score on test set: 0.4825293793921952\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.34242552448502234 f_score on training set: 0.5089810887382922, loss on test set: 0.3759470155032809 f_score on test set: 0.4868304005397225\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.3442391707101775 f_score on training set: 0.5043111020458336, loss on test set: 0.3761597492002654 f_score on test set: 0.484732300997716\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.34606318594356683 f_score on training set: 0.5043111020458336, loss on test set: 0.37686063970825506 f_score on test set: 0.4855214572882181\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.34784267972243965 f_score on training set: 0.5174503828282259, loss on test set: 0.377242808783375 f_score on test set: 0.48620799852170354\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.3495696122784268 f_score on training set: 0.5126330399436381, loss on test set: 0.3778512642631317 f_score on test set: 0.4850430018315171\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.3513026485413444 f_score on training set: 0.5044028520499109, loss on test set: 0.37851578759774857 f_score on test set: 0.48569702413270793\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.3530168186291628 f_score on training set: 0.503857127335241, loss on test set: 0.3789714833778956 f_score on test set: 0.4827568895758838\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.354650528220376 f_score on training set: 0.4996466010194516, loss on test set: 0.37948310511658573 f_score on test set: 0.4875216863426404\n"
     ]
    }
   ],
   "source": [
    "r5_l2_build = {'input_shape': r5_x_train.shape, 'neurons_num': [40, 40, 5], 'activations': [ReLU(), ReLU(), Softmax()]}\n",
    "r5_l2_fit = [{'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 70, 'learning_rate': 0.0001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 3},\n",
    "             {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 200, 'learning_rate': 0.00001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 3}]\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 400, 'learning_rate': 0.00001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 1}]\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 # {'x_train': r5_x_train, 'y_train': r5_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': r5_x_test, 'y_test': r5_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test, r5_l2_nns = cv_network(build_args=r5_l2_build, fit_args=r5_l2_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test))\n",
    "# reg 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test)) # reg 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(results_train), np.mean(results_test)) # reg 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rings3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "xor3_x_train, xor3_y_train, xor3_x_test, xor3_y_test = read_classification_data('xor3-balance')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(1050, 2)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor3_x_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/200\n",
      "Loss on training set: 0.4163152452142537 f_score on training set: 0.9320206020713635, loss on test set: 3.810485247519712 f_score on test set: 0.6195315323321189\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.44067042076256707 f_score on training set: 0.9487508785762785, loss on test set: 5.44519749267293 f_score on test set: 0.550974769544831\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.3421899378400168 f_score on training set: 0.9352515164415869, loss on test set: 4.168717915822777 f_score on test set: 0.5452163191019354\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.2630368688938292 f_score on training set: 0.9359160929619332, loss on test set: 2.674186801561012 f_score on test set: 0.6573405211876977\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.25582579426987984 f_score on training set: 0.9478652574755082, loss on test set: 3.4501145633232784 f_score on test set: 0.6360891148113729\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.2939445190209044 f_score on training set: 0.9541159632247703, loss on test set: 4.224621812037318 f_score on test set: 0.6151006019782269\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.34052283467059735 f_score on training set: 0.9375437247047438, loss on test set: 2.4396171789476835 f_score on test set: 0.7217341936414101\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.2519315695261631 f_score on training set: 0.9548578176311996, loss on test set: 3.723400826823111 f_score on test set: 0.5958830917874396\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.2146897937425962 f_score on training set: 0.9490495486333839, loss on test set: 2.804085674941181 f_score on test set: 0.7028813319764814\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.3135725720432891 f_score on training set: 0.9252820145770538, loss on test set: 1.341901967850032 f_score on test set: 0.8162941625010592\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.21180464927184253 f_score on training set: 0.9675770864462169, loss on test set: 3.4610050328387394 f_score on test set: 0.6560685359589838\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.20448005652570017 f_score on training set: 0.9679064992497828, loss on test set: 3.245860327553327 f_score on test set: 0.6645745446293663\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.15446792555375646 f_score on training set: 0.965470316258686, loss on test set: 2.632194507280403 f_score on test set: 0.6575077894611974\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.16791932006540264 f_score on training set: 0.9554730123417554, loss on test set: 2.326528402077762 f_score on test set: 0.7396721841458682\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.15276825192536764 f_score on training set: 0.97074969423122, loss on test set: 2.5465047372068197 f_score on test set: 0.6732978192597812\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.1456991558308837 f_score on training set: 0.9546660415231086, loss on test set: 1.6949943431303653 f_score on test set: 0.7603394250748237\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.1608737336103745 f_score on training set: 0.9519936088314328, loss on test set: 1.5299163721331461 f_score on test set: 0.7797428372388222\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.26985272812168565 f_score on training set: 0.9529320579303933, loss on test set: 3.0937013171015635 f_score on test set: 0.6964186696557791\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.2613823828036471 f_score on training set: 0.9476915863728178, loss on test set: 2.7490608394648017 f_score on test set: 0.7217341936414101\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.266160113837491 f_score on training set: 0.9623081174267732, loss on test set: 3.9029400271301413 f_score on test set: 0.6475909772688277\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.07905668531199551 f_score on training set: 0.979055787418157, loss on test set: 1.621409405004257 f_score on test set: 0.726722873164106\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.07120151950773773 f_score on training set: 0.976689478186484, loss on test set: 1.4103068500693945 f_score on test set: 0.7357305515335\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.06778383048303757 f_score on training set: 0.9799415620311142, loss on test set: 1.4406610105708682 f_score on test set: 0.7269615027862156\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.07034545394024921 f_score on training set: 0.9799415620311142, loss on test set: 1.3848152522745811 f_score on test set: 0.7362974529346622\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.06646752112169925 f_score on training set: 0.981739433528698, loss on test set: 1.4303329197147605 f_score on test set: 0.7197383305447509\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.06655547130403175 f_score on training set: 0.9810520645203622, loss on test set: 1.5732772859133337 f_score on test set: 0.6998721846570735\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.06485809656639803 f_score on training set: 0.982652169763532, loss on test set: 1.412590678855759 f_score on test set: 0.7164685909081924\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.0699735512652605 f_score on training set: 0.9775461096631854, loss on test set: 1.2122212490424007 f_score on test set: 0.7414679950625992\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.06478589148023467 f_score on training set: 0.9835745572795527, loss on test set: 1.3731446898643052 f_score on test set: 0.7255698005698005\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.0660098572389274 f_score on training set: 0.98221969256728, loss on test set: 1.574817397326425 f_score on test set: 0.675896922869725\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.6217634264825564 f_score on training set: 0.9313224267211998, loss on test set: 3.6038142091588314 f_score on test set: 0.6778145732877343\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.5946520240037599 f_score on training set: 0.922547928262214, loss on test set: 2.624206950261515 f_score on test set: 0.7330611709110002\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.40605595224120017 f_score on training set: 0.9414933822228656, loss on test set: 4.084220462324477 f_score on test set: 0.6122380952380952\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.7394058522405516 f_score on training set: 0.9239873138372663, loss on test set: 3.3734638131670693 f_score on test set: 0.6817587514388245\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.36460812123721065 f_score on training set: 0.9463080778870252, loss on test set: 3.097794485813522 f_score on test set: 0.7077376727281597\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.4180977691761802 f_score on training set: 0.9452607424405048, loss on test set: 3.256541551210436 f_score on test set: 0.6958151611665319\n",
      "Epoch number 70/200\n",
      "Loss on training set: 1.1161781649125546 f_score on training set: 0.875415369779473, loss on test set: 1.6017915459048435 f_score on test set: 0.8022698310151758\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.3222173668446793 f_score on training set: 0.9548300325187695, loss on test set: 4.105300181948565 f_score on test set: 0.5958099381538573\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.2797287650996727 f_score on training set: 0.959218175641029, loss on test set: 3.2947631784647773 f_score on test set: 0.6893566539869682\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.37634892697503236 f_score on training set: 0.9529344803251536, loss on test set: 4.13474667945305 f_score on test set: 0.6277647357875534\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.5697004992972005 f_score on training set: 0.9177018633540374, loss on test set: 2.2704598792070976 f_score on test set: 0.7159971289079692\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.2946444050619333 f_score on training set: 0.9688535596251415, loss on test set: 4.335095520512802 f_score on test set: 0.5948753830439224\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.4463467824403623 f_score on training set: 0.9458712280774445, loss on test set: 2.250512482315013 f_score on test set: 0.7724249779973311\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.328761402661072 f_score on training set: 0.959433134245285, loss on test set: 4.412568577795819 f_score on test set: 0.5998020412568221\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.4434916899194515 f_score on training set: 0.9338836184699555, loss on test set: 1.7339620083463876 f_score on test set: 0.801862611519806\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.2454613531781202 f_score on training set: 0.9553857415495522, loss on test set: 2.134210105209581 f_score on test set: 0.769183400711335\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.30263671667287656 f_score on training set: 0.9469915121220902, loss on test set: 2.230349255883795 f_score on test set: 0.768283974519857\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.37783258729461794 f_score on training set: 0.9465481558275239, loss on test set: 4.343526457917361 f_score on test set: 0.5809976152623212\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.6370283389915685 f_score on training set: 0.9112781954887218, loss on test set: 1.567303098191379 f_score on test set: 0.7931986477810965\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.26315821907762493 f_score on training set: 0.963497191473468, loss on test set: 3.571272131665067 f_score on test set: 0.6606734284703386\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.15647696528024668 f_score on training set: 0.9621610661793357, loss on test set: 2.208373523174819 f_score on test set: 0.7377644680203487\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.14253780998114757 f_score on training set: 0.9700417464132717, loss on test set: 2.2098344908856267 f_score on test set: 0.7362210338680927\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.1331741441398039 f_score on training set: 0.9682093123976891, loss on test set: 1.9217603426822734 f_score on test set: 0.7377749197920125\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.13517574324310605 f_score on training set: 0.9759298744373374, loss on test set: 2.409431690694527 f_score on test set: 0.7205602115454333\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.11684240580730335 f_score on training set: 0.9728043912175648, loss on test set: 2.0345397577024187 f_score on test set: 0.7338576604491509\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.12491909902429527 f_score on training set: 0.975361835919329, loss on test set: 2.310261823856392 f_score on test set: 0.7168472155810419\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.12282502132696885 f_score on training set: 0.9750566893424036, loss on test set: 2.2688877597151484 f_score on test set: 0.7048691202714116\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.11844097187776946 f_score on training set: 0.9620221327967806, loss on test set: 1.578419639614919 f_score on test set: 0.753226197321942\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.10584635999429767 f_score on training set: 0.9768014816316574, loss on test set: 1.9036768154251376 f_score on test set: 0.728099446290144\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.10261282260166696 f_score on training set: 0.9695238095238096, loss on test set: 1.6857462916557018 f_score on test set: 0.7465872471182792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/200\n",
      "Loss on training set: nan f_score on training set: 0.9473675860482457, loss on test set: nan f_score on test set: 0.5880112992521125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 20/200\n",
      "Loss on training set: nan f_score on training set: 0.9397119622007177, loss on test set: nan f_score on test set: 0.5811782247960414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 30/200\n",
      "Loss on training set: nan f_score on training set: 0.9320928460292763, loss on test set: nan f_score on test set: 0.4615156325156326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 40/200\n",
      "Loss on training set: nan f_score on training set: 0.917581912897427, loss on test set: nan f_score on test set: 0.7649128994866701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 50/200\n",
      "Loss on training set: nan f_score on training set: 0.9561201941077577, loss on test set: nan f_score on test set: 0.5800356534850917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 60/200\n",
      "Loss on training set: nan f_score on training set: 0.9483741483741485, loss on test set: nan f_score on test set: 0.5213607768582565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 70/200\n",
      "Loss on training set: nan f_score on training set: 0.9513051560765278, loss on test set: nan f_score on test set: 0.6091712650725526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 80/200\n",
      "Loss on training set: nan f_score on training set: 0.9615983026874114, loss on test set: nan f_score on test set: 0.5948995945573032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 90/200\n",
      "Loss on training set: nan f_score on training set: 0.9597180807806994, loss on test set: nan f_score on test set: 0.5968594092072401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 100/200\n",
      "Loss on training set: nan f_score on training set: 0.9584278155706727, loss on test set: nan f_score on test set: 0.5987779106858053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 110/200\n",
      "Loss on training set: nan f_score on training set: 0.9468482597654819, loss on test set: nan f_score on test set: 0.6316469771187505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 120/200\n",
      "Loss on training set: nan f_score on training set: 0.9321858124693176, loss on test set: nan f_score on test set: 0.7679649479117286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 130/200\n",
      "Loss on training set: nan f_score on training set: 0.9521213657876942, loss on test set: nan f_score on test set: 0.7354406403387324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 140/200\n",
      "Loss on training set: nan f_score on training set: 0.9477771859289726, loss on test set: nan f_score on test set: 0.4539736414515256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 150/200\n",
      "Loss on training set: nan f_score on training set: 0.923821036794271, loss on test set: nan f_score on test set: 0.7377366666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 160/200\n",
      "Loss on training set: nan f_score on training set: 0.9586206896551726, loss on test set: nan f_score on test set: 0.617250858449514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 170/200\n",
      "Loss on training set: nan f_score on training set: 0.9723691945914168, loss on test set: nan f_score on test set: 0.6617769420011583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 180/200\n",
      "Loss on training set: nan f_score on training set: 0.976689478186484, loss on test set: nan f_score on test set: 0.6887710906056028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 190/200\n",
      "Loss on training set: nan f_score on training set: 0.927392253075111, loss on test set: nan f_score on test set: 0.7758976084890721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 200/200\n",
      "Loss on training set: nan f_score on training set: 0.9420953388851793, loss on test set: nan f_score on test set: 0.7837440508326757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/100\n",
      "Loss on training set: nan f_score on training set: 0.9742108171867395, loss on test set: nan f_score on test set: 0.7333038264856288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 20/100\n",
      "Loss on training set: nan f_score on training set: 0.9722483635527113, loss on test set: nan f_score on test set: 0.7473673638189767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 30/100\n",
      "Loss on training set: nan f_score on training set: 0.9801278916928424, loss on test set: nan f_score on test set: 0.7142159331170613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 40/100\n",
      "Loss on training set: nan f_score on training set: 0.9785703273549515, loss on test set: nan f_score on test set: 0.7191053805886896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 50/100\n",
      "Loss on training set: nan f_score on training set: 0.9785703273549515, loss on test set: nan f_score on test set: 0.7130909846579386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 60/100\n",
      "Loss on training set: nan f_score on training set: 0.9774150162593848, loss on test set: nan f_score on test set: 0.7037798725077987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 70/100\n",
      "Loss on training set: nan f_score on training set: 0.9779357182342258, loss on test set: nan f_score on test set: 0.7216924126448211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 80/100\n",
      "Loss on training set: nan f_score on training set: 0.9785703273549515, loss on test set: nan f_score on test set: 0.7142159331170613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 90/100\n",
      "Loss on training set: nan f_score on training set: 0.98221969256728, loss on test set: nan f_score on test set: 0.6970802980027208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 100/100\n",
      "Loss on training set: nan f_score on training set: 0.9779900814383572, loss on test set: nan f_score on test set: 0.7408134171907755\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.2844412581056593 f_score on training set: 0.9551544642020592, loss on test set: 2.8209645720172216 f_score on test set: 0.7115327102803738\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.2411719062290719 f_score on training set: 0.9427686889343332, loss on test set: 1.5548265534822678 f_score on test set: 0.8050513008064155\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.4124701579140883 f_score on training set: 0.9354195719677951, loss on test set: 2.71376576174977 f_score on test set: 0.7304149207971383\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.1503242952757255 f_score on training set: 0.9761946004800672, loss on test set: 2.2236107797870375 f_score on test set: 0.734008773217371\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.1615302470935897 f_score on training set: 0.9703344575908293, loss on test set: 2.070543654314439 f_score on test set: 0.7535299267707523\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.19382546122385033 f_score on training set: 0.9667422524565382, loss on test set: 2.579845274245619 f_score on test set: 0.6709031915461484\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.23490024255806577 f_score on training set: 0.9723691945914168, loss on test set: 3.4949701743482326 f_score on test set: 0.6096305183915831\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.4282386258260391 f_score on training set: 0.9452723612015648, loss on test set: 4.291269451342843 f_score on test set: 0.5829234364132261\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.34942032314140875 f_score on training set: 0.9337225274725275, loss on test set: 4.3659033624254535 f_score on test set: 0.43141080872824067\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.16437345293880753 f_score on training set: 0.9722483635527113, loss on test set: 2.235368803010749 f_score on test set: 0.7092106393449958\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.5498292138478897 f_score on training set: 0.8966647387700019, loss on test set: 1.467534182003613 f_score on test set: 0.7757331770793309\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.17119129835240124 f_score on training set: 0.9618472523162387, loss on test set: 1.5134536610481009 f_score on test set: 0.7856515017074024\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.14784414590527514 f_score on training set: 0.9764475570249737, loss on test set: 2.0315640698251958 f_score on test set: 0.7434525004791537\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.21663366850059373 f_score on training set: 0.9683322375630067, loss on test set: 3.064055034631329 f_score on test set: 0.6135083267416103\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.23871235687303935 f_score on training set: 0.9532296363490835, loss on test set: 3.0766697021013956 f_score on test set: 0.6075637958532696\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.12594483829096428 f_score on training set: 0.9798676244111278, loss on test set: 2.0835478898335866 f_score on test set: 0.7148276668465552\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.4810020209476622 f_score on training set: 0.917397851481671, loss on test set: 1.0028331721408796 f_score on test set: 0.834388037483062\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.13905584536198648 f_score on training set: 0.9713896906143398, loss on test set: 1.9923971100771636 f_score on test set: 0.7488820771885895\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.1309657036243973 f_score on training set: 0.9714285714285714, loss on test set: 1.708223113557878 f_score on test set: 0.7698841132868681\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.11575495496971606 f_score on training set: 0.976689478186484, loss on test set: 1.8062444167009721 f_score on test set: 0.7427105543710022\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.08062483825565188 f_score on training set: 0.981739433528698, loss on test set: 1.4016429711778946 f_score on test set: 0.7809018368065381\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.07651089090804275 f_score on training set: 0.9845069541464226, loss on test set: 1.4242746087169678 f_score on test set: 0.765472799056913\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.07894446623987564 f_score on training set: 0.9801278916928424, loss on test set: 1.4613687981112482 f_score on test set: 0.7469107741139127\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.07230892241205791 f_score on training set: 0.9776815298684085, loss on test set: 1.2728559893407538 f_score on test set: 0.7760850278769981\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.07254269987649807 f_score on training set: 0.98221969256728, loss on test set: 1.412675437567954 f_score on test set: 0.7469107741139127\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.07120814776633104 f_score on training set: 0.9812925170068028, loss on test set: 1.3588127119435671 f_score on test set: 0.7562521041409493\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.07390390593487538 f_score on training set: 0.975840118075219, loss on test set: 1.1751976385315637 f_score on test set: 0.7933048779654297\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.07117368025522577 f_score on training set: 0.9835745572795527, loss on test set: 1.4404905376749764 f_score on test set: 0.7358525558745257\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.07645002731409328 f_score on training set: 0.9725107721234496, loss on test set: 1.1456165218538337 f_score on test set: 0.7891895600872131\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.06801486008720066 f_score on training set: 0.9812925170068028, loss on test set: 1.3669141782646006 f_score on test set: 0.7421931186568365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n",
      "C:\\Users\\marys\\PycharmProjects\\NN\\activation_functions.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x) / (np.sum(np.exp(x), axis=1).reshape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/200\n",
      "Loss on training set: 0.4507386630756022 f_score on training set: 0.943206387943399, loss on test set: nan f_score on test set: 0.5559122212215923\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.26041418623086654 f_score on training set: 0.9597759594474082, loss on test set: 2.3973581161743938 f_score on test set: 0.7724061032863849\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.35551178493692764 f_score on training set: 0.9688197575660557, loss on test set: 4.889833747783084 f_score on test set: 0.6037381488578711\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.3227373649761177 f_score on training set: 0.9574250218981704, loss on test set: 2.9711317100003383 f_score on test set: 0.7286051164871303\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.29272626400259205 f_score on training set: 0.9623105841973417, loss on test set: 3.412006268935458 f_score on test set: 0.6820262120915246\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.33987439802175207 f_score on training set: 0.9623081174267732, loss on test set: 3.5769490952509977 f_score on test set: 0.6310871775560158\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.3164972487959617 f_score on training set: 0.9461641439380002, loss on test set: 2.045337385122731 f_score on test set: 0.7871840864352404\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.44584040368589434 f_score on training set: 0.9622429577206286, loss on test set: 4.884669479907263 f_score on test set: 0.6176941646447563\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.29908628654915587 f_score on training set: 0.9722654408297973, loss on test set: 4.226233273476814 f_score on test set: 0.6182641844277124\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.2944239840649746 f_score on training set: 0.972227033893412, loss on test set: 3.816921918994983 f_score on test set: 0.6887046380396623\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.20108237837284199 f_score on training set: 0.97115129768191, loss on test set: 2.681204219707039 f_score on test set: 0.7462131376659679\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.46024433320114405 f_score on training set: 0.926898974701573, loss on test set: 2.0410426888255073 f_score on test set: 0.7844852942373863\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.30202448218818123 f_score on training set: 0.9595491365402871, loss on test set: 4.195075675301199 f_score on test set: 0.5567622027534419\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.23657602096129846 f_score on training set: 0.967783548647179, loss on test set: 2.839720474118185 f_score on test set: 0.7554301075268817\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.299840277495004 f_score on training set: 0.9666330527972058, loss on test set: 2.430168642818531 f_score on test set: 0.7565052826640114\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.18778568964340758 f_score on training set: 0.975593597459984, loss on test set: 2.4170253994188067 f_score on test set: 0.7776768502145488\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.29897887733291945 f_score on training set: 0.9503091740535885, loss on test set: 1.7440345463184246 f_score on test set: 0.8066241716275414\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.5821977961779783 f_score on training set: 0.931473028450033, loss on test set: 6.062945659864377 f_score on test set: 0.41263806777217016\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.6765301814739371 f_score on training set: 0.9236961676602031, loss on test set: 1.5500907296286632 f_score on test set: 0.8264179300816236\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.28053599903223536 f_score on training set: 0.9653916805252117, loss on test set: 3.9481529301847695 f_score on test set: 0.6296251075639542\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.12460398612338593 f_score on training set: 0.9808360065652821, loss on test set: 1.8181893664989155 f_score on test set: 0.7958207847295864\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.12070399424750779 f_score on training set: 0.98221969256728, loss on test set: 1.9334289877760387 f_score on test set: 0.7658288770053477\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.10439228951114285 f_score on training set: 0.975840118075219, loss on test set: 1.4136831241268626 f_score on test set: 0.815389794113932\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.09707025374071605 f_score on training set: 0.981739433528698, loss on test set: 1.5345832126477927 f_score on test set: 0.7974139891516464\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.10018164858375961 f_score on training set: 0.975593597459984, loss on test set: 1.4738497769960022 f_score on test set: 0.79357762856207\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.09524823257421805 f_score on training set: 0.9792821897552115, loss on test set: 1.4490977277763633 f_score on test set: 0.7964896951115654\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.09401729504730445 f_score on training set: 0.9808360065652821, loss on test set: 1.6085642011029873 f_score on test set: 0.7812124093828384\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.09369313355859912 f_score on training set: 0.9808360065652821, loss on test set: 1.6112364433841393 f_score on test set: 0.7661804324707551\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.09370326792442694 f_score on training set: 0.9810504743307134, loss on test set: 1.5125416489435195 f_score on test set: 0.7847770143788698\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.09127718061923802 f_score on training set: 0.9808360065652821, loss on test set: 1.502373012387489 f_score on test set: 0.7915611623911714\n"
     ]
    }
   ],
   "source": [
    "xor3_no_reg_build = {'input_shape': xor3_x_train.shape, 'neurons_num': [40, 40, 2], 'activations': [ReLU(), ReLU(), Softmax()]}\n",
    "xor3_no_reg_fit = [{'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 200, 'learning_rate': 0.0001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 100, 'learning_rate': 0.00001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "                 # {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 # {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test, xor3_no_reg_nns = cv_network(build_args=xor3_no_reg_build, fit_args=xor3_no_reg_fit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [70]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(i)\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;241m~\u001B[39mnp\u001B[38;5;241m.\u001B[39misnan(np\u001B[38;5;241m.\u001B[39marray(xor3_no_reg_nns[i]\u001B[38;5;241m.\u001B[39mhistory[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss_train\u001B[39m\u001B[38;5;124m'\u001B[39m]))\u001B[38;5;241m.\u001B[39many()\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(3, 5):\n",
    "    print(i)\n",
    "\n",
    "    assert ~np.isnan(np.array(xor3_no_reg_nns[i].history['loss_train'])).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "avg_loss_xor3_train, avg_loss_xor3_test = average_loss([-1, xor3_no_reg_nns[0], xor3_no_reg_nns[1],  xor3_no_reg_nns[3]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x269b5075730>]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 864x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[12, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(300), avg_loss_xor3_train)\n",
    "plt.ylim(0, 6)\n",
    "plt.plot(np.arange(300), avg_loss_xor3_test, c='red')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(300), avg_loss_xor3_train_l2)\n",
    "plt.ylim(0, 6)\n",
    "plt.plot(np.arange(300), avg_loss_xor3_test_l2, c='red')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.9078927732162341,\n 0.6370099815493944,\n 0.614944643304527,\n 0.5684329537279968,\n 0.5783559653027235,\n 0.5350565325900972,\n 0.5064034724589316,\n 0.5078811132734525,\n 0.5114051389776941,\n 0.4610929348390105,\n 0.4569512259262468,\n 0.44460185474509284,\n 0.44784898131749046,\n 0.42753850772481106,\n 0.3912819029636708,\n 0.42950594571550527,\n 0.40334855557283317,\n 0.372789679538394,\n 0.3356606012774197,\n 0.33370328166067903,\n 0.36147574236795726,\n 0.31822599572539534,\n 0.34301455087869015,\n 0.3175596749315593,\n 0.2967637103035491,\n 0.30406599101619586,\n 0.30763158163822935,\n 0.32006470183704716,\n 0.37506454334746037,\n 0.32126637461493557,\n 0.3807324095630874,\n 0.3452340897123939,\n 0.2743392332930884,\n 0.3441073212818905,\n 0.3148537584688785,\n 0.2524168793881786,\n 0.2862655280801794,\n 0.273377569634697,\n 0.3418325049632782,\n 0.2443017157687338,\n 0.2750378757171811,\n 0.2538339350719671,\n 0.24632807957256275,\n 0.3023155089472177,\n 0.30551929322508337,\n 0.2674634158561517,\n 0.24976684467341248,\n 0.25793820919210825,\n 0.2529563974535449,\n 0.23169241957234188,\n 0.2360217347420141,\n 0.24102898575989837,\n 0.2376433443682902,\n 0.2448843562029888,\n 0.29724688712785724,\n 0.2778378573504596,\n 0.3053999730790742,\n 0.3279713415837452,\n 0.22937107224652545,\n 0.272077709306788,\n 0.24324622499484966,\n 0.26042753095807125,\n 0.2411117920067518,\n 0.275102525877954,\n 0.2687223201459567,\n 0.2727831171522219,\n 0.22390356762129288,\n 0.2288939285393758,\n 0.22908302257179147,\n 0.234262873253766,\n 0.21783165514375258,\n 0.24015905480302857,\n 0.21612938442609064,\n 0.2019742927436583,\n 0.228115254911945,\n 0.28748235187050547,\n 0.21394510421580695,\n 0.24110896016411226,\n 0.2529643731890071,\n 0.2432345666243079,\n 0.23373281193619622,\n 0.21823315418923675,\n 0.20334652463600375,\n 0.21215163456100483,\n 0.20574404682301342,\n 0.1979217789099393,\n 0.20631352506777786,\n 0.2065506732058046,\n 0.22492012309593543,\n 0.18475387893272185,\n 0.2928212304690472,\n 0.3852123221989309,\n 0.3205345110067829,\n 0.24191302345409743,\n 0.2132525735876323,\n 0.2023124212606622,\n 0.18794846628744613,\n 0.21966989637474585,\n 0.26305827118753616,\n 0.1903527423642653,\n 0.29060486053261164,\n 0.190163362332504,\n 0.220134753410368,\n 0.39385121145981794,\n 0.17813230050364734,\n 0.23128731552429155,\n 0.21811044602506868,\n 0.18405554151640655,\n 0.1866026836181711,\n 0.1708490348943969,\n 0.22550485037682805,\n 0.22342926695172527,\n 0.16077722097160047,\n 0.19441188048981958,\n 0.24710351872750005,\n 0.22063036121489055,\n 0.22343158969565924,\n 0.19405447885728314,\n 0.21065062173045535,\n 0.17939709207907914,\n 0.15612558142957986,\n 0.20278294024690743,\n 0.1868517849879558,\n 0.16682526536795428,\n 0.2116317926313554,\n 0.21264066006761753,\n 0.15244753602902572,\n 0.21281499940997833,\n 0.2063470527876993,\n 0.1666683972487475,\n 0.15140794179990902,\n 0.1442967531866541,\n 0.139683363891592,\n 0.16485565626178977,\n 0.22108035365631432,\n 0.2159633645422211,\n 0.16606911724537665,\n 0.16657342836809025,\n 0.14150086511241056,\n 0.17226513824719783,\n 0.17568375677708722,\n 0.1755264606781059,\n 0.14792862592930173,\n 0.15342792630404162,\n 0.17077777693937113,\n 0.17001244713917568,\n 0.159601413286929,\n 0.14789927240700254,\n 0.14960707692250172,\n 0.15482429995590236,\n 0.17920578304325707,\n 0.16735890966397443,\n 0.17310111803196593,\n 0.18616798902885762,\n 0.23089277995804594,\n 0.16465875402590147,\n 0.14062003796136893,\n 0.14739267777594528,\n 0.15355218484415845,\n 0.18993014545925413,\n 0.21492098306915872,\n 0.17793355974749328,\n 0.15298363820270774,\n 0.1545216383910102,\n 0.16450182592944684,\n 0.12533323401768345,\n 0.25249967058132555,\n 0.17107658245561436,\n 0.28614110008175797,\n 0.16978199659873922,\n 0.1362861612824554,\n 0.20251587595499287,\n 0.20023677014280447,\n 0.15272181673089774,\n 0.12123416647334775,\n 0.13886152349966768,\n 0.15902562700357079,\n 0.19845462253402982,\n 0.14468173646744914,\n 0.18995076107295814,\n 0.17514702901595486,\n 0.15823431891889256,\n 0.13968741349980177,\n 0.15027427968236634,\n 0.15176057451486452,\n 0.14666677061393257,\n 0.1255613827878932,\n 0.13965055192425585,\n 0.250460259012699,\n 0.17241764621692807,\n 0.14365667026539047,\n 0.1385024767629825,\n 0.13201339037742005,\n 0.14625006639863466,\n 0.1727422454765606,\n 0.20411741033822522,\n 0.1261899529968037,\n 0.1256653937919491,\n 0.1518697629842618,\n 0.16944412953139767,\n 0.10929639920697747,\n 0.10617300758926844,\n 0.10537556342527388,\n 0.10384556029859052,\n 0.101680353256614,\n 0.1053370181467811,\n 0.10265427423674485,\n 0.10232761719035466,\n 0.09984383921082632,\n 0.09884744755804402,\n 0.09858031179796058,\n 0.09806351047263208,\n 0.09848630406778894,\n 0.09665315009109099,\n 0.1024162811610193,\n 0.0965055215668047,\n 0.09624085142889055,\n 0.09851446950166047,\n 0.09686762252207984,\n 0.09889063858196158,\n 0.09696137356432814,\n 0.09535963112988437,\n 0.09616477869449584,\n 0.09723304525232493,\n 0.09598027819742021,\n 0.09494811831003351,\n 0.09622785626897372,\n 0.09617757026785284,\n 0.09613736496336887,\n 0.09557436336366767,\n 0.09596742218058621,\n 0.098101522766236,\n 0.0984332930290158,\n 0.10223297250520877,\n 0.09402255960065767,\n 0.09775872735970147,\n 0.0974319435098986,\n 0.09346095638327333,\n 0.09562305785093185,\n 0.09938369538606064,\n 0.09807133039794065,\n 0.09462490860850889,\n 0.09423653687055746,\n 0.09609918938066467,\n 0.0961950964191542,\n 0.09431001047335236,\n 0.09545542508370859,\n 0.09391248110652436,\n 0.09833288917161465,\n 0.09459393398319135,\n 0.10078206921564048,\n 0.1050639018604985,\n 0.09472680536935381,\n 0.09303756255773626,\n 0.09462415577537558,\n 0.09750789407091408,\n 0.09490760818492441,\n 0.09238713448329536,\n 0.09333279931458244,\n 0.0937600145463831,\n 0.09252402959225396,\n 0.09506328915562914,\n 0.0935241796474395,\n 0.10008156922161851,\n 0.0943099688174218,\n 0.09795371292993926,\n 0.09262699600766734,\n 0.09669041089138716,\n 0.09265963291378321,\n 0.09255618132951192,\n 0.09529848681085995,\n 0.09234798354020377,\n 0.09313143749841298,\n 0.09166631458668405,\n 0.09326932918940935,\n 0.09240237906859917,\n 0.09305557582485005,\n 0.09243203798323267,\n 0.09151870278724158,\n 0.09550399315932749,\n 0.09236300130271935,\n 0.09428268039756957,\n 0.09221177340886023,\n 0.09163608940734587,\n 0.09250655438588774,\n 0.09204033835741955,\n 0.09158479158429836,\n 0.09096744861985048,\n 0.09328565801148607,\n 0.09220130912457851,\n 0.09479789667055406,\n 0.09032495493258819,\n 0.0902936096853246,\n 0.09114645755388337,\n 0.09179054243035266,\n 0.09128427034481684,\n 0.09389807505168728,\n 0.09223176735823535,\n 0.08971518372635247,\n 0.09150524774375535]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor3_no_reg_nns[0].history['loss_train']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number 10/200\n",
      "Loss on training set: 0.25792015674845414 f_score on training set: 0.9356754886125802, loss on test set: 2.7028349718617166 f_score on test set: 0.6433707271922561\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.16373986158953685 f_score on training set: 0.9414933822228656, loss on test set: 1.1255214979331434 f_score on test set: 0.6805512171400784\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.14884978201940977 f_score on training set: 0.9365354026836886, loss on test set: 0.9496338356844618 f_score on test set: 0.6107550119965258\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.15465359925279498 f_score on training set: 0.9343731645239184, loss on test set: 0.8438143192632289 f_score on test set: 0.5643803974078896\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.1699354108115148 f_score on training set: 0.9152258062163487, loss on test set: 0.8918623208541813 f_score on test set: 0.40777158617584147\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.1879410478134766 f_score on training set: 0.9180820823642728, loss on test set: 0.9508465539697911 f_score on test set: 0.3874675324675325\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.20295196146934463 f_score on training set: 0.9253299752810036, loss on test set: 0.9597412228567491 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.21107664423316191 f_score on training set: 0.9291521486643438, loss on test set: 0.9633259746997487 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.21286907955141732 f_score on training set: 0.9291521486643438, loss on test set: 0.958779042148332 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.21344236716607318 f_score on training set: 0.9291521486643438, loss on test set: 0.968052157604825 f_score on test set: 0.39324476650563606\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.21179394466685958 f_score on training set: 0.9291521486643438, loss on test set: 0.9820902565255518 f_score on test set: 0.39324476650563606\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.21032981756194463 f_score on training set: 0.9291521486643438, loss on test set: 0.9853199853343853 f_score on test set: 0.39324476650563606\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.20874172286786993 f_score on training set: 0.9291521486643438, loss on test set: 0.9878700220803611 f_score on test set: 0.39324476650563606\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.2070913614998438 f_score on training set: 0.9291521486643438, loss on test set: 0.9909448980331219 f_score on test set: 0.39324476650563606\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.2061334030624576 f_score on training set: 0.9291521486643438, loss on test set: 0.9934276664554386 f_score on test set: 0.39324476650563606\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.20548497760886242 f_score on training set: 0.9291521486643438, loss on test set: 0.9974891323681107 f_score on test set: 0.39324476650563606\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.20521968627015566 f_score on training set: 0.9291521486643438, loss on test set: 1.0013703825137201 f_score on test set: 0.39324476650563606\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.20490847412112237 f_score on training set: 0.9291521486643438, loss on test set: 1.0059468481821454 f_score on test set: 0.39324476650563606\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.20492961773842447 f_score on training set: 0.9291521486643438, loss on test set: 1.0090113947976103 f_score on test set: 0.39324476650563606\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.20498460376842134 f_score on training set: 0.9291521486643438, loss on test set: 1.0120235293116313 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.2049873496928602 f_score on training set: 0.9291521486643438, loss on test set: 1.0123366398060356 f_score on test set: 0.39324476650563606\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.2049902248706796 f_score on training set: 0.9291521486643438, loss on test set: 1.012649379641013 f_score on test set: 0.39324476650563606\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.20499373590728423 f_score on training set: 0.9291521486643438, loss on test set: 1.0129572165463336 f_score on test set: 0.39324476650563606\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.20498884544889287 f_score on training set: 0.9291521486643438, loss on test set: 1.0133064298811507 f_score on test set: 0.39324476650563606\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.20499216547313898 f_score on training set: 0.9291521486643438, loss on test set: 1.01361304267285 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.20499879110614036 f_score on training set: 0.9291521486643438, loss on test set: 1.0139050444249034 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.20498601337931885 f_score on training set: 0.9291521486643438, loss on test set: 1.0142934607853435 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.20497901284618766 f_score on training set: 0.9291521486643438, loss on test set: 1.0146543489237827 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.2049872598784024 f_score on training set: 0.9291521486643438, loss on test set: 1.0149369623243245 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.2049877673309147 f_score on training set: 0.9291521486643438, loss on test set: 1.0152581168349448 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.3325308227164707 f_score on training set: 0.9306122448979591, loss on test set: 2.774697022864312 f_score on test set: 0.6352953018190245\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.17020190487269293 f_score on training set: 0.9255121632170814, loss on test set: 1.1309562473360908 f_score on test set: 0.6558856924820202\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.14014573189690904 f_score on training set: 0.9429826786575168, loss on test set: 1.1587566578528097 f_score on test set: 0.5061497175141244\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.139877281268598 f_score on training set: 0.9480347694633408, loss on test set: 0.8379365464618238 f_score on test set: 0.5027728458624607\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.15213702018326153 f_score on training set: 0.9420836827881837, loss on test set: 0.8573883437873014 f_score on test set: 0.43141080872824067\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.16697223349301368 f_score on training set: 0.9291521486643438, loss on test set: 0.9765172458047451 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.18088782810628456 f_score on training set: 0.9291521486643438, loss on test set: 1.028993535277829 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.19135479185137683 f_score on training set: 0.9291521486643438, loss on test set: 1.0399750515103028 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.19867098283719103 f_score on training set: 0.9291521486643438, loss on test set: 1.0356952361263148 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.20200102156266797 f_score on training set: 0.9291521486643438, loss on test set: 1.0317412050811159 f_score on test set: 0.39324476650563606\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.20284524399240364 f_score on training set: 0.9291521486643438, loss on test set: 1.0329463986629681 f_score on test set: 0.39324476650563606\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.2031191625359758 f_score on training set: 0.9291521486643438, loss on test set: 1.036558571666469 f_score on test set: 0.39324476650563606\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.20334518911277882 f_score on training set: 0.9291521486643438, loss on test set: 1.036966229394741 f_score on test set: 0.39324476650563606\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.20355480949409177 f_score on training set: 0.9291521486643438, loss on test set: 1.0389226046762454 f_score on test set: 0.39324476650563606\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.2038323827078286 f_score on training set: 0.9291521486643438, loss on test set: 1.0413912437575141 f_score on test set: 0.39324476650563606\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.20409270162577386 f_score on training set: 0.9291521486643438, loss on test set: 1.0444895946188395 f_score on test set: 0.39324476650563606\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.20437796314203496 f_score on training set: 0.9291521486643438, loss on test set: 1.0478188378097537 f_score on test set: 0.39324476650563606\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.2047479278895126 f_score on training set: 0.9291521486643438, loss on test set: 1.0507276692173932 f_score on test set: 0.39324476650563606\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.20503747218637158 f_score on training set: 0.9291521486643438, loss on test set: 1.0538166823856232 f_score on test set: 0.39324476650563606\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.20531329463842857 f_score on training set: 0.9291521486643438, loss on test set: 1.0566192827442993 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.20532503086069762 f_score on training set: 0.9291521486643438, loss on test set: 1.0569707815286637 f_score on test set: 0.39324476650563606\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.2053467644460175 f_score on training set: 0.9291521486643438, loss on test set: 1.0572603835669783 f_score on test set: 0.39324476650563606\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.20537808306103594 f_score on training set: 0.9291521486643438, loss on test set: 1.0574934364499837 f_score on test set: 0.39324476650563606\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.2054034267157133 f_score on training set: 0.9291521486643438, loss on test set: 1.0577555735052926 f_score on test set: 0.39324476650563606\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.2054346574525974 f_score on training set: 0.9291521486643438, loss on test set: 1.0579801007905056 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.205463061737004 f_score on training set: 0.9291521486643438, loss on test set: 1.0582149540740682 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.20548507626925747 f_score on training set: 0.9291521486643438, loss on test set: 1.0584830540863168 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.20551861415242517 f_score on training set: 0.9291521486643438, loss on test set: 1.0586798648810025 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.2055503614150376 f_score on training set: 0.9291521486643438, loss on test set: 1.0588836308257765 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.20558166241344808 f_score on training set: 0.9291521486643438, loss on test set: 1.059085634912857 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.2853674448117419 f_score on training set: 0.9359971711456858, loss on test set: 2.797470148830082 f_score on test set: 0.5122845036319612\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.18468793366983993 f_score on training set: 0.9404865012001683, loss on test set: 1.720774614430403 f_score on test set: 0.5487733235477568\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.13423895999492208 f_score on training set: 0.9411884411884412, loss on test set: 1.001594639108516 f_score on test set: 0.47649464001385655\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.13570244748873886 f_score on training set: 0.9455244945579542, loss on test set: 0.7802513275785369 f_score on test set: 0.47882848417732143\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.14774561100181452 f_score on training set: 0.9291521486643438, loss on test set: 0.8523275290325745 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.16290176791132482 f_score on training set: 0.9291521486643438, loss on test set: 0.9445041605466021 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.17692625774158283 f_score on training set: 0.9291521486643438, loss on test set: 0.9906472154898472 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.1896080165056986 f_score on training set: 0.9291521486643438, loss on test set: 1.024454863336741 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.1992381004932347 f_score on training set: 0.9291521486643438, loss on test set: 1.0209215511108285 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.20523605748910714 f_score on training set: 0.9291521486643438, loss on test set: 1.0107998477200852 f_score on test set: 0.39324476650563606\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.20808700966842236 f_score on training set: 0.9291521486643438, loss on test set: 1.002868138408641 f_score on test set: 0.39324476650563606\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.20955063456024733 f_score on training set: 0.9291521486643438, loss on test set: 0.9999058957566911 f_score on test set: 0.39324476650563606\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.2099371963642853 f_score on training set: 0.9291521486643438, loss on test set: 1.0035872601459621 f_score on test set: 0.39324476650563606\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.20965847958002726 f_score on training set: 0.9291521486643438, loss on test set: 1.010145528009409 f_score on test set: 0.39324476650563606\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.20959912810287734 f_score on training set: 0.9291521486643438, loss on test set: 1.0162804224310105 f_score on test set: 0.39324476650563606\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.20961988316933716 f_score on training set: 0.9291521486643438, loss on test set: 1.0221503508694885 f_score on test set: 0.39324476650563606\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.2096702787660524 f_score on training set: 0.9291521486643438, loss on test set: 1.0276072996667036 f_score on test set: 0.39324476650563606\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.2099461569500751 f_score on training set: 0.9291521486643438, loss on test set: 1.0319162954200187 f_score on test set: 0.39324476650563606\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.21018893292002 f_score on training set: 0.9291521486643438, loss on test set: 1.0361519385957747 f_score on test set: 0.39324476650563606\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.21046194426009923 f_score on training set: 0.9291521486643438, loss on test set: 1.0398747878158934 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.21049172854271453 f_score on training set: 0.9291521486643438, loss on test set: 1.040211380993039 f_score on test set: 0.39324476650563606\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.2105171619512109 f_score on training set: 0.9291521486643438, loss on test set: 1.0405655832641052 f_score on test set: 0.39324476650563606\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.2105467550655499 f_score on training set: 0.9291521486643438, loss on test set: 1.0408950199515232 f_score on test set: 0.39324476650563606\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.21057554146303303 f_score on training set: 0.9291521486643438, loss on test set: 1.0412245594190315 f_score on test set: 0.39324476650563606\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.21059999153601214 f_score on training set: 0.9291521486643438, loss on test set: 1.0415698086139935 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.21062061245763533 f_score on training set: 0.9291521486643438, loss on test set: 1.041931140228283 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.21064492578538718 f_score on training set: 0.9291521486643438, loss on test set: 1.0422688262944206 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.2106725612758336 f_score on training set: 0.9291521486643438, loss on test set: 1.0425847555778527 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.21069625686352814 f_score on training set: 0.9291521486643438, loss on test set: 1.0429162174649367 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.210715636231012 f_score on training set: 0.9291521486643438, loss on test set: 1.043265209028331 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.14888314727136165 f_score on training set: 0.9644692315032997, loss on test set: 1.5312405570320238 f_score on test set: 0.7412113644765939\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.08757656869665387 f_score on training set: 0.9759298744373374, loss on test set: 0.6785366232517298 f_score on test set: 0.7915611623911714\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.09504467270383017 f_score on training set: 0.9774456811078612, loss on test set: 0.5899737120529134 f_score on test set: 0.7015509183666064\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.1110943765521681 f_score on training set: 0.9547342419212816, loss on test set: 0.7281651492875335 f_score on test set: 0.5460258272800647\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.13495886359313908 f_score on training set: 0.9337225274725275, loss on test set: 0.8737248618548992 f_score on test set: 0.41263806777217016\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.15787166492007443 f_score on training set: 0.9291521486643438, loss on test set: 0.9803542294040295 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.1767180621063578 f_score on training set: 0.9291521486643438, loss on test set: 1.0327848008626104 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.19049256601453302 f_score on training set: 0.9291521486643438, loss on test set: 1.0264945589299157 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.2000632688832546 f_score on training set: 0.9291521486643438, loss on test set: 1.0078240254802282 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.20570092717749627 f_score on training set: 0.9291521486643438, loss on test set: 0.9999758837374665 f_score on test set: 0.39324476650563606\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.20843747138187374 f_score on training set: 0.9291521486643438, loss on test set: 1.00383770582871 f_score on test set: 0.39324476650563606\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.2098664637954685 f_score on training set: 0.9291521486643438, loss on test set: 1.015135660727549 f_score on test set: 0.39324476650563606\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.20952788996771368 f_score on training set: 0.9291521486643438, loss on test set: 1.024294275805548 f_score on test set: 0.39324476650563606\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.2089665256454286 f_score on training set: 0.9291521486643438, loss on test set: 1.0286110064237455 f_score on test set: 0.39324476650563606\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.20902299540425837 f_score on training set: 0.9291521486643438, loss on test set: 1.0311783910071097 f_score on test set: 0.39324476650563606\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.20936122759147893 f_score on training set: 0.9291521486643438, loss on test set: 1.0334313571128044 f_score on test set: 0.39324476650563606\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.20968451139452166 f_score on training set: 0.9291521486643438, loss on test set: 1.0359043981436384 f_score on test set: 0.39324476650563606\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.20995741412856778 f_score on training set: 0.9291521486643438, loss on test set: 1.0385779062046192 f_score on test set: 0.39324476650563606\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.21017129615692037 f_score on training set: 0.9291521486643438, loss on test set: 1.0414011175632194 f_score on test set: 0.39324476650563606\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.21034593666049453 f_score on training set: 0.9291521486643438, loss on test set: 1.0442151136355289 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.2103747923882685 f_score on training set: 0.9291521486643438, loss on test set: 1.0444282511291076 f_score on test set: 0.39324476650563606\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.21040211941146233 f_score on training set: 0.9291521486643438, loss on test set: 1.0446471747793522 f_score on test set: 0.39324476650563606\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.21042449875646715 f_score on training set: 0.9291521486643438, loss on test set: 1.0448880158158824 f_score on test set: 0.39324476650563606\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.21044170078224633 f_score on training set: 0.9291521486643438, loss on test set: 1.0451523392953483 f_score on test set: 0.39324476650563606\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.21046406490286224 f_score on training set: 0.9291521486643438, loss on test set: 1.0453920635702219 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.21048210482323912 f_score on training set: 0.9291521486643438, loss on test set: 1.0456521940514787 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.21050680705236366 f_score on training set: 0.9291521486643438, loss on test set: 1.0458761350861365 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.21052815903680072 f_score on training set: 0.9291521486643438, loss on test set: 1.046115836558082 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.2105514624364879 f_score on training set: 0.9291521486643438, loss on test set: 1.046342356579876 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.21057089164551804 f_score on training set: 0.9291521486643438, loss on test set: 1.0465865790819098 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/200\n",
      "Loss on training set: 0.2955269499969172 f_score on training set: 0.9532569162304201, loss on test set: 2.772115886698528 f_score on test set: 0.6543148554499044\n",
      "Epoch number 20/200\n",
      "Loss on training set: 0.11097713742504711 f_score on training set: 0.9687191544853049, loss on test set: 0.8321015076058081 f_score on test set: 0.7678717026378896\n",
      "Epoch number 30/200\n",
      "Loss on training set: 0.10102993254472795 f_score on training set: 0.9697015921073336, loss on test set: 0.9105238426591619 f_score on test set: 0.6211893630911015\n",
      "Epoch number 40/200\n",
      "Loss on training set: 0.10912795669164052 f_score on training set: 0.9683322375630067, loss on test set: 0.7206854851084795 f_score on test set: 0.601798411303263\n",
      "Epoch number 50/200\n",
      "Loss on training set: 0.1312992385143159 f_score on training set: 0.9495798319327731, loss on test set: 0.7836505805296451 f_score on test set: 0.477080525535071\n",
      "Epoch number 60/200\n",
      "Loss on training set: 0.1562987885810625 f_score on training set: 0.9291521486643438, loss on test set: 0.8859338193048899 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/200\n",
      "Loss on training set: 0.17695521550360382 f_score on training set: 0.9291521486643438, loss on test set: 0.9466180372181381 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/200\n",
      "Loss on training set: 0.19391699155741074 f_score on training set: 0.9291521486643438, loss on test set: 0.9676125044010948 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/200\n",
      "Loss on training set: 0.20631465487084197 f_score on training set: 0.9291521486643438, loss on test set: 0.9731629426899399 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/200\n",
      "Loss on training set: 0.21167046182990285 f_score on training set: 0.9291521486643438, loss on test set: 0.9710375772925672 f_score on test set: 0.39324476650563606\n",
      "Epoch number 110/200\n",
      "Loss on training set: 0.21207206430328754 f_score on training set: 0.9291521486643438, loss on test set: 0.9748048537019407 f_score on test set: 0.39324476650563606\n",
      "Epoch number 120/200\n",
      "Loss on training set: 0.2105659222010611 f_score on training set: 0.9291521486643438, loss on test set: 0.9796942557079591 f_score on test set: 0.39324476650563606\n",
      "Epoch number 130/200\n",
      "Loss on training set: 0.2091577442004429 f_score on training set: 0.9291521486643438, loss on test set: 0.9863229129461749 f_score on test set: 0.39324476650563606\n",
      "Epoch number 140/200\n",
      "Loss on training set: 0.2086100062858708 f_score on training set: 0.9291521486643438, loss on test set: 0.9918849332574028 f_score on test set: 0.39324476650563606\n",
      "Epoch number 150/200\n",
      "Loss on training set: 0.2083300350458261 f_score on training set: 0.9291521486643438, loss on test set: 0.9970087293654825 f_score on test set: 0.39324476650563606\n",
      "Epoch number 160/200\n",
      "Loss on training set: 0.20811258400536034 f_score on training set: 0.9291521486643438, loss on test set: 1.0016832111288991 f_score on test set: 0.39324476650563606\n",
      "Epoch number 170/200\n",
      "Loss on training set: 0.20796127844343426 f_score on training set: 0.9291521486643438, loss on test set: 1.0060669965124795 f_score on test set: 0.39324476650563606\n",
      "Epoch number 180/200\n",
      "Loss on training set: 0.20798544874716818 f_score on training set: 0.9291521486643438, loss on test set: 1.0097230080674189 f_score on test set: 0.39324476650563606\n",
      "Epoch number 190/200\n",
      "Loss on training set: 0.20795962650448388 f_score on training set: 0.9291521486643438, loss on test set: 1.0135955266784142 f_score on test set: 0.39324476650563606\n",
      "Epoch number 200/200\n",
      "Loss on training set: 0.2080050758443663 f_score on training set: 0.9291521486643438, loss on test set: 1.0169813253616442 f_score on test set: 0.39324476650563606\n",
      "Epoch number 10/100\n",
      "Loss on training set: 0.20801764069109302 f_score on training set: 0.9291521486643438, loss on test set: 1.0172746899900094 f_score on test set: 0.39324476650563606\n",
      "Epoch number 20/100\n",
      "Loss on training set: 0.20802939296500805 f_score on training set: 0.9291521486643438, loss on test set: 1.0175715180192997 f_score on test set: 0.39324476650563606\n",
      "Epoch number 30/100\n",
      "Loss on training set: 0.2080409608884678 f_score on training set: 0.9291521486643438, loss on test set: 1.0178679568020597 f_score on test set: 0.39324476650563606\n",
      "Epoch number 40/100\n",
      "Loss on training set: 0.20804066535336357 f_score on training set: 0.9291521486643438, loss on test set: 1.0182190680924177 f_score on test set: 0.39324476650563606\n",
      "Epoch number 50/100\n",
      "Loss on training set: 0.2080480004342529 f_score on training set: 0.9291521486643438, loss on test set: 1.0185370080826905 f_score on test set: 0.39324476650563606\n",
      "Epoch number 60/100\n",
      "Loss on training set: 0.20804962882103387 f_score on training set: 0.9291521486643438, loss on test set: 1.0188836183560335 f_score on test set: 0.39324476650563606\n",
      "Epoch number 70/100\n",
      "Loss on training set: 0.2080569320164699 f_score on training set: 0.9291521486643438, loss on test set: 1.019204533156282 f_score on test set: 0.39324476650563606\n",
      "Epoch number 80/100\n",
      "Loss on training set: 0.20806111386742496 f_score on training set: 0.9291521486643438, loss on test set: 1.0195428467294545 f_score on test set: 0.39324476650563606\n",
      "Epoch number 90/100\n",
      "Loss on training set: 0.20807788513141795 f_score on training set: 0.9291521486643438, loss on test set: 1.019823963729305 f_score on test set: 0.39324476650563606\n",
      "Epoch number 100/100\n",
      "Loss on training set: 0.2080885080180103 f_score on training set: 0.9291521486643438, loss on test set: 1.020133350227604 f_score on test set: 0.39324476650563606\n"
     ]
    }
   ],
   "source": [
    "xor3_l2_build = {'input_shape': xor3_x_train.shape, 'neurons_num': [40, 40, 2], 'activations': [ReLU(), ReLU(), Softmax()]}\n",
    "xor3_l2_fit = [{'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 200, 'learning_rate': 0.0001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 1},\n",
    "                 {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 100, 'learning_rate': 0.00001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': cross_entropy, 'metric': f_score, 'verbose_step': 10, 'regularization_rate': 1}]\n",
    "                 # {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 1130, 'learning_rate': 0.0001, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0},\n",
    "                 # {'x_train': xor3_x_train, 'y_train': xor3_y_train, 'batch_size': 4, 'n_epochs': 1000, 'learning_rate': 0.00005, 'x_test': xor3_x_test, 'y_test': xor3_y_test, 'loss': mse, 'metric': mse, 'verbose_step': 10, 'regularization_rate': 0}]\n",
    "results_train, results_test, xor3_l2_nns = cv_network(build_args=xor3_l2_build, fit_args=xor3_l2_fit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(i)\n",
    "\n",
    "    assert ~np.isnan(np.array(xor3_l2_nns[i].history['loss_train'])).any()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "avg_loss_xor3_train_l2, avg_loss_xor3_test_l2 = average_loss([-1, xor3_l2_nns[0], xor3_l2_nns[1],  xor3_l2_nns[3]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}